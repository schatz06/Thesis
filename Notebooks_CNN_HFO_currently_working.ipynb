{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Notebooks_CNN_HFO_v2.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "PYXlwVnXKhOJ",
        "CVtiFO19ymk_",
        "fzh5PNccJ28-",
        "gcdrh-r9JRdS",
        "2pQEA9LADkBP",
        "J5uoUeQBDq5Q",
        "vMCxn-8P5tsH",
        "UwH_nXJZDz2T",
        "NNNnI_mU6LN5"
      ],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/schatz06/Thesis/blob/main/Notebooks_CNN_HFO_currently_working.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aa_PJwlt7zI2"
      },
      "source": [
        "##Mount drive## "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9DjpeG7t71KC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70cfa457-017c-43c9-abf4-ac4edbf200aa"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IMo4HooFl0dv"
      },
      "source": [
        "##General Variables to use to load data##"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sJHXZcSlNqL9"
      },
      "source": [
        "fold = 8 # which fold to take \n",
        "embedding = \"protbert\"\n",
        "#embedding = \"seqvec\"\n",
        "#dataset=\"PISCES\" \n",
        "dataset=\"CB513\"\n",
        "USE_HFO=True "
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PYXlwVnXKhOJ"
      },
      "source": [
        "## Imports ##"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WCuZCtMgg5ns"
      },
      "source": [
        "# Reload all modules (except those excluded by %aimport) every time before executing the Python code typed.\n",
        "%load_ext autoreload \n",
        "%autoreload 2\n",
        "# matplotlib graphs will be included in your notebook, next to the code. \n",
        "%matplotlib inline"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HKDaAU3F5Bgs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b311b7fe-d78a-4694-c8f3-ccbd72c01857"
      },
      "source": [
        "# install hdf5storage package \r\n",
        "!pip install hdf5storage"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: hdf5storage in /usr/local/lib/python3.7/dist-packages (0.1.16)\n",
            "Requirement already satisfied: h5py>=2.1; python_version >= \"3.3\" in /usr/local/lib/python3.7/dist-packages (from hdf5storage) (2.10.0)\n",
            "Requirement already satisfied: numpy; python_version >= \"3.4\" in /usr/local/lib/python3.7/dist-packages (from hdf5storage) (1.19.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from h5py>=2.1; python_version >= \"3.3\"->hdf5storage) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cjrANRzOFauh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c48ec75b-d0ea-4e5b-b9ed-9c087ba4cdd5"
      },
      "source": [
        "import pdb # python debugger\n",
        "import numpy as np # import numpy \n",
        "import tensorflow as tf # import tensorflow  \n",
        "tf.compat.v1.disable_eager_execution() # disable eager execution\n",
        "import time # import time\n",
        "import math # import math\n",
        "import argparse # import argparse\n",
        "import os # import os\n",
        "import scipy.io as sio # import scipy.io \n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior() # makes different behaviors betweem tf_v1 & tf_v2 behave the same \n",
        "from tensorflow.python.client import device_lib # package to find available gpus\n",
        "import pandas as pd # import padas\n",
        "import hdf5storage # import hdf5storage"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CVtiFO19ymk_"
      },
      "source": [
        "## Get data ##"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yfOssHbYEh-W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "137b9cfc-ac46-4c8d-de88-ae8d2ab0fad8"
      },
      "source": [
        "VALID_FILE = \"/content/drive/MyDrive/Datasets/{0}_{1}_testSet{2}.mat\".format(dataset.lower(),embedding,str(fold)) # validation set\n",
        "TRAIN_FILE = \"/content/drive/MyDrive/Datasets/{0}_{1}_trainSet{2}.mat\".format(dataset.lower(),embedding,str(fold)) # train set  \n",
        "TEST_FILE = \"/content/drive/MyDrive/Datasets/casp13_{0}.mat\".format(embedding) # test set CASP13\n",
        "print(VALID_FILE)\n",
        "print(TRAIN_FILE)\n",
        "print(TEST_FILE)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/Datasets/cb513_protbert_testSet8.mat\n",
            "/content/drive/MyDrive/Datasets/cb513_protbert_trainSet8.mat\n",
            "/content/drive/MyDrive/Datasets/casp13_protbert.mat\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QKVIpZww1-dw"
      },
      "source": [
        "HEIGHT = 32\n",
        "WIDTH = 32\n",
        "DEPTH = 1\n",
        "CATEGORIES = 3 # number of different classification categories"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fzh5PNccJ28-"
      },
      "source": [
        "## VGG ##"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NwaNt8clJ5Vu"
      },
      "source": [
        "\"\"\"\n",
        "Codes are modifeid from PyTorch and Tensorflow Versions of VGG: \n",
        "https://github.com/pytorch/vision/blob/master/torchvision/models/vgg.py, and\n",
        "https://github.com/keras-team/keras-applications/blob/master/keras_applications/vgg16.py\n",
        "\"\"\"\n",
        "\n",
        "# import tensorflow.compat.v1 as tf\n",
        "# tf.disable_v2_behavior()\n",
        "# import numpy as np \n",
        "# import pdb\n",
        "from tensorflow.keras.applications.vgg16 import VGG16 as vgg16 # import vgg16 convolutional network\n",
        "from tensorflow.keras.applications.vgg19 import VGG19 as vgg19 # import vgg19 convolutional network\n",
        "\n",
        "__all__ = ['VGG11', 'VGG13', 'VGG16','VGG19'] # array holds all vgg CNN's names\n",
        " \n",
        "def VGG(feature, num_cls): # define VGG \n",
        "\n",
        "\twith tf.variable_scope('fully_connected') as scope:\n",
        "\t\tdim =np.prod(feature.shape[1:]) # returns the product of the given array\n",
        "\t\tx = tf.reshape(feature, [-1, dim]) # reshape tensor\n",
        "\n",
        "\t\tx = tf.keras.layers.Dense(units=4096, activation='relu', name=scope.name)(x) # define layers \n",
        "\t\tx = tf.keras.layers.Dense(units=4096, activation='relu', name=scope.name)(x)\n",
        "\t\tx = tf.keras.layers.Dense(units=num_cls, name=scope.name)(x)\n",
        "\n",
        "\treturn x\n",
        "# make the layers of CNN \n",
        "def make_layers(x, cfg):\n",
        "\tfor v in cfg:\n",
        "\t\tif v == 'M':\n",
        "\t\t\tx = tf.keras.layers.MaxPool2D(pool_size=[2, 2], strides=2, padding='valid')(x)\n",
        "\t\telse:\n",
        "\t\t\tx = tf.keras.layers.Conv2D(\n",
        "\t\t\tfilters=v,\n",
        "\t\t\tkernel_size=[3, 3],\n",
        "\t\t\tpadding='SAME',\n",
        "\t\t\tactivation=tf.nn.relu\n",
        "\t\t\t)(x)\n",
        "\treturn x\n",
        "\n",
        "cfg = {\n",
        "\t'A': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
        "\t'B': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
        "\t'D': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
        "\t'E': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', \n",
        "\t\t  512, 512, 512, 512, 'M'],\n",
        "}\n",
        "\n",
        "def VGG11(x_images, num_cls):\n",
        "\tfeature = make_layers(x_images, cfg['A'])\n",
        "\treturn VGG(feature, num_cls)\n",
        "\n",
        "def VGG13(x_images, num_cls):\n",
        "\tfeature = make_layers(x_images, cfg['B'])\n",
        "\treturn VGG(feature, num_cls)\n",
        "\n",
        "def VGG16(x_images, num_cls):\n",
        "\tfeature = make_layers(x_images, cfg['D'])\n",
        "\treturn VGG(feature, num_cls)\n",
        "\n",
        "def VGG19(x_images, num_cls):\n",
        "\tfeature = make_layers(x_images, cfg['E'])\n",
        "\treturn VGG(feature, num_cls)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gcdrh-r9JRdS"
      },
      "source": [
        "## Net ##"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kHA5zxK1JRtg"
      },
      "source": [
        "# import tensorflow.compat.v1 as tf\n",
        "# tf.disable_v2_behavior()\n",
        "# import math\n",
        "# import pdb\n",
        "# from tensorflow.python.client import device_lib\n",
        "# import numpy as np\n",
        "# from net.vgg import *\n",
        "\n",
        "def CNN_4layers(x_image, num_cls, reuse=False):\n",
        "\t_NUM_CLASSES = num_cls\n",
        "\twith tf.variable_scope('conv1', reuse=reuse) as scope:\n",
        "\t\tconv = tf.keras.layers.Conv2D(\n",
        "\t\t\tfilters=64,\n",
        "\t\t\tkernel_size=[5,5],\n",
        "\t\t\tpadding='SAME',\n",
        "\t\t\tactivation=tf.nn.relu\n",
        "\t\t)(x_image)\n",
        "\n",
        "\twith tf.variable_scope('conv2', reuse=reuse) as scope:\n",
        "\t\tconv = tf.keras.layers.Conv2D(\n",
        "\t\t\tfilters=128,\n",
        "\t\t\tkernel_size=[5,5],\n",
        "\t\t\tpadding='SAME',\n",
        "\t\t\tactivation=tf.nn.relu\n",
        "\t\t)(conv)\n",
        "\t\t\n",
        "\twith tf.variable_scope('conv3', reuse=reuse) as scope:\n",
        "\t\tconv = tf.keras.layers.Conv2D(\n",
        "\t\t\tfilters=256,\n",
        "\t\t\tkernel_size=[3,3],\n",
        "\t\t\tpadding='SAME',\n",
        "\t\t\tactivation=tf.nn.relu\n",
        "\t\t)(conv)\n",
        "\n",
        "\twith tf.variable_scope('fully_connected', reuse=reuse) as scope:\n",
        "\t\tdim =np.prod(conv.shape[1:])\n",
        "\t\tflat = tf.reshape(conv, [-1, dim])\n",
        "\t\toutputs = tf.keras.layers.Dense(units=_NUM_CLASSES, name=scope.name)(flat)\n",
        "\n",
        "\treturn outputs\n",
        "\n",
        "\n",
        "\t# with tf.variable_scope('conv1', reuse=reuse) as scope:\n",
        "\t# \tconv = tf.keras.layers.Conv2D(\n",
        "\t# \t\tfilters=32,\n",
        "\t# \t\tkernel_size=[5, 5],\n",
        "\t# \t\tpadding='SAME',\n",
        "\t# \t\tactivation=tf.nn.relu\n",
        "\t# \t)(x_image)\n",
        "\t# \tpool = tf.keras.layers.MaxPool2D(pool_size=[2, 2], strides=2, padding='valid')(conv)\n",
        "\t# \t# N x 16 x 16 x 32\n",
        "\n",
        "\t# with tf.variable_scope('conv2', reuse=reuse) as scope:\n",
        "\t# \tconv = tf.keras.layers.Conv2D(\n",
        "\t# \t\tfilters=64,\n",
        "\t# \t\tkernel_size=[3, 3],\n",
        "\t# \t\tpadding='SAME',\n",
        "\t# \t\tactivation=tf.nn.relu\n",
        "\t# \t)(pool)\n",
        "\t# \tpool = tf.keras.layers.MaxPool2D(pool_size=[2, 2], strides=2, padding='valid')(conv)\n",
        "\t# \t# N x 8 x 8 x 64\n",
        "\t\t\n",
        "\t# with tf.variable_scope('conv3', reuse=reuse) as scope:\n",
        "\t# \tconv = tf.keras.layers.Conv2D(\n",
        "\t# \t\tfilters=64,\n",
        "\t# \t\tkernel_size=[3, 3],\n",
        "\t# \t\tpadding='SAME',\n",
        "\t# \t\tactivation=tf.nn.relu\n",
        "\t# \t)(pool)\n",
        "\t# \tpool = tf.keras.layers.MaxPool2D(pool_size=[2, 2], strides=2, padding='valid')(conv)\n",
        "\t# \t# N x 4 x 4 x 64\n",
        "\n",
        "\t# with tf.variable_scope('fully_connected', reuse=reuse) as scope:\n",
        "\t# \tdim =np.prod(pool.shape[1:])\n",
        "\t# \tflat = tf.reshape(pool, [-1, dim])\n",
        "\t# \toutputs = tf.keras.layers.Dense(units=_NUM_CLASSES, name=scope.name)(flat)\n",
        "\n",
        "\t# return outputs\n",
        "\n",
        "def CNN_7layers(x_image, num_cls, reuse=False):\n",
        "\t_NUM_CLASSES = num_cls\n",
        "\twith tf.variable_scope('conv1', reuse=reuse) as scope:\n",
        "\t\tconv = tf.keras.layers.Conv2D(\n",
        "\t\t\tfilters=64,\n",
        "\t\t\tkernel_size=[3, 3],\n",
        "\t\t\tpadding='SAME',\n",
        "\t\t\tactivation=tf.nn.relu\n",
        "\t\t)(x_image)\n",
        "\t\tconv = tf.keras.layers.Conv2D(\n",
        "\t\t\tfilters=64,\n",
        "\t\t\tkernel_size=[3, 3],\n",
        "\t\t\tpadding='SAME',\n",
        "\t\t\tactivation=tf.nn.relu\n",
        "\t\t)(conv)\n",
        "\n",
        "\twith tf.variable_scope('conv2', reuse=reuse) as scope:\n",
        "\t\tconv = tf.keras.layers.Conv2D(\n",
        "\t\t\tfilters=64,\n",
        "\t\t\tkernel_size=[3, 3],\n",
        "\t\t\tpadding='SAME',\n",
        "\t\t\tactivation=tf.nn.relu\n",
        "\t\t)(conv)\n",
        "\t\tconv = tf.keras.layers.Conv2D(\n",
        "\t\t\tfilters=64,\n",
        "\t\t\tkernel_size=[3, 3],\n",
        "\t\t\tpadding='SAME',\n",
        "\t\t\tactivation=tf.nn.relu\n",
        "\t\t)(conv)\n",
        "\n",
        "\twith tf.variable_scope('conv3', reuse=reuse) as scope:\n",
        "\t\tconv = tf.keras.layers.Conv2D(\n",
        "\t\t\tfilters=64,\n",
        "\t\t\tkernel_size=[3, 3],\n",
        "\t\t\tpadding='SAME',\n",
        "\t\t\tactivation=tf.nn.relu\n",
        "\t\t)(conv)\n",
        "\t\tconv = tf.keras.layers.Conv2D(\n",
        "\t\t\tfilters=64,\n",
        "\t\t\tkernel_size=[3, 3],\n",
        "\t\t\tpadding='SAME',\n",
        "\t\t\tactivation=tf.nn.relu\n",
        "\t\t)(conv)\n",
        "\n",
        "\twith tf.variable_scope('fully_connected', reuse=reuse) as scope:\n",
        "\t\tdim = np.prod(conv.shape[1:])\n",
        "\t\tflat = tf.reshape(conv, [-1, dim])\n",
        "\t\toutputs = tf.keras.layers.Dense(units=_NUM_CLASSES, name=scope.name)(flat)\n",
        "\n",
        "\treturn outputs\n",
        "\n",
        "\t# with tf.variable_scope('conv1', reuse=reuse) as scope:\n",
        "\t# \tconv = tf.keras.layers.Conv2D(\n",
        "\t# \t\tfilters=32,\n",
        "\t# \t\tkernel_size=[5, 5],\n",
        "\t# \t\tpadding='SAME',\n",
        "\t# \t\tactivation=tf.nn.relu\n",
        "\t# \t)(x_image)\n",
        "\t# \tconv = tf.keras.layers.Conv2D(\n",
        "\t# \t\tfilters=32,\n",
        "\t# \t\tkernel_size=[3, 3],\n",
        "\t# \t\tpadding='SAME',\n",
        "\t# \t\tactivation=tf.nn.relu\n",
        "\t# \t)(conv)\n",
        "\t# \tpool = tf.keras.layers.MaxPool2D(pool_size=[2, 2], strides=2, padding='valid')(conv)\n",
        "\t# \t# N x 16 x 16 x 32\n",
        "\n",
        "\t# with tf.variable_scope('conv2', reuse=reuse) as scope:\n",
        "\t# \tconv = tf.keras.layers.Conv2D(\n",
        "\t# \t\tfilters=64,\n",
        "\t# \t\tkernel_size=[3, 3],\n",
        "\t# \t\tpadding='SAME',\n",
        "\t# \t\tactivation=tf.nn.relu\n",
        "\t# \t)(pool)\n",
        "\t# \tconv = tf.keras.layers.Conv2D(\n",
        "\t# \t\tfilters=64,\n",
        "\t# \t\tkernel_size=[3, 3],\n",
        "\t# \t\tpadding='SAME',\n",
        "\t# \t\tactivation=tf.nn.relu\n",
        "\t# \t)(conv)\n",
        "\t# \tpool = tf.keras.layers.MaxPool2D(pool_size=[2, 2], strides=2, padding='valid')(conv)\n",
        "\t# \t# N x 8 x 8 x 64\n",
        "\n",
        "\t# with tf.variable_scope('conv3', reuse=reuse) as scope:\n",
        "\t# \tconv = tf.keras.layers.Conv2D(\n",
        "\t# \t\tfilters=64,\n",
        "\t# \t\tkernel_size=[3, 3],\n",
        "\t# \t\tpadding='SAME',\n",
        "\t# \t\tactivation=tf.nn.relu\n",
        "\t# \t)(pool)\n",
        "\t# \tconv = tf.keras.layers.Conv2D(\n",
        "\t# \t\tfilters=128,\n",
        "\t# \t\tkernel_size=[3, 3],\n",
        "\t# \t\tpadding='SAME',\n",
        "\t# \t\tactivation=tf.nn.relu\n",
        "\t# \t)(conv)\n",
        "\t# \tpool = tf.keras.layers.MaxPool2D(pool_size=[2, 2], strides=2, padding='valid')(conv)\n",
        "\t# \t# pool = tf.layers.dropout(pool, rate=0.25, name=scope.name)\n",
        "\t# \t# N x 4 x 4 x 128\n",
        "\n",
        "\t# with tf.variable_scope('fully_connected', reuse=reuse) as scope:\n",
        "\t# \tdim = np.prod(pool.shape[1:])\n",
        "\t# \tflat = tf.reshape(pool, [-1, dim])\n",
        "\t# \toutputs = tf.keras.layers.Dense(units=_NUM_CLASSES, name=scope.name)(flat)\n",
        "\n",
        "\t# return outputs\n",
        "\n",
        "def CNN(net, num_cls, dim):\n",
        "\n",
        "\t_NUM_CLASSES = num_cls\n",
        "\t_IMAGE_HEIGHT, _IMAGE_WIDTH, _IMAGE_CHANNELS = dim\n",
        "\n",
        "\twith tf.name_scope('main_params'):\n",
        "\t\tx = tf.placeholder(tf.float32, shape=[None, _IMAGE_HEIGHT, _IMAGE_WIDTH, _IMAGE_CHANNELS], name='input_of_net')\n",
        "\t\ty = tf.placeholder(tf.float32, shape=[None, _NUM_CLASSES], name='labels')\n",
        "\n",
        "\t# call CNN structure according to string net\n",
        "\toutputs = globals()[net](x, _NUM_CLASSES)\n",
        "\toutputs = tf.identity(outputs, name='output_of_net')\n",
        "\n",
        "\treturn (x, y, outputs)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2pQEA9LADkBP"
      },
      "source": [
        "## Utilities ##"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JxQzW-93DIdN"
      },
      "source": [
        "# import numpy as np\n",
        "# import math\n",
        "# import scipy.io as sio\n",
        "# import os\n",
        "# import math\n",
        "# import pdb\n",
        "\n",
        "class ConfigClass(object):\n",
        "    def __init__(self, args, num_data, num_cls):\n",
        "        super(ConfigClass, self).__init__()\n",
        "        self.args = args\n",
        "        self.iter_max = args.iter_max\n",
        "        \n",
        "        # Different notations of regularization term:\n",
        "        # In SGD, weight decay:\n",
        "        #     weight_decay <- lr/(C*num_of_training_samples)\n",
        "        # In Newton method:\n",
        "        #     C <- C * num_of_training_samples\n",
        "\n",
        "        self.seed = args.seed\n",
        "\n",
        "        if self.seed is None:\n",
        "            print('You choose not to specify a random seed.'+\\\n",
        "                'A different result is produced after each run.')\n",
        "        elif isinstance(self.seed, int) and self.seed >= 0:\n",
        "            print('You specify random seed {}.'.format(self.seed))\n",
        "        else:\n",
        "            raise ValueError('Only accept None type or nonnegative integers for'+\\\n",
        "                    ' random seed argument!')\n",
        "\n",
        "        self.train_set = args.train_set\n",
        "        self.val_set = args.val_set\n",
        "        self.num_cls = num_cls\n",
        "        self.dim = args.dim\n",
        "\n",
        "        self.num_data = num_data\n",
        "        self.GNsize = min(args.GNsize, self.num_data)\n",
        "        self.C = args.C * self.num_data\n",
        "        self.net = args.net\n",
        "\n",
        "        self.xi = 0.1\n",
        "        self.CGmax = args.CGmax\n",
        "        self._lambda = args._lambda\n",
        "        self.drop = args.drop\n",
        "        self.boost = args.boost\n",
        "        self.eta = args.eta\n",
        "        self.lr = args.lr\n",
        "        self.lr_decay = args.lr_decay\n",
        "\n",
        "        self.bsize = args.bsize\n",
        "        if args.momentum < 0:\n",
        "            raise ValueError('Momentum needs to be larger than 0!')\n",
        "        self.momentum = args.momentum\n",
        "\n",
        "        self.loss = args.loss\n",
        "        if self.loss not in ('MSELoss', 'CrossEntropy'):\n",
        "            raise ValueError('Unrecognized loss type!')\n",
        "        self.optim = args.optim\n",
        "        if self.optim not in ('SGD', 'NewtonCG', 'Adam'):\n",
        "            raise ValueError('Only support SGD, Adam & NewtonCG optimizer!')\n",
        "        \n",
        "        self.log_file = args.log_file\n",
        "        self.model_file = args.model_file\n",
        "        self.screen_log_only = args.screen_log_only\n",
        "\n",
        "        if self.screen_log_only:\n",
        "            print('You choose not to store running log. Only store model to {}'.format(self.log_file))\n",
        "        else:\n",
        "            print('Saving log to: {}'.format(self.log_file))\n",
        "            dir_name, _ = os.path.split(self.log_file)\n",
        "            if not os.path.isdir(dir_name):\n",
        "                os.makedirs(dir_name, exist_ok=True)\n",
        "\n",
        "        dir_name, _ = os.path.split(self.model_file)\n",
        "        if not os.path.isdir(dir_name):\n",
        "            os.makedirs(dir_name, exist_ok=True)\n",
        "        \n",
        "        self.elapsed_time = 0.0\n",
        "\n",
        "def read_data(filename, dim, label_enum=None):\n",
        "    \"\"\"\n",
        "    args:\n",
        "        filename: the path where .mat files are stored\n",
        "        label_enum (default None): the list that stores the original labels. \n",
        "            If label_enum is None, the function will generate a new list which stores the \n",
        "            original labels in a sequence, and map original labels to [0, 1, ... number_of_classes-1]. \n",
        "            If label_enum is a list, the function will use it to convert \n",
        "            original labels to [0, 1,..., number_of_classes-1].\n",
        "    \"\"\"\n",
        "\n",
        "    #mat_contents = sio.loadmat(filename)\n",
        "    mat_contents = hdf5storage.loadmat(filename)\n",
        "    images, labels = mat_contents['x'], mat_contents['y']\n",
        "    \n",
        "    labels = labels.reshape(-1)\n",
        "    images = images.reshape(images.shape[0], -1)\n",
        "\n",
        "    _IMAGE_HEIGHT, _IMAGE_WIDTH, _IMAGE_CHANNELS = dim\n",
        "    zero_to_append = np.zeros((images.shape[0],\n",
        "            _IMAGE_CHANNELS*_IMAGE_HEIGHT*_IMAGE_WIDTH-np.prod(images.shape[1:])))\n",
        "    images = np.append(images, zero_to_append, axis=1)\n",
        "\n",
        "    # check data validity\n",
        "    if label_enum is None:\n",
        "        label_enum, labels = np.unique(labels, return_inverse=True)\n",
        "        num_cls = labels.max() + 1\n",
        "\n",
        "        if len(label_enum) != num_cls:\n",
        "            raise ValueError('The number of classes is not equal to the number of\\\n",
        "                            labels in dataset. Please verify them.')\n",
        "    else:\n",
        "        num_cls = len(label_enum)\n",
        "        forward_map = dict(zip(label_enum, np.arange(num_cls)))\n",
        "        labels = np.expand_dims(labels, axis=1)\n",
        "        labels = np.apply_along_axis(lambda x:forward_map[x[0]], axis=1, arr=labels)\n",
        "        \n",
        "\n",
        "    # convert groundtruth to one-hot encoding\n",
        "    labels = np.eye(num_cls)[labels]\n",
        "    labels = labels.astype('float32')\n",
        "\n",
        "    return [images, labels], num_cls, label_enum\n",
        "\n",
        "def normalize_and_reshape(images, dim, mean_tr=None):\n",
        "    _IMAGE_HEIGHT, _IMAGE_WIDTH, _IMAGE_CHANNELS = dim\n",
        "    images_shape = [images.shape[0], _IMAGE_CHANNELS, _IMAGE_HEIGHT, _IMAGE_WIDTH]\n",
        "    # images normalization and zero centering\n",
        "    images = images.reshape(images_shape[0], -1)\n",
        "    #images = (images/255.0).astype(np.single)\n",
        "    if mean_tr is None:\n",
        "        #print('No mean of data provided! Normalize images by their own mean.')\n",
        "        # if no mean_tr is provided, we calculate it according to the current data\n",
        "        mean_tr = images.mean(axis=0) \n",
        "    else:\n",
        "        #print('Normalize images according to the provided mean.')\n",
        "        if np.prod(mean_tr.shape) != np.prod(dim):\n",
        "            raise ValueError('Dimension of provided mean does not agree with the data! Please verify them!')\n",
        "    #images = (images - mean_tr).astype(np.single)\n",
        "\n",
        "    images = images.reshape(images_shape)\n",
        "    # Tensorflow accepts data shape: B x H x W x C\n",
        "    images = np.transpose(images, (0, 2, 3, 1))\n",
        "    return images, mean_tr\n",
        "\n",
        "\n",
        "def predict(sess, network, test_batch, bsize):\n",
        "    x, y, loss, outputs = network\n",
        "\n",
        "    test_inputs, test_labels = test_batch\n",
        "    batch_size = bsize\n",
        "\n",
        "    num_data = test_labels.shape[0]\n",
        "    num_batches = math.ceil(num_data/batch_size)\n",
        "\n",
        "    results = np.zeros(shape=num_data, dtype=np.int)\n",
        "    infer_loss = 0.0\n",
        "\n",
        "    for i in range(num_batches):\n",
        "        batch_idx = np.arange(i*batch_size, min((i+1)*batch_size, num_data))\n",
        "\n",
        "        batch_input = test_inputs[batch_idx]\n",
        "        batch_labels = test_labels[batch_idx]\n",
        "\n",
        "        net_outputs, _loss = sess.run(\n",
        "            [outputs, loss], feed_dict={x: batch_input, y: batch_labels}\n",
        "            )\n",
        "        \n",
        "        results[batch_idx] = np.argmax(net_outputs, axis=1)\n",
        "        # note that _loss was summed over batches\n",
        "        infer_loss = infer_loss + _loss\n",
        "\n",
        "    avg_acc = (np.argmax(test_labels, axis=1) == results).mean()\n",
        "    avg_loss = infer_loss/num_data\n",
        "    \n",
        "    return avg_loss, avg_acc, results"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J5uoUeQBDq5Q"
      },
      "source": [
        "## Newton - CG ##"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IDug0aiKCqeG"
      },
      "source": [
        "# import pdb\n",
        "# import tensorflow as tf\n",
        "# import time\n",
        "# import numpy as np\n",
        "# import os\n",
        "# import math\n",
        "# from utilities import predict\n",
        "\n",
        "def Rop(f, weights, v):\n",
        "    \"\"\"Implementation of R operator\n",
        "    Args:\n",
        "        f: any function of weights\n",
        "        weights: list of tensors.\n",
        "        v: vector for right multiplication\n",
        "    Returns:\n",
        "        Jv: Jaccobian vector product, length same as\n",
        "            the number of output of f\n",
        "    \"\"\"\n",
        "    if type(f) == list:\n",
        "        u = [tf.zeros_like(ff) for ff in f]\n",
        "    else:\n",
        "        u = tf.zeros_like(f)  # dummy variable\n",
        "    g = tf.gradients(ys=f, xs=weights, grad_ys=u)\n",
        "    return tf.gradients(ys=g, xs=u, grad_ys=v)\n",
        "\n",
        "def Gauss_Newton_vec(outputs, loss, weights, v):\n",
        "    \"\"\"Implements Gauss-Newton vector product.\n",
        "    Args:\n",
        "        loss: Loss function.\n",
        "        outputs: outputs of the last layer (pre-softmax).\n",
        "        weights: Weights, list of tensors.\n",
        "        v: vector to be multiplied with Gauss Newton matrix\n",
        "    Returns:\n",
        "        J'BJv: Guass-Newton vector product.\n",
        "    \"\"\"\n",
        "    # Validate the input\n",
        "    if type(weights) == list:\n",
        "        if len(v) != len(weights):\n",
        "            raise ValueError(\"weights and v must have the same length.\")\n",
        "\n",
        "    grads_outputs = tf.gradients(ys=loss, xs=outputs)\n",
        "    BJv = Rop(grads_outputs, weights, v)\n",
        "    JBJv = tf.gradients(ys=outputs, xs=weights, grad_ys=BJv)\n",
        "    return JBJv\n",
        "    \n",
        "\n",
        "class newton_cg(object):\n",
        "    def __init__(self, config, sess, outputs, loss):\n",
        "        \"\"\"\n",
        "        initialize operations and vairables that will be used in newton\n",
        "        args:\n",
        "            sess: tensorflow session\n",
        "            outputs: output of the neural network (pre-softmax layer)\n",
        "            loss: function to calculate loss\n",
        "        \"\"\"\n",
        "        super(newton_cg, self).__init__()\n",
        "        self.sess = sess\n",
        "        self.config = config\n",
        "        self.outputs = outputs\n",
        "        self.loss = loss\n",
        "        self.param = tf.compat.v1.trainable_variables()\n",
        "\n",
        "        self.CGiter = 0\n",
        "        FLOAT = tf.float32\n",
        "        model_weight = self.vectorize(self.param)\n",
        "        \n",
        "        # initial variable used in CG\n",
        "        zeros = tf.zeros(model_weight.get_shape(), dtype=FLOAT)\n",
        "        self.r = tf.Variable(zeros, dtype=FLOAT, trainable=False)\n",
        "        self.v = tf.Variable(zeros, dtype=FLOAT, trainable=False)\n",
        "        self.s = tf.Variable(zeros, dtype=FLOAT, trainable=False)\n",
        "        self.g = tf.Variable(zeros, dtype=FLOAT, trainable=False)\n",
        "        # initial Gv, f for method minibatch\n",
        "        self.Gv = tf.Variable(zeros, dtype=FLOAT, trainable=False)\n",
        "        self.f = tf.Variable(0., dtype=FLOAT, trainable=False)\n",
        "\n",
        "        # rTr, cgtol and beta to be used in CG\n",
        "        self.rTr = tf.Variable(0., dtype=FLOAT, trainable=False)\n",
        "        self.cgtol = tf.Variable(0., dtype=FLOAT, trainable=False)\n",
        "        self.beta = tf.Variable(0., dtype=FLOAT, trainable=False)\n",
        "\n",
        "        # placeholder alpha, old_alpha and lambda\n",
        "        self.alpha = tf.compat.v1.placeholder(FLOAT, shape=[])\n",
        "        self.old_alpha = tf.compat.v1.placeholder(FLOAT, shape=[])\n",
        "        self._lambda = tf.compat.v1.placeholder(FLOAT, shape=[])\n",
        "\n",
        "        self.num_grad_segment = math.ceil(self.config.num_data/self.config.bsize)\n",
        "        self.num_Gv_segment = math.ceil(self.config.GNsize/self.config.bsize)\n",
        "\n",
        "        cal_loss, cal_lossgrad, cal_lossGv, \\\n",
        "        add_reg_avg_loss, add_reg_avg_grad, add_reg_avg_Gv, \\\n",
        "        zero_loss, zero_grad, zero_Gv = self._ops_in_minibatch()\n",
        "\n",
        "        # initial operations that will be used in minibatch and newton\n",
        "        self.cal_loss = cal_loss\n",
        "        self.cal_lossgrad = cal_lossgrad\n",
        "        self.cal_lossGv = cal_lossGv\n",
        "        self.add_reg_avg_loss = add_reg_avg_loss\n",
        "        self.add_reg_avg_grad = add_reg_avg_grad\n",
        "        self.add_reg_avg_Gv = add_reg_avg_Gv\n",
        "        self.zero_loss = zero_loss\n",
        "        self.zero_grad = zero_grad\n",
        "        self.zero_Gv = zero_Gv\n",
        "\n",
        "        self.CG, self.update_v = self._CG()\n",
        "        self.init_cg_vars = self._init_cg_vars()\n",
        "        self.update_gs = tf.tensordot(self.s, self.g, axes=1)\n",
        "        self.update_sGs = 0.5*tf.tensordot(self.s, -self.g-self.r-self._lambda*self.s, axes=1)\n",
        "        self.update_model = self._update_model()\n",
        "        self.gnorm = self.calc_norm(self.g)\n",
        "\n",
        "\n",
        "    def vectorize(self, tensors):\n",
        "        if isinstance(tensors, list) or isinstance(tensors, tuple):\n",
        "            vector = [tf.reshape(tensor, [-1]) for tensor in tensors]\n",
        "            return tf.concat(vector, 0) \n",
        "        else:\n",
        "            return tensors \n",
        "    \n",
        "    def inverse_vectorize(self, vector, param):\n",
        "        if isinstance(vector, list):\n",
        "            return vector\n",
        "        else:\n",
        "            tensors = []\n",
        "            offset = 0\n",
        "            num_total_param = np.sum([np.prod(p.shape.as_list()) for p in param])\n",
        "            for p in param:\n",
        "                numel = np.prod(p.shape.as_list())\n",
        "                tensors.append(tf.reshape(vector[offset: offset+numel], p.shape))\n",
        "                offset += numel\n",
        "\n",
        "            assert offset == num_total_param\n",
        "            return tensors\n",
        "\n",
        "    def calc_norm(self, v):\n",
        "        # default: frobenius norm\n",
        "        if isinstance(v, list):\n",
        "            norm = 0.\n",
        "            for p in v:\n",
        "                norm = norm + tf.norm(tensor=p)**2\n",
        "            return norm**0.5\n",
        "        else:\n",
        "            return tf.norm(tensor=v)\n",
        "\n",
        "    def _ops_in_minibatch(self):\n",
        "        \"\"\"\n",
        "        Define operations that will be used in method minibatch\n",
        "        Vectorization is already a deep copy operation.\n",
        "        Before using newton method, loss needs to be summed over training samples\n",
        "        to make results consistent.\n",
        "        \"\"\"\n",
        "\n",
        "        def cal_loss():\n",
        "            return tf.compat.v1.assign(self.f, self.f + self.loss)\n",
        "\n",
        "        def cal_lossgrad():\n",
        "            update_f = tf.compat.v1.assign(self.f, self.f + self.loss)\n",
        "\n",
        "            grad = tf.gradients(ys=self.loss, xs=self.param)\n",
        "            grad = self.vectorize(grad)\n",
        "            update_grad = tf.compat.v1.assign(self.g, self.g + grad)\n",
        "\n",
        "            return tf.group(*[update_f, update_grad])\n",
        "\n",
        "        def cal_lossGv():\n",
        "            v = self.inverse_vectorize(self.v, self.param)\n",
        "            Gv = Gauss_Newton_vec(self.outputs, self.loss, self.param, v)\n",
        "            Gv = self.vectorize(Gv)\n",
        "            return tf.compat.v1.assign(self.Gv, self.Gv + Gv) \n",
        "\n",
        "        # add regularization term to loss, gradient and Gv and further average over batches \n",
        "        def add_reg_avg_loss():\n",
        "            model_weight = self.vectorize(self.param)\n",
        "            reg = (self.calc_norm(model_weight))**2\n",
        "            reg = 1.0/(2*self.config.C) * reg\n",
        "            return tf.compat.v1.assign(self.f, reg + self.f/self.config.num_data)\n",
        "\n",
        "        def add_reg_avg_lossgrad():\n",
        "            model_weight = self.vectorize(self.param)\n",
        "            reg_grad = model_weight/self.config.C\n",
        "            return tf.compat.v1.assign(self.g, reg_grad + self.g/self.config.num_data)\n",
        "\n",
        "        def add_reg_avg_lossGv():\n",
        "            return tf.compat.v1.assign(self.Gv, (self._lambda + 1/self.config.C)*self.v\n",
        "             + self.Gv/self.config.GNsize) \n",
        "\n",
        "        # zero out loss, grad and Gv \n",
        "        def zero_loss():\n",
        "            return tf.compat.v1.assign(self.f, tf.zeros_like(self.f))\n",
        "        def zero_grad():\n",
        "            return tf.compat.v1.assign(self.g, tf.zeros_like(self.g))\n",
        "        def zero_Gv():\n",
        "            return tf.compat.v1.assign(self.Gv, tf.zeros_like(self.Gv))\n",
        "\n",
        "        return (cal_loss(), cal_lossgrad(), cal_lossGv(),\n",
        "                add_reg_avg_loss(), add_reg_avg_lossgrad(), add_reg_avg_lossGv(),\n",
        "                zero_loss(), zero_grad(), zero_Gv())\n",
        "\n",
        "    def minibatch(self, data_batch, place_holder_x, place_holder_y, mode):\n",
        "        \"\"\"\n",
        "        A function to evaluate either function value, global gradient or sub-sampled Gv\n",
        "        \"\"\"\n",
        "        if mode not in ('funonly', 'fungrad', 'Gv'):\n",
        "            raise ValueError('Unknown mode other than funonly & fungrad & Gv!')\n",
        "\n",
        "        inputs, labels = data_batch\n",
        "        num_data = labels.shape[0]\n",
        "        num_segment = math.ceil(num_data/self.config.bsize)\n",
        "        x, y = place_holder_x, place_holder_y\n",
        "\n",
        "        # before estimation starts, need to zero out f, grad and Gv according to the mode\n",
        "\n",
        "        if mode == 'funonly':\n",
        "            assert num_data == self.config.num_data\n",
        "            assert num_segment == self.num_grad_segment\n",
        "            self.sess.run(self.zero_loss)\n",
        "        elif mode == 'fungrad':\n",
        "            assert num_data == self.config.num_data\n",
        "            assert num_segment == self.num_grad_segment\n",
        "            self.sess.run([self.zero_loss, self.zero_grad])\n",
        "        else:\n",
        "            assert num_data == self.config.GNsize\n",
        "            assert num_segment == self.num_Gv_segment\n",
        "            self.sess.run(self.zero_Gv)\n",
        "\n",
        "        for i in range(num_segment):\n",
        "            \n",
        "            load_time = time.time()\n",
        "            idx = np.arange(i * self.config.bsize, min((i+1) * self.config.bsize, num_data))\n",
        "            batch_input = inputs[idx]\n",
        "            batch_labels = labels[idx]\n",
        "            batch_input = np.ascontiguousarray(batch_input)\n",
        "            batch_labels = np.ascontiguousarray(batch_labels)\n",
        "            self.config.elapsed_time += time.time() - load_time\n",
        "\n",
        "            if mode == 'funonly':\n",
        "\n",
        "                self.sess.run(self.cal_loss, feed_dict={\n",
        "                            x: batch_input, \n",
        "                            y: batch_labels,})\n",
        "\n",
        "            elif mode == 'fungrad':\n",
        "                \n",
        "                self.sess.run(self.cal_lossgrad, feed_dict={\n",
        "                            x: batch_input, \n",
        "                            y: batch_labels,})\n",
        "                \n",
        "            else:\n",
        "                \n",
        "                self.sess.run(self.cal_lossGv, feed_dict={\n",
        "                            x: batch_input, \n",
        "                            y: batch_labels})\n",
        "\n",
        "        # average over batches\n",
        "        if mode == 'funonly':\n",
        "            self.sess.run(self.add_reg_avg_loss)\n",
        "        elif mode == 'fungrad':\n",
        "            self.sess.run([self.add_reg_avg_loss, self.add_reg_avg_grad])\n",
        "        else:\n",
        "            self.sess.run(self.add_reg_avg_Gv, \n",
        "                feed_dict={self._lambda: self.config._lambda})\n",
        "\n",
        "\n",
        "    def _update_model(self):\n",
        "        update_model_ops = []\n",
        "        x = self.inverse_vectorize(self.s, self.param)\n",
        "        for i, p in enumerate(self.param):\n",
        "            op = tf.compat.v1.assign(p, p + (self.alpha-self.old_alpha) * x[i])\n",
        "            update_model_ops.append(op)\n",
        "        return tf.group(*update_model_ops)\n",
        "\n",
        "    def _init_cg_vars(self):\n",
        "        init_ops = []\n",
        "\n",
        "        init_r = tf.compat.v1.assign(self.r, -self.g)\n",
        "        init_v = tf.compat.v1.assign(self.v, -self.g)\n",
        "        init_s = tf.compat.v1.assign(self.s, tf.zeros_like(self.g))\n",
        "        gnorm = self.calc_norm(self.g)\n",
        "        init_rTr = tf.compat.v1.assign(self.rTr, gnorm**2)\n",
        "        init_cgtol = tf.compat.v1.assign(self.cgtol, self.config.xi*gnorm)\n",
        "\n",
        "        init_ops = [init_r, init_v, init_s, init_rTr, init_cgtol]\n",
        "\n",
        "        return tf.group(*init_ops)\n",
        "\n",
        "    def _CG(self):\n",
        "        \"\"\"\n",
        "        CG:\n",
        "            define operations that will be used in method newton\n",
        "        Same as the previous loss calculation,\n",
        "        Gv has been summed over batches when samples were fed into Neural Network.\n",
        "        \"\"\"\n",
        "\n",
        "        def CG_ops():\n",
        "            \n",
        "            vGv = tf.tensordot(self.v, self.Gv, axes=1)\n",
        "\n",
        "            alpha = self.rTr / vGv\n",
        "            with tf.control_dependencies([alpha]):\n",
        "                update_s = tf.compat.v1.assign(self.s, self.s + alpha * self.v, name='update_s_ops')\n",
        "                update_r = tf.compat.v1.assign(self.r, self.r - alpha * self.Gv, name='update_r_ops')\n",
        "\n",
        "                with tf.control_dependencies([update_s, update_r]):\n",
        "                    rnewTrnew = self.calc_norm(update_r)**2\n",
        "                    update_beta = tf.compat.v1.assign(self.beta, rnewTrnew / self.rTr)\n",
        "                    with tf.control_dependencies([update_beta]):\n",
        "                        update_rTr = tf.compat.v1.assign(self.rTr, rnewTrnew, name='update_rTr_ops')\n",
        "\n",
        "            return tf.group(*[update_s, update_beta, update_rTr])\n",
        "\n",
        "        def update_v():\n",
        "            return tf.compat.v1.assign(self.v, self.r + self.beta*self.v, name='update_v')\n",
        "\n",
        "        return (CG_ops(), update_v())\n",
        "\n",
        "\n",
        "    def newton(self, full_batch, val_batch, saver, network, test_network=None):\n",
        "        \"\"\"\n",
        "        Conduct newton steps for training\n",
        "        args:\n",
        "            full_batch & val_batch: provide training set and validation set. The function will\n",
        "                save the best model evaluted on validation set for future prediction.\n",
        "            network: a tuple contains (x, y, loss, outputs).\n",
        "            test_network: a tuple similar to argument network. If you use layers which behave differently\n",
        "                in test phase such as batchnorm, a separate test_network is needed.\n",
        "        return:\n",
        "            None\n",
        "        \"\"\"\n",
        "        # check whether data is valid\n",
        "        full_inputs, full_labels = full_batch\n",
        "        assert full_inputs.shape[0] == full_labels.shape[0]\n",
        "\n",
        "        if full_inputs.shape[0] != self.config.num_data:\n",
        "            raise ValueError('The number of full batch inputs does not agree with the config argument.\\\n",
        "                            This is important because global loss is averaged over those inputs')\n",
        "\n",
        "        x, y, _, outputs = network\n",
        "\n",
        "        tf.compat.v1.summary.scalar('loss', self.f)\n",
        "        merged = tf.compat.v1.summary.merge_all()\n",
        "        train_writer = tf.compat.v1.summary.FileWriter('./summary/train', self.sess.graph)\n",
        "\n",
        "        print(self.config.args)\n",
        "        if not self.config.screen_log_only:\n",
        "            log_file = open(self.config.log_file, 'w')\n",
        "            print(self.config.args, file=log_file)\n",
        "        \n",
        "        self.minibatch(full_batch, x, y, mode='fungrad')\n",
        "        f = self.sess.run(self.f)\n",
        "        output_str = 'initial f: {:.3f}'.format(f)\n",
        "        print(output_str)\n",
        "        if not self.config.screen_log_only:\n",
        "            print(output_str, file=log_file)\n",
        "        \n",
        "        best_acc = 0.0\n",
        "\n",
        "        total_running_time = 0.0\n",
        "        self.config.elapsed_time = 0.0\n",
        "        total_CG = 0\n",
        "        \n",
        "        for k in range(self.config.iter_max):\n",
        "\n",
        "            # randomly select the batch for Gv estimation\n",
        "            idx = np.random.choice(np.arange(0, full_labels.shape[0]),\n",
        "                    size=self.config.GNsize, replace=False)\n",
        "\n",
        "            mini_inputs = full_inputs[idx]\n",
        "            mini_labels = full_labels[idx]\n",
        "\n",
        "            start = time.time()\n",
        "\n",
        "            self.sess.run(self.init_cg_vars)\n",
        "            cgtol = self.sess.run(self.cgtol)\n",
        "\n",
        "            avg_cg_time = 0.0\n",
        "            for CGiter in range(1, self.config.CGmax+1):\n",
        "                \n",
        "                cg_time = time.time()\n",
        "                self.minibatch((mini_inputs, mini_labels), x, y, mode='Gv')\n",
        "                avg_cg_time += time.time() - cg_time\n",
        "                \n",
        "                self.sess.run(self.CG)\n",
        "\n",
        "                rnewTrnew = self.sess.run(self.rTr)\n",
        "                \n",
        "                if rnewTrnew**0.5 <= cgtol or CGiter == self.config.CGmax:\n",
        "                    break\n",
        "\n",
        "                self.sess.run(self.update_v)\n",
        "\n",
        "            print('Avg time per Gv iteration: {:.5f} s\\r\\n'.format(avg_cg_time/CGiter))\n",
        "\n",
        "            gs, sGs = self.sess.run([self.update_gs, self.update_sGs], feed_dict={\n",
        "                    self._lambda: self.config._lambda\n",
        "                })\n",
        "            \n",
        "            # line_search\n",
        "            f_old = f\n",
        "            alpha = 1\n",
        "            while True:\n",
        "\n",
        "                old_alpha = 0 if alpha == 1 else alpha/0.5\n",
        "                \n",
        "                self.sess.run(self.update_model, feed_dict={\n",
        "                    self.alpha:alpha, self.old_alpha:old_alpha\n",
        "                    })\n",
        "\n",
        "                prered = alpha*gs + (alpha**2)*sGs\n",
        "\n",
        "                self.minibatch(full_batch, x, y, mode='funonly')\n",
        "                f = self.sess.run(self.f)\n",
        "\n",
        "                actred = f - f_old\n",
        "\n",
        "                if actred <= self.config.eta*alpha*gs:\n",
        "                    break\n",
        "\n",
        "                alpha *= 0.5\n",
        "\n",
        "            # update lambda\n",
        "            ratio = actred / prered\n",
        "            if ratio < 0.25:\n",
        "                self.config._lambda *= self.config.boost\n",
        "            elif ratio >= 0.75:\n",
        "                self.config._lambda *= self.config.drop\n",
        "\n",
        "            self.minibatch(full_batch, x, y, mode='fungrad')\n",
        "            f = self.sess.run(self.f)\n",
        "\n",
        "            gnorm = self.sess.run(self.gnorm)\n",
        "\n",
        "            summary = self.sess.run(merged)\n",
        "            train_writer.add_summary(summary, k)\n",
        "\n",
        "            # exclude data loading time for fair comparison\n",
        "            end = time.time() \n",
        "            \n",
        "            end = end - self.config.elapsed_time\n",
        "            total_running_time += end-start\n",
        "\n",
        "            self.config.elapsed_time = 0.0\n",
        "            \n",
        "            total_CG += CGiter\n",
        "\n",
        "            output_str = '{}-iter f: {:.3f} |g|: {:.5f} alpha: {:.3e} ratio: {:.3f} lambda: {:.5f} #CG: {} actred: {:.5f} prered: {:.5f} time: {:.3f}'.\\\n",
        "                            format(k, f, gnorm, alpha, actred/prered, self.config._lambda, CGiter, actred, prered, end-start)\n",
        "            print(output_str)\n",
        "            if not self.config.screen_log_only:\n",
        "                print(output_str, file=log_file)\n",
        "\n",
        "            if val_batch is not None:\n",
        "                # Evaluate the performance after every Newton Step\n",
        "                if test_network == None:\n",
        "                    val_loss, val_acc, _ = predict(\n",
        "                        self.sess, \n",
        "                        network=(x, y, self.loss, outputs),\n",
        "                        test_batch=val_batch,\n",
        "                        bsize=self.config.bsize,\n",
        "                        )\n",
        "                else:\n",
        "                    # A separat test network part has not been done...\n",
        "                    val_loss, val_acc, _ = predict(\n",
        "                        self.sess, \n",
        "                        network=test_network,\n",
        "                        test_batch=val_batch,\n",
        "                        bsize=self.config.bsize\n",
        "                        )\n",
        "\n",
        "                output_str = '\\r\\n {}-iter val_acc: {:.3f}% val_loss {:.3f}\\r\\n'.\\\n",
        "                    format(k, val_acc*100, val_loss)\n",
        "                print(output_str)\n",
        "                if not self.config.screen_log_only:\n",
        "                    print(output_str, file=log_file)\n",
        "\n",
        "                if val_acc > best_acc:\n",
        "                    best_acc = val_acc\n",
        "                    checkpoint_path = self.config.model_file\n",
        "                    save_path = saver.save(self.sess, checkpoint_path)\n",
        "                    print('Best model saved in {}\\r\\n'.format(save_path))\n",
        "\n",
        "        if val_batch is None:\n",
        "            checkpoint_path = self.config.model_file\n",
        "            save_path = saver.save(self.sess, checkpoint_path)\n",
        "            print('Model at the last iteration saved in {}\\r\\n'.format(save_path))\n",
        "            output_str = 'total_#CG {} | total running time {:.3f}s'.format(total_CG, total_running_time)\n",
        "        else:\n",
        "            output_str = 'Final acc: {:.3f}% | best acc {:.3f}% | total_#CG {} | total running time {:.3f}s'.\\\n",
        "                format(val_acc*100, best_acc*100, total_CG, total_running_time)\n",
        "        print(output_str)\n",
        "        if not self.config.screen_log_only:\n",
        "            print(output_str, file=log_file)\n",
        "            log_file.close()"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xatdN2Zm7BOQ"
      },
      "source": [
        "##Set Train Arguments##"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v_DZPFd4m7Og"
      },
      "source": [
        "if USE_HFO:\n",
        "    # Arguments for HFO - PSSP dataset\n",
        "    train_args = (\"--optim NewtonCG --GNsize 200 --C 0.05 --net CNN_4layers --bsize 1024 --iter_max 50 \" +\n",
        "              \"--train_set \" + TRAIN_FILE + \" --val_set \" + VALID_FILE + \" --dim \" + \n",
        "              str(HEIGHT) + \" \" + str(WIDTH) + \" \" + str(DEPTH)).split()\n",
        "else:\n",
        "    # Arguments for SGD - PSSP dataset\n",
        "    train_args = (\"--optim SGD --lr 0.05 --momentum 0.01 --C 0.01 --net CNN_4layers --bsize 1024 --epoch_max 1000 \" +\n",
        "              \"--train_set \" + TRAIN_FILE + \" --val_set \" + VALID_FILE + \" --dim \" +\n",
        "              str(HEIGHT) + \" \" + str(WIDTH) + \" \" + str(DEPTH)).split()"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vMCxn-8P5tsH"
      },
      "source": [
        "##Declare Train Function##"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cr528VD1EDj9"
      },
      "source": [
        "# import pdb\n",
        "# import numpy as np\n",
        "# import tensorflow as tf\n",
        "# tf.compat.v1.disable_eager_execution()\n",
        "# import time\n",
        "# import math\n",
        "# import argparse\n",
        "\n",
        "# from net.net import CNN\n",
        "# from newton_cg import newton_cg\n",
        "# from utilities import read_data, predict, ConfigClass, normalize_and_reshape\n",
        "\n",
        "def parse_args():\n",
        "    parser = argparse.ArgumentParser(description='Newton method on DNN')\n",
        "    parser.add_argument('--C', dest='C',\n",
        "                      help='regularization term, or so-called weight decay where'+\\\n",
        "                              'weight_decay = lr/(C*num_of_samples) in this implementation' ,\n",
        "                      default=0.01, type=float)\n",
        "\n",
        "    # Newton method arguments\n",
        "    parser.add_argument('--GNsize', dest='GNsize',\n",
        "                      help='number of samples for estimating Gauss-Newton matrix',\n",
        "                      default=4096, type=int)\n",
        "    parser.add_argument('--iter_max', dest='iter_max',\n",
        "                      help='the maximal number of Newton iterations',\n",
        "                      default=100, type=int)\n",
        "    parser.add_argument('--xi', dest='xi',\n",
        "                      help='the tolerance in the relative stopping condition for CG',\n",
        "                      default=0.1, type=float)\n",
        "    parser.add_argument('--drop', dest='drop',\n",
        "                      help='the drop constants for the LM method',\n",
        "                      default=2/3, type=float)\n",
        "    parser.add_argument('--boost', dest='boost',\n",
        "                      help='the boost constants for the LM method',\n",
        "                      default=3/2, type=float)\n",
        "    parser.add_argument('--eta', dest='eta',\n",
        "                      help='the parameter for the line search stopping condition',\n",
        "                      default=0.0001, type=float)\n",
        "    parser.add_argument('--CGmax', dest='CGmax',\n",
        "                      help='the maximal number of CG iterations',\n",
        "                      default=250, type=int)\n",
        "    parser.add_argument('--lambda', dest='_lambda',\n",
        "                      help='the initial lambda for the LM method',\n",
        "                      default=1, type=float)\n",
        "\n",
        "    # SGD arguments\n",
        "    parser.add_argument('--epoch_max', dest='epoch',\n",
        "                      help='number of training epoch',\n",
        "                      default=500, type=int)\n",
        "    parser.add_argument('--lr', dest='lr',\n",
        "                      help='learning rate',\n",
        "                      default=0.01, type=float)\n",
        "    parser.add_argument('--decay', dest='lr_decay',\n",
        "                      help='learning rate decay over each mini-batch update',\n",
        "                      default=0, type=float)\n",
        "    parser.add_argument('--momentum', dest='momentum',\n",
        "                      help='momentum of learning',\n",
        "                      default=0, type=float)\n",
        "\n",
        "    # Model training arguments\n",
        "    parser.add_argument('--bsize', dest='bsize',\n",
        "                      help='batch size to evaluate stochastic gradient, Gv, etc. Since the sampled data \\\n",
        "                      for computing Gauss-Newton matrix and etc. might not fit into memeory \\\n",
        "                      for one time, we will split the data into several segements and average\\\n",
        "                      over them.',\n",
        "                      default=1024, type=int)\n",
        "    parser.add_argument('--net', dest='net',\n",
        "                      help='classifier type',\n",
        "                      default='CNN_4layers', type=str)\n",
        "    parser.add_argument('--train_set', dest='train_set',\n",
        "                      help='provide the directory of .mat file for training',\n",
        "                      default=None, type=str)\n",
        "    parser.add_argument('--val_set', dest='val_set',\n",
        "                      help='provide the directory of .mat file for validation',\n",
        "                      default=None, type=str)\n",
        "    parser.add_argument('--model', dest='model_file',\n",
        "                      help='model saving address',\n",
        "                      default='./saved_model/model.ckpt', type=str)\n",
        "    parser.add_argument('--log', dest='log_file',\n",
        "                      help='log saving directory',\n",
        "                      default='./running_log/logger.log', type=str)\n",
        "    parser.add_argument('--screen_log_only', dest='screen_log_only',\n",
        "                      help='screen printing running log instead of storing it',\n",
        "                      action='store_true')\n",
        "    parser.add_argument('--optim', '-optim', \n",
        "                      help='which optimizer to use: SGD, Adam or NewtonCG',\n",
        "                      default='NewtonCG', type=str)\n",
        "    parser.add_argument('--loss', dest='loss', \n",
        "                      help='which loss function to use: MSELoss or CrossEntropy',\n",
        "                      default='MSELoss', type=str)\n",
        "    parser.add_argument('--dim', dest='dim', nargs='+', help='input dimension of data,'+\\\n",
        "                        'shape must be:  height width num_channels',\n",
        "                      default=[32, 32, 3], type=int)\n",
        "    parser.add_argument('--seed', dest='seed', help='a nonnegative integer for \\\n",
        "                        reproducibility', type=int)     \n",
        "    \n",
        "    args = parser.parse_args(args=train_args)\n",
        "    return args\n",
        "\n",
        "\n",
        "args = parse_args()\n",
        "\n",
        "def init_model(param):\n",
        "    init_ops = []\n",
        "    for p in param:\n",
        "        if 'kernel' in p.name:\n",
        "            weight = np.random.standard_normal(p.shape)* np.sqrt(2.0 / ((np.prod(p.get_shape().as_list()[:-1]))))\n",
        "            opt = tf.compat.v1.assign(p, weight)\n",
        "        elif 'bias' in p.name:\n",
        "            zeros = np.zeros(p.shape)\n",
        "            opt = tf.compat.v1.assign(p, zeros)\n",
        "        init_ops.append(opt)\n",
        "    return tf.group(*init_ops)\n",
        "\n",
        "def gradient_trainer(config, sess, network, full_batch, val_batch, saver, test_network):\n",
        "    x, y, loss, outputs,  = network\n",
        "    \n",
        "    global_step = tf.Variable(initial_value=0, trainable=False, name='global_step')\n",
        "    learning_rate = tf.compat.v1.placeholder(tf.float32, shape=[], name='learning_rate')\n",
        "\n",
        "    # Probably not a good way to add regularization.\n",
        "    # Just to confirm the implementation is the same as MATLAB.\n",
        "    reg = 0.0\n",
        "    param = tf.compat.v1.trainable_variables()\n",
        "    for p in param:\n",
        "        reg = reg + tf.reduce_sum(input_tensor=tf.pow(p,2))\n",
        "    reg_const = 1/(2*config.C)\n",
        "    batch_size = tf.compat.v1.cast(tf.shape(x)[0], tf.float32)\n",
        "    loss_with_reg = reg_const*reg + loss/batch_size\n",
        "\n",
        "    if config.optim == 'SGD':\n",
        "        optimizer = tf.compat.v1.train.MomentumOptimizer(\n",
        "                    learning_rate=learning_rate, \n",
        "                    momentum=config.momentum).minimize(\n",
        "                    loss_with_reg, \n",
        "                    global_step=global_step)\n",
        "    elif config.optim == 'Adam':\n",
        "        optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate,\n",
        "                                beta1=0.9,\n",
        "                                beta2=0.999,\n",
        "                                epsilon=1e-08).minimize(\n",
        "                                loss_with_reg, \n",
        "                                global_step=global_step)\n",
        "\n",
        "    train_inputs, train_labels = full_batch\n",
        "    num_data = train_labels.shape[0]\n",
        "    num_iters = math.ceil(num_data/config.bsize)\n",
        "\n",
        "    print(config.args)\n",
        "    if not config.screen_log_only:\n",
        "        log_file = open(config.log_file, 'w')\n",
        "        print(config.args, file=log_file)\n",
        "    sess.run(tf.compat.v1.global_variables_initializer())\n",
        "    \n",
        "\n",
        "    print('-------------- initializing network by methods in He et al. (2015) --------------')\n",
        "    param = tf.compat.v1.trainable_variables()\n",
        "    sess.run(init_model(param))\n",
        "\n",
        "    total_running_time = 0.0\n",
        "    best_acc = 0.0\n",
        "    lr = config.lr\n",
        "\n",
        "    for epoch in range(0, args.epoch):\n",
        "        \n",
        "        loss_avg = 0.0\n",
        "        start = time.time()\n",
        "\n",
        "        for i in range(num_iters):\n",
        "            \n",
        "            load_time = time.time()\n",
        "            # randomly select the batch\n",
        "            idx = np.random.choice(np.arange(0, num_data), \n",
        "                    size=config.bsize, replace=False)\n",
        "\n",
        "            batch_input = train_inputs[idx]\n",
        "            batch_labels = train_labels[idx]\n",
        "            batch_input = np.ascontiguousarray(batch_input)\n",
        "            batch_labels = np.ascontiguousarray(batch_labels)\n",
        "            config.elapsed_time += time.time() - load_time\n",
        "\n",
        "            step, _, batch_loss= sess.run(\n",
        "                [global_step, optimizer, loss_with_reg],\n",
        "                feed_dict = {x: batch_input, y: batch_labels, learning_rate: lr}\n",
        "                )\n",
        "\n",
        "            # print initial loss\n",
        "            if epoch == 0 and i == 0:\n",
        "                output_str = 'initial f (reg + avg. loss of 1st batch): {:.3f}'.format(batch_loss)\n",
        "                print(output_str)\n",
        "                if not config.screen_log_only:\n",
        "                    print(output_str, file=log_file)\n",
        "\n",
        "            loss_avg = loss_avg + batch_loss\n",
        "            # print log every 10% of the iterations\n",
        "            if i % math.ceil(num_iters/10) == 0:\n",
        "                end = time.time()\n",
        "                output_str = 'Epoch {}: {}/{} | loss {:.4f} | lr {:.6} | elapsed time {:.3f}'\\\n",
        "                    .format(epoch, i, num_iters, batch_loss , lr, end-start)\n",
        "                print(output_str)\n",
        "                if not config.screen_log_only:\n",
        "                    print(output_str, file=log_file)\n",
        "            \n",
        "            # adjust learning rate for SGD by inverse time decay\n",
        "            if args.optim != 'Adam':\n",
        "                lr = config.lr/(1 + args.lr_decay*step)\n",
        "\n",
        "        # exclude data loading time for fair comparison\n",
        "        epoch_end = time.time() - config.elapsed_time\n",
        "        total_running_time += epoch_end - start\n",
        "        config.elapsed_time = 0.0\n",
        "        \n",
        "        if val_batch is None:\n",
        "            output_str = 'In epoch {} train loss: {:.3f} | epoch time {:.3f}'\\\n",
        "                .format(epoch, loss_avg/(i+1), epoch_end-start)            \n",
        "        else:\n",
        "            if test_network == None:\n",
        "                val_loss, val_acc, _ = predict(\n",
        "                    sess, \n",
        "                    network=(x, y, loss, outputs),\n",
        "                    test_batch=val_batch,\n",
        "                    bsize=config.bsize\n",
        "                    )\n",
        "            else:\n",
        "                # A separat test network part have been done...\n",
        "                val_loss, val_acc, _ = predict(\n",
        "                    sess, \n",
        "                    network=test_network,\n",
        "                    test_batch=val_batch,\n",
        "                    bsize=config.bsize\n",
        "                    )\n",
        "            \n",
        "            output_str = 'In epoch {} train loss: {:.3f} | val loss: {:.3f} | val accuracy: {:.3f}% | epoch time {:.3f}'\\\n",
        "                .format(epoch, loss_avg/(i+1), val_loss, val_acc*100, epoch_end-start)\n",
        "        \n",
        "            if val_acc > best_acc:\n",
        "                best_acc = val_acc\n",
        "                checkpoint_path = config.model_file \n",
        "                save_path = saver.save(sess, checkpoint_path)\n",
        "                print('Saved best model in {}'.format(save_path))\n",
        "\n",
        "        print(output_str)\n",
        "        if not config.screen_log_only:\n",
        "            print(output_str, file=log_file)\n",
        "\n",
        "    if val_batch is None:\n",
        "        checkpoint_path = config.model_file \n",
        "        save_path = saver.save(sess, checkpoint_path)\n",
        "        print('Model at the last iteration saved in {}\\r\\n'.format(save_path))\n",
        "        output_str = 'total running time {:.3f}s'.format(total_running_time)\n",
        "    else:\n",
        "        output_str = 'Final acc: {:.3f}% | best acc {:.3f}% | total running time {:.3f}s'\\\n",
        "            .format(val_acc*100, best_acc*100, total_running_time)\n",
        "    \n",
        "    print(output_str)\n",
        "    if not config.screen_log_only:\n",
        "        print(output_str, file=log_file)\n",
        "        log_file.close()\n",
        "\n",
        "def newton_trainer(config, sess, network, full_batch, val_batch, saver, test_network):\n",
        "    _, _, loss, outputs = network\n",
        "    newton_solver = newton_cg(config, sess, outputs, loss)\n",
        "    sess.run(tf.compat.v1.global_variables_initializer())\n",
        "\n",
        "    print('-------------- initializing network by methods in He et al. (2015) --------------')\n",
        "    param = tf.compat.v1.trainable_variables()\n",
        "    sess.run(init_model(param))\n",
        "    newton_solver.newton(full_batch, val_batch, saver, network, test_network)\n",
        "\n",
        "\n",
        "def train_model():\n",
        "    full_batch, num_cls, label_enum = read_data(filename=args.train_set, dim=args.dim)\n",
        "    \n",
        "    if args.val_set is None:\n",
        "        print('No validation set is provided. Will output model at the last iteration.')\n",
        "        val_batch = None\n",
        "    else:\n",
        "        val_batch, _, _ = read_data(filename=args.val_set, dim=args.dim, label_enum=label_enum)\n",
        "\n",
        "    num_data = full_batch[0].shape[0]\n",
        "    \n",
        "    config = ConfigClass(args, num_data, num_cls)\n",
        "\n",
        "    if isinstance(config.seed, int):\n",
        "        tf.compat.v1.random.set_random_seed(config.seed)\n",
        "        np.random.seed(config.seed)\n",
        "\n",
        "    if config.net in ('CNN_4layers', 'CNN_7layers', 'VGG11', 'VGG13', 'VGG16','VGG19'):\n",
        "        x, y, outputs = CNN(config.net, num_cls, config.dim)\n",
        "        test_network = None\n",
        "    else:\n",
        "        raise ValueError('Unrecognized training model')\n",
        "\n",
        "    if config.loss == 'MSELoss':\n",
        "        loss = tf.reduce_sum(input_tensor=tf.pow(outputs-y, 2))\n",
        "    else:\n",
        "        loss = tf.reduce_sum(input_tensor=tf.nn.softmax_cross_entropy_with_logits(logits=outputs, labels=y))\n",
        "    \n",
        "    network = (x, y, loss, outputs)\n",
        "\n",
        "    sess_config = tf.compat.v1.ConfigProto()\n",
        "    sess_config.gpu_options.allow_growth = True\n",
        "\n",
        "    with tf.compat.v1.Session(config=sess_config) as sess:\n",
        "        \n",
        "        full_batch[0], mean_tr = normalize_and_reshape(full_batch[0], dim=config.dim, mean_tr=None)\n",
        "        if val_batch is not None:\n",
        "            val_batch[0], _ = normalize_and_reshape(val_batch[0], dim=config.dim, mean_tr=mean_tr)\n",
        "\n",
        "        param = tf.compat.v1.trainable_variables()\n",
        "\n",
        "        mean_param = tf.compat.v1.get_variable(name='mean_tr', initializer=mean_tr, trainable=False, \n",
        "                    validate_shape=True, use_resource=False)\n",
        "        label_enum_var=tf.compat.v1.get_variable(name='label_enum', initializer=label_enum, trainable=False,\n",
        "                    validate_shape=True, use_resource=False)\n",
        "        saver = tf.compat.v1.train.Saver(var_list=param+[mean_param])\n",
        "        \n",
        "        if config.optim in ('SGD', 'Adam'):\n",
        "            gradient_trainer(\n",
        "                config, sess, network, full_batch, val_batch, saver, test_network)\n",
        "        elif config.optim == 'NewtonCG':\n",
        "            newton_trainer(\n",
        "                config, sess, network, full_batch, val_batch, saver, test_network=test_network)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wi6UglYiD7Zd"
      },
      "source": [
        "## Train ##"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cSgKfh4r5lu7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b97b5797-8ed6-4172-e45d-87c23a7271d4"
      },
      "source": [
        "train_model()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "You choose not to specify a random seed.A different result is produced after each run.\n",
            "Saving log to: ./running_log/logger.log\n",
            "-------------- initializing network by methods in He et al. (2015) --------------\n",
            "Namespace(C=0.05, CGmax=250, GNsize=200, _lambda=1, boost=1.5, bsize=1024, dim=[32, 32, 1], drop=0.6666666666666666, epoch=500, eta=0.0001, iter_max=50, log_file='./running_log/logger.log', loss='MSELoss', lr=0.01, lr_decay=0, model_file='./saved_model/model.ckpt', momentum=0, net='CNN_4layers', optim='NewtonCG', screen_log_only=False, seed=None, train_set='/content/drive/MyDrive/Datasets/cb513_protbert_trainSet8.mat', val_set='/content/drive/MyDrive/Datasets/cb513_protbert_testSet8.mat', xi=0.1)\n",
            "initial f: 1.413\n",
            "Avg time per Gv iteration: 1.44111 s\n",
            "\n",
            "0-iter f: 0.786 |g|: 5.03568 alpha: 1.000e+00 ratio: 0.977 lambda: 0.66667 #CG: 1 actred: -0.62740 prered: -0.64203 time: 23.515\n",
            "\n",
            " 0-iter val_acc: 44.230% val_loss 0.668\n",
            "\n",
            "Best model saved in ./saved_model/model.ckpt\n",
            "\n",
            "Avg time per Gv iteration: 0.09628 s\n",
            "\n",
            "1-iter f: 0.586 |g|: 8.69784 alpha: 1.250e-01 ratio: 0.337 lambda: 0.66667 #CG: 24 actred: -0.19925 prered: -0.59091 time: 41.020\n",
            "\n",
            " 1-iter val_acc: 69.356% val_loss 0.488\n",
            "\n",
            "Best model saved in ./saved_model/model.ckpt\n",
            "\n",
            "Avg time per Gv iteration: 0.09640 s\n",
            "\n",
            "2-iter f: 0.583 |g|: 13.66160 alpha: 1.250e-01 ratio: 0.011 lambda: 1.00000 #CG: 19 actred: -0.00341 prered: -0.30165 time: 40.630\n",
            "\n",
            " 2-iter val_acc: 72.862% val_loss 0.479\n",
            "\n",
            "Best model saved in ./saved_model/model.ckpt\n",
            "\n",
            "Avg time per Gv iteration: 0.09629 s\n",
            "\n",
            "3-iter f: 0.500 |g|: 5.82325 alpha: 1.250e-01 ratio: 0.353 lambda: 1.00000 #CG: 14 actred: -0.08306 prered: -0.23504 time: 40.035\n",
            "\n",
            " 3-iter val_acc: 75.020% val_loss 0.403\n",
            "\n",
            "Best model saved in ./saved_model/model.ckpt\n",
            "\n",
            "Avg time per Gv iteration: 0.09630 s\n",
            "\n",
            "4-iter f: 0.494 |g|: 10.92167 alpha: 2.500e-01 ratio: 0.049 lambda: 1.50000 #CG: 18 actred: -0.00597 prered: -0.12074 time: 34.927\n",
            "\n",
            " 4-iter val_acc: 76.548% val_loss 0.399\n",
            "\n",
            "Best model saved in ./saved_model/model.ckpt\n",
            "\n",
            "Avg time per Gv iteration: 0.09647 s\n",
            "\n",
            "5-iter f: 0.494 |g|: 10.74030 alpha: 2.500e-01 ratio: 0.001 lambda: 2.25000 #CG: 12 actred: -0.00015 prered: -0.13351 time: 34.264\n",
            "\n",
            " 5-iter val_acc: 76.166% val_loss 0.406\n",
            "\n",
            "Avg time per Gv iteration: 0.09624 s\n",
            "\n",
            "6-iter f: 0.460 |g|: 1.80137 alpha: 2.500e-01 ratio: 0.437 lambda: 2.25000 #CG: 11 actred: -0.03373 prered: -0.07713 time: 34.238\n",
            "\n",
            " 6-iter val_acc: 78.009% val_loss 0.367\n",
            "\n",
            "Best model saved in ./saved_model/model.ckpt\n",
            "\n",
            "Avg time per Gv iteration: 0.09644 s\n",
            "\n",
            "7-iter f: 0.457 |g|: 1.82162 alpha: 5.000e-01 ratio: 0.046 lambda: 3.37500 #CG: 14 actred: -0.00269 prered: -0.05854 time: 28.878\n",
            "\n",
            " 7-iter val_acc: 77.750% val_loss 0.364\n",
            "\n",
            "Avg time per Gv iteration: 0.09615 s\n",
            "\n",
            "8-iter f: 0.441 |g|: 1.21870 alpha: 5.000e-01 ratio: 0.328 lambda: 3.37500 #CG: 14 actred: -0.01604 prered: -0.04894 time: 28.927\n",
            "\n",
            " 8-iter val_acc: 78.335% val_loss 0.349\n",
            "\n",
            "Best model saved in ./saved_model/model.ckpt\n",
            "\n",
            "Avg time per Gv iteration: 0.09627 s\n",
            "\n",
            "9-iter f: 0.437 |g|: 1.60087 alpha: 5.000e-01 ratio: 0.236 lambda: 5.06250 #CG: 12 actred: -0.00489 prered: -0.02073 time: 28.683\n",
            "\n",
            " 9-iter val_acc: 78.492% val_loss 0.346\n",
            "\n",
            "Best model saved in ./saved_model/model.ckpt\n",
            "\n",
            "Avg time per Gv iteration: 0.09620 s\n",
            "\n",
            "10-iter f: 0.431 |g|: 1.74831 alpha: 5.000e-01 ratio: 0.394 lambda: 5.06250 #CG: 10 actred: -0.00526 prered: -0.01335 time: 28.522\n",
            "\n",
            " 10-iter val_acc: 78.503% val_loss 0.341\n",
            "\n",
            "Best model saved in ./saved_model/model.ckpt\n",
            "\n",
            "Avg time per Gv iteration: 0.09630 s\n",
            "\n",
            "11-iter f: 0.428 |g|: 1.08708 alpha: 5.000e-01 ratio: 0.539 lambda: 5.06250 #CG: 8 actred: -0.00305 prered: -0.00565 time: 28.282\n",
            "\n",
            " 11-iter val_acc: 78.773% val_loss 0.337\n",
            "\n",
            "Best model saved in ./saved_model/model.ckpt\n",
            "\n",
            "Avg time per Gv iteration: 0.09621 s\n",
            "\n",
            "12-iter f: 0.427 |g|: 3.19160 alpha: 1.000e+00 ratio: 0.180 lambda: 7.59375 #CG: 8 actred: -0.00112 prered: -0.00620 time: 22.735\n",
            "\n",
            " 12-iter val_acc: 78.616% val_loss 0.338\n",
            "\n",
            "Avg time per Gv iteration: 0.09643 s\n",
            "\n",
            "13-iter f: 0.424 |g|: 0.67692 alpha: 5.000e-01 ratio: 0.516 lambda: 7.59375 #CG: 7 actred: -0.00338 prered: -0.00655 time: 28.189\n",
            "\n",
            " 13-iter val_acc: 78.930% val_loss 0.335\n",
            "\n",
            "Best model saved in ./saved_model/model.ckpt\n",
            "\n",
            "Avg time per Gv iteration: 0.09640 s\n",
            "\n",
            "14-iter f: 0.422 |g|: 0.87305 alpha: 1.000e+00 ratio: 0.535 lambda: 7.59375 #CG: 8 actred: -0.00186 prered: -0.00347 time: 22.750\n",
            "\n",
            " 14-iter val_acc: 79.054% val_loss 0.334\n",
            "\n",
            "Best model saved in ./saved_model/model.ckpt\n",
            "\n",
            "Avg time per Gv iteration: 0.09617 s\n",
            "\n",
            "15-iter f: 0.421 |g|: 1.28440 alpha: 1.000e+00 ratio: 0.299 lambda: 7.59375 #CG: 8 actred: -0.00124 prered: -0.00414 time: 22.711\n",
            "\n",
            " 15-iter val_acc: 79.177% val_loss 0.333\n",
            "\n",
            "Best model saved in ./saved_model/model.ckpt\n",
            "\n",
            "Avg time per Gv iteration: 0.09631 s\n",
            "\n",
            "16-iter f: 0.420 |g|: 2.41936 alpha: 1.000e+00 ratio: 0.052 lambda: 11.39062 #CG: 8 actred: -0.00031 prered: -0.00605 time: 22.755\n",
            "\n",
            " 16-iter val_acc: 79.076% val_loss 0.334\n",
            "\n",
            "Avg time per Gv iteration: 0.09629 s\n",
            "\n",
            "17-iter f: 0.420 |g|: 3.04619 alpha: 1.000e+00 ratio: 0.003 lambda: 17.08594 #CG: 7 actred: -0.00002 prered: -0.00884 time: 22.611\n",
            "\n",
            " 17-iter val_acc: 79.099% val_loss 0.335\n",
            "\n",
            "Avg time per Gv iteration: 0.09644 s\n",
            "\n",
            "18-iter f: 0.417 |g|: 1.36899 alpha: 1.000e+00 ratio: 0.566 lambda: 17.08594 #CG: 4 actred: -0.00328 prered: -0.00580 time: 22.359\n",
            "\n",
            " 18-iter val_acc: 79.166% val_loss 0.332\n",
            "\n",
            "Avg time per Gv iteration: 0.09625 s\n",
            "\n",
            "19-iter f: 0.416 |g|: 0.61870 alpha: 1.000e+00 ratio: 0.757 lambda: 11.39062 #CG: 5 actred: -0.00115 prered: -0.00151 time: 22.420\n",
            "\n",
            " 19-iter val_acc: 79.222% val_loss 0.331\n",
            "\n",
            "Best model saved in ./saved_model/model.ckpt\n",
            "\n",
            "Avg time per Gv iteration: 0.09622 s\n",
            "\n",
            "20-iter f: 0.415 |g|: 0.38054 alpha: 1.000e+00 ratio: 0.895 lambda: 7.59375 #CG: 7 actred: -0.00115 prered: -0.00129 time: 22.651\n",
            "\n",
            " 20-iter val_acc: 79.267% val_loss 0.330\n",
            "\n",
            "Best model saved in ./saved_model/model.ckpt\n",
            "\n",
            "Avg time per Gv iteration: 0.09623 s\n",
            "\n",
            "21-iter f: 0.413 |g|: 0.40679 alpha: 1.000e+00 ratio: 0.940 lambda: 5.06250 #CG: 7 actred: -0.00154 prered: -0.00164 time: 22.611\n",
            "\n",
            " 21-iter val_acc: 79.267% val_loss 0.329\n",
            "\n",
            "Avg time per Gv iteration: 0.09629 s\n",
            "\n",
            "22-iter f: 0.411 |g|: 1.10408 alpha: 1.000e+00 ratio: 0.797 lambda: 3.37500 #CG: 8 actred: -0.00188 prered: -0.00236 time: 22.748\n",
            "\n",
            " 22-iter val_acc: 79.279% val_loss 0.329\n",
            "\n",
            "Best model saved in ./saved_model/model.ckpt\n",
            "\n",
            "Avg time per Gv iteration: 0.09619 s\n",
            "\n",
            "23-iter f: 0.410 |g|: 2.49395 alpha: 1.000e+00 ratio: 0.327 lambda: 3.37500 #CG: 8 actred: -0.00138 prered: -0.00424 time: 22.712\n",
            "\n",
            " 23-iter val_acc: 79.380% val_loss 0.329\n",
            "\n",
            "Best model saved in ./saved_model/model.ckpt\n",
            "\n",
            "Avg time per Gv iteration: 0.09625 s\n",
            "\n",
            "24-iter f: 0.408 |g|: 0.56986 alpha: 2.500e-01 ratio: 0.421 lambda: 3.37500 #CG: 9 actred: -0.00180 prered: -0.00428 time: 34.033\n",
            "\n",
            " 24-iter val_acc: 79.245% val_loss 0.328\n",
            "\n",
            "Avg time per Gv iteration: 0.09619 s\n",
            "\n",
            "25-iter f: 0.407 |g|: 0.88479 alpha: 5.000e-01 ratio: 0.444 lambda: 3.37500 #CG: 12 actred: -0.00126 prered: -0.00284 time: 28.685\n",
            "\n",
            " 25-iter val_acc: 79.492% val_loss 0.327\n",
            "\n",
            "Best model saved in ./saved_model/model.ckpt\n",
            "\n",
            "Avg time per Gv iteration: 0.09620 s\n",
            "\n",
            "26-iter f: 0.406 |g|: 2.00765 alpha: 1.000e+00 ratio: 0.117 lambda: 5.06250 #CG: 11 actred: -0.00061 prered: -0.00521 time: 23.038\n",
            "\n",
            " 26-iter val_acc: 79.312% val_loss 0.329\n",
            "\n",
            "Avg time per Gv iteration: 0.09623 s\n",
            "\n",
            "27-iter f: 0.404 |g|: 1.18500 alpha: 5.000e-01 ratio: 0.398 lambda: 5.06250 #CG: 7 actred: -0.00233 prered: -0.00585 time: 28.186\n",
            "\n",
            " 27-iter val_acc: 79.380% val_loss 0.327\n",
            "\n",
            "Avg time per Gv iteration: 0.09633 s\n",
            "\n",
            "28-iter f: 0.403 |g|: 0.66342 alpha: 5.000e-01 ratio: 0.526 lambda: 5.06250 #CG: 8 actred: -0.00124 prered: -0.00235 time: 28.348\n",
            "\n",
            " 28-iter val_acc: 79.425% val_loss 0.326\n",
            "\n",
            "Avg time per Gv iteration: 0.09623 s\n",
            "\n",
            "29-iter f: 0.402 |g|: 1.33719 alpha: 1.000e+00 ratio: 0.456 lambda: 5.06250 #CG: 9 actred: -0.00109 prered: -0.00239 time: 22.799\n",
            "\n",
            " 29-iter val_acc: 79.402% val_loss 0.326\n",
            "\n",
            "Avg time per Gv iteration: 0.09615 s\n",
            "\n",
            "30-iter f: 0.400 |g|: 0.80460 alpha: 5.000e-01 ratio: 0.480 lambda: 5.06250 #CG: 8 actred: -0.00110 prered: -0.00229 time: 28.346\n",
            "\n",
            " 30-iter val_acc: 79.470% val_loss 0.326\n",
            "\n",
            "Avg time per Gv iteration: 0.09622 s\n",
            "\n",
            "31-iter f: 0.400 |g|: 0.84758 alpha: 5.000e-01 ratio: 0.514 lambda: 5.06250 #CG: 8 actred: -0.00076 prered: -0.00148 time: 28.288\n",
            "\n",
            " 31-iter val_acc: 79.593% val_loss 0.325\n",
            "\n",
            "Best model saved in ./saved_model/model.ckpt\n",
            "\n",
            "Avg time per Gv iteration: 0.09631 s\n",
            "\n",
            "32-iter f: 0.400 |g|: 2.22527 alpha: 1.000e+00 ratio: 0.071 lambda: 7.59375 #CG: 8 actred: -0.00018 prered: -0.00253 time: 22.749\n",
            "\n",
            " 32-iter val_acc: 79.402% val_loss 0.327\n",
            "\n",
            "Avg time per Gv iteration: 0.09634 s\n",
            "\n",
            "33-iter f: 0.398 |g|: 0.58068 alpha: 5.000e-01 ratio: 0.481 lambda: 7.59375 #CG: 7 actred: -0.00162 prered: -0.00337 time: 28.183\n",
            "\n",
            " 33-iter val_acc: 79.560% val_loss 0.325\n",
            "\n",
            "Avg time per Gv iteration: 0.09661 s\n",
            "\n",
            "34-iter f: 0.397 |g|: 1.03664 alpha: 1.000e+00 ratio: 0.543 lambda: 7.59375 #CG: 8 actred: -0.00084 prered: -0.00155 time: 22.758\n",
            "\n",
            " 34-iter val_acc: 79.503% val_loss 0.325\n",
            "\n",
            "Avg time per Gv iteration: 0.09613 s\n",
            "\n",
            "35-iter f: 0.396 |g|: 1.28415 alpha: 1.000e+00 ratio: 0.389 lambda: 7.59375 #CG: 7 actred: -0.00065 prered: -0.00166 time: 22.605\n",
            "\n",
            " 35-iter val_acc: 79.649% val_loss 0.325\n",
            "\n",
            "Best model saved in ./saved_model/model.ckpt\n",
            "\n",
            "Avg time per Gv iteration: 0.09629 s\n",
            "\n",
            "36-iter f: 0.396 |g|: 1.49020 alpha: 1.000e+00 ratio: 0.328 lambda: 7.59375 #CG: 7 actred: -0.00061 prered: -0.00187 time: 22.648\n",
            "\n",
            " 36-iter val_acc: 79.503% val_loss 0.325\n",
            "\n",
            "Avg time per Gv iteration: 0.09639 s\n",
            "\n",
            "37-iter f: 0.395 |g|: 1.11460 alpha: 5.000e-01 ratio: 0.274 lambda: 7.59375 #CG: 6 actred: -0.00050 prered: -0.00182 time: 28.094\n",
            "\n",
            " 37-iter val_acc: 79.683% val_loss 0.325\n",
            "\n",
            "Best model saved in ./saved_model/model.ckpt\n",
            "\n",
            "Avg time per Gv iteration: 0.09624 s\n",
            "\n",
            "38-iter f: 0.395 |g|: 0.57810 alpha: 5.000e-01 ratio: 0.462 lambda: 7.59375 #CG: 7 actred: -0.00063 prered: -0.00136 time: 28.181\n",
            "\n",
            " 38-iter val_acc: 79.604% val_loss 0.325\n",
            "\n",
            "Avg time per Gv iteration: 0.09616 s\n",
            "\n",
            "39-iter f: 0.394 |g|: 0.64837 alpha: 1.000e+00 ratio: 0.700 lambda: 7.59375 #CG: 8 actred: -0.00097 prered: -0.00139 time: 22.702\n",
            "\n",
            " 39-iter val_acc: 79.649% val_loss 0.324\n",
            "\n",
            "Avg time per Gv iteration: 0.09632 s\n",
            "\n",
            "40-iter f: 0.393 |g|: 0.80371 alpha: 1.000e+00 ratio: 0.654 lambda: 7.59375 #CG: 8 actred: -0.00082 prered: -0.00126 time: 22.714\n",
            "\n",
            " 40-iter val_acc: 79.571% val_loss 0.324\n",
            "\n",
            "Avg time per Gv iteration: 0.09633 s\n",
            "\n",
            "41-iter f: 0.392 |g|: 0.95996 alpha: 1.000e+00 ratio: 0.572 lambda: 7.59375 #CG: 7 actred: -0.00073 prered: -0.00127 time: 22.613\n",
            "\n",
            " 41-iter val_acc: 79.627% val_loss 0.324\n",
            "\n",
            "Avg time per Gv iteration: 0.09625 s\n",
            "\n",
            "42-iter f: 0.391 |g|: 1.18264 alpha: 1.000e+00 ratio: 0.478 lambda: 7.59375 #CG: 7 actred: -0.00067 prered: -0.00140 time: 22.609\n",
            "\n",
            " 42-iter val_acc: 79.604% val_loss 0.324\n",
            "\n",
            "Avg time per Gv iteration: 0.09629 s\n",
            "\n",
            "43-iter f: 0.391 |g|: 0.48773 alpha: 5.000e-01 ratio: 0.523 lambda: 7.59375 #CG: 7 actred: -0.00056 prered: -0.00107 time: 28.186\n",
            "\n",
            " 43-iter val_acc: 79.638% val_loss 0.324\n",
            "\n",
            "Avg time per Gv iteration: 0.09617 s\n",
            "\n",
            "44-iter f: 0.390 |g|: 0.78132 alpha: 1.000e+00 ratio: 0.615 lambda: 7.59375 #CG: 8 actred: -0.00074 prered: -0.00120 time: 22.706\n",
            "\n",
            " 44-iter val_acc: 79.515% val_loss 0.324\n",
            "\n",
            "Avg time per Gv iteration: 0.09629 s\n",
            "\n",
            "45-iter f: 0.389 |g|: 0.84963 alpha: 1.000e+00 ratio: 0.577 lambda: 7.59375 #CG: 7 actred: -0.00077 prered: -0.00133 time: 22.609\n",
            "\n",
            " 45-iter val_acc: 79.638% val_loss 0.324\n",
            "\n",
            "Avg time per Gv iteration: 0.09621 s\n",
            "\n",
            "46-iter f: 0.389 |g|: 1.40663 alpha: 1.000e+00 ratio: 0.239 lambda: 11.39062 #CG: 7 actred: -0.00035 prered: -0.00147 time: 22.607\n",
            "\n",
            " 46-iter val_acc: 79.515% val_loss 0.324\n",
            "\n",
            "Avg time per Gv iteration: 0.09639 s\n",
            "\n",
            "47-iter f: 0.388 |g|: 0.79599 alpha: 1.000e+00 ratio: 0.522 lambda: 11.39062 #CG: 4 actred: -0.00063 prered: -0.00120 time: 22.316\n",
            "\n",
            " 47-iter val_acc: 79.593% val_loss 0.324\n",
            "\n",
            "Avg time per Gv iteration: 0.09614 s\n",
            "\n",
            "48-iter f: 0.388 |g|: 1.19702 alpha: 1.000e+00 ratio: 0.203 lambda: 17.08594 #CG: 7 actred: -0.00022 prered: -0.00110 time: 22.609\n",
            "\n",
            " 48-iter val_acc: 79.425% val_loss 0.324\n",
            "\n",
            "Avg time per Gv iteration: 0.09627 s\n",
            "\n",
            "49-iter f: 0.388 |g|: 1.26936 alpha: 1.000e+00 ratio: 0.318 lambda: 17.08594 #CG: 7 actred: -0.00045 prered: -0.00143 time: 22.609\n",
            "\n",
            " 49-iter val_acc: 79.694% val_loss 0.324\n",
            "\n",
            "Best model saved in ./saved_model/model.ckpt\n",
            "\n",
            "Final acc: 79.694% | best acc 79.694% | total_#CG 441 | total running time 1320.390s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UwH_nXJZDz2T"
      },
      "source": [
        "## Predict ##"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9zyzmNHFnOwD"
      },
      "source": [
        "# Arguments for prediction PSSP dataset\n",
        "pred_args = (\"--bsize 1024 --valid_set \" + VALID_FILE + \" --train_set \" + TRAIN_FILE + \n",
        "\t\t\t\t\t\t \" --model ./saved_model/model.ckpt --dim \" +\n",
        "             str(HEIGHT) + \" \" + str(WIDTH) + \" \" + str(DEPTH)).split()"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s6MctxH5_nTR",
        "outputId": "dbe3b884-204a-4fd0-9577-617f3e9805b8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "valid_f =  \"/content/drive/MyDrive/Datasets/{0}_test_fold{1}.txt\".format(dataset.lower(),str(fold)) # train set  \n",
        "train_f = \"/content/drive/MyDrive/Datasets/{0}_train_fold{1}.txt\".format(dataset.lower(),str(fold)) # validation set\n",
        "test_f = \"/content/drive/MyDrive/Datasets/CASP13_3class.txt\" # test set CASP13\n",
        "print(valid_f)\n",
        "print(train_f)\n",
        "print(test_f)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/Datasets/cb513_test_fold8.txt\n",
            "/content/drive/MyDrive/Datasets/cb513_train_fold8.txt\n",
            "/content/drive/MyDrive/Datasets/CASP13_3class.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IeQqT96h_NHr",
        "outputId": "87f003d9-5047-4176-8fed-375a588436cc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "VALID_PRED_FILE=\"pred_test_fold{0}.txt\".format(fold)\n",
        "TRAIN_PRED_FILE=\"pred_train_fold{0}.txt\".format(fold)\n",
        "TEST_PRED_FILE=\"pred_casp13_fold{0}.txt\".format(fold)\n",
        "print(VALID_PRED_FILE)\n",
        "print(TRAIN_PRED_FILE)\n",
        "print(TEST_PRED_FILE)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "pred_test_fold8.txt\n",
            "pred_train_fold8.txt\n",
            "pred_casp13_fold8.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NNNnI_mU6LN5"
      },
      "source": [
        "##Declare Predict Methods##"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kYGPhdqk5wYp"
      },
      "source": [
        "def create_output_pred(pred, origin_f, outFileName):\n",
        "    labels = ['C','H','E']\n",
        "    read_file = open(origin_f,\"r\")\n",
        "    output_file = open(outFileName,\"w\")\n",
        "    count =1 \n",
        "    target_name =1\n",
        "    target_primary = 2\n",
        "    target_secondary = 3\n",
        "    counter = 0\n",
        "    while True:\n",
        "        line = read_file.readline()\n",
        "        if not line:\n",
        "            break\n",
        "        if count == target_name:\n",
        "            output_file.write(line)\n",
        "            target_name+=3\n",
        "        if count == target_primary:\n",
        "            output_file.write(line)\n",
        "            target_primary+=3 \n",
        "        if count == target_secondary:\n",
        "            output_file.write(line)\n",
        "            target_secondary+=3\n",
        "            line = line.replace(\"\\n\",\"\") \n",
        "            prediction = \"\"\n",
        "            for c in line:\n",
        "                if (c!='!'):\n",
        "                    prediction = prediction + labels[pred[counter]]\n",
        "                    counter +=1\n",
        "            output_file.write(prediction + \"\\n\")        \n",
        "        count+=1\n",
        "        "
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EA8Pq5a0C3m5"
      },
      "source": [
        "# import tensorflow as tf \n",
        "# tf.compat.v1.disable_eager_execution()\n",
        "# from utilities import predict, read_data, normalize_and_reshape\n",
        "# from net.net import CNN\n",
        "# import numpy as np \n",
        "# import argparse\n",
        "# import pdb\n",
        "\n",
        "def parse_args():\n",
        "        parser = argparse.ArgumentParser(description='prediction')\n",
        "        parser.add_argument('--test_set', dest='test_set',\n",
        "                            help='provide the directory of .mat file for testing',\n",
        "                            default=None, type=str)\n",
        "        parser.add_argument('--valid_set', dest='valid_set',\n",
        "                            help='provide the directory of .mat file for validation',\n",
        "                            default=None, type=str)\n",
        "        parser.add_argument('--train_set', dest='train_set',\n",
        "                            help='provide the directory of .mat file for training',\n",
        "                            default=None, type=str)\n",
        "        parser.add_argument('--model', dest='model_file',\n",
        "                            help='provide file storing network parameters, i.e. ./dir/model.ckpt',\n",
        "                            default='./saved_model/model.ckpt', type=str)\n",
        "        parser.add_argument('--bsize', dest='bsize',\n",
        "                            help='batch size',\n",
        "                            default=1024, type=int)\n",
        "        parser.add_argument('--loss', dest='loss', \n",
        "                            help='which loss function to use: MSELoss or CrossEntropy',\n",
        "                            default='MSELoss', type=str)\n",
        "        parser.add_argument('--dim', dest='dim', nargs='+', help='input dimension of data,'+\\\n",
        "                            'shape must be:  height width num_channels',\n",
        "                            default=[32, 32, 3], type=int)\n",
        "\n",
        "        args = parser.parse_args(args=pred_args)\n",
        "        return args\n",
        "\n",
        "def predict_model():\n",
        "        args = parse_args()\n",
        "\n",
        "        sess_config = tf.compat.v1.ConfigProto()\n",
        "        sess_config.gpu_options.allow_growth = True\n",
        "\n",
        "        with tf.compat.v1.Session(config=sess_config) as sess:\n",
        "                graph_address = args.model_file + '.meta'\n",
        "                imported_graph = tf.compat.v1.train.import_meta_graph(graph_address)\n",
        "                imported_graph.restore(sess, args.model_file)\n",
        "                mean_param = [v for v in tf.compat.v1.global_variables() if 'mean_tr:0' in v.name][0]\n",
        "                label_enum_var = [v for v in tf.compat.v1.global_variables() if 'label_enum:0' in v.name][0]\n",
        "\n",
        "                sess.run(tf.compat.v1.variables_initializer([mean_param, label_enum_var]))\n",
        "                mean_tr = sess.run(mean_param)\n",
        "                label_enum = sess.run(label_enum_var)\n",
        "\n",
        "                x = tf.compat.v1.get_default_graph().get_tensor_by_name('main_params/input_of_net:0')\n",
        "                y = tf.compat.v1.get_default_graph().get_tensor_by_name('main_params/labels:0')\n",
        "                outputs = tf.compat.v1.get_default_graph().get_tensor_by_name('output_of_net:0')\n",
        "\n",
        "                if args.loss == 'MSELoss':\n",
        "                        loss = tf.reduce_sum(input_tensor=tf.pow(outputs-y, 2))\n",
        "                else:\n",
        "                        loss = tf.reduce_sum(input_tensor=\n",
        "                            tf.nn.softmax_cross_entropy_with_logits(logits=outputs, labels=tf.stop_gradient(y)))\n",
        "                \n",
        "                network = (x, y, loss, outputs)\n",
        "\n",
        "                if args.valid_set is not None:\n",
        "                        valid_batch, num_cls, _ = read_data(args.valid_set, dim=args.dim, label_enum=label_enum)\n",
        "                        valid_batch[0], _ = normalize_and_reshape(valid_batch[0], dim=args.dim, mean_tr=mean_tr)\n",
        "            \n",
        "                        avg_loss_valid, avg_acc_valid, results_valid = predict(sess, network, valid_batch, args.bsize)\n",
        "\n",
        "                        # convert results back to the original labels\n",
        "                        inverse_map = dict(zip(np.arange(num_cls), label_enum))\n",
        "                        results_valid = np.expand_dims(results_valid, axis=1)\n",
        "                        results_valid = np.apply_along_axis(lambda x: inverse_map[x[0]], axis=1, arr=results_valid)\n",
        "                        create_output_pred(results_valid, valid_f, VALID_PRED_FILE)\n",
        "                        print('In valid phase, average loss: {:.3f} | average accuracy: {:.3f}%'.\\\n",
        "                            format(avg_loss_valid, avg_acc_valid*100))\n",
        "                \n",
        "                if args.train_set is not None:\n",
        "                        train_batch, num_cls, _ = read_data(args.train_set, dim=args.dim, label_enum=label_enum)\n",
        "                        train_batch[0], _ = normalize_and_reshape(train_batch[0], dim=args.dim, mean_tr=mean_tr)\n",
        "\n",
        "                        avg_loss_train, avg_acc_train, results_train = predict(sess, network, train_batch, args.bsize)\n",
        "                        # convert results back to the original labels\n",
        "                        inverse_map = dict(zip(np.arange(num_cls), label_enum))\n",
        "                        results_train = np.expand_dims(results_train, axis=1)\n",
        "                        results_train = np.apply_along_axis(lambda x: inverse_map[x[0]], axis=1, arr=results_train)\n",
        "                        # create_output_pred(results, results_train)\n",
        "\n",
        "                        create_output_pred(results_train, train_f, TRAIN_PRED_FILE)\n",
        "                        print('In train phase, average loss: {:.3f} | average accuracy: {:.3f}%'.\\\n",
        "                            format(avg_loss_train, avg_acc_train*100))\n",
        "\n",
        "                if args.test_set is not None:\n",
        "                        test_batch, num_cls, _ = read_data(args.test_set, dim=args.dim, label_enum=label_enum)\n",
        "                        test_batch[0], _ = normalize_and_reshape(test_batch[0], dim=args.dim, mean_tr=mean_tr)\n",
        "\n",
        "                        avg_loss_test, avg_acc_test, results_test = predict(sess, network, test_batch, args.bsize)\n",
        "                        # convert results back to the original labels\n",
        "                        inverse_map = dict(zip(np.arange(num_cls), label_enum))\n",
        "                        results_test = np.expand_dims(results_test, axis=1)\n",
        "                        results_test = np.apply_along_axis(lambda x: inverse_map[x[0]], axis=1, arr=results_test)\n",
        "                        # create_output_pred(results, results_train)\n",
        "\n",
        "                        create_output_pred(results_test, test_f, TEST_PRED_FILE)\n",
        "                        print('In test phase, average loss: {:.3f} | average accuracy: {:.3f}%'.\\\n",
        "                            format(avg_loss_test, avg_acc_test*100))\n",
        "            "
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aN8BTASh6eSL"
      },
      "source": [
        "##Run Predict and Display output##"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gCInY5uB6Y3G",
        "outputId": "e8eef5d0-7ad6-414a-a30b-eab7224fe479",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "predict_model()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from ./saved_model/model.ckpt\n",
            "In valid phase, average loss: 0.324 | average accuracy: 79.694%\n",
            "In train phase, average loss: 0.269 | average accuracy: 83.597%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "loR25EiOE4cQ"
      },
      "source": [
        "# !head \"$VALID_PRED_FILE\""
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NOnpS-N3DwhK"
      },
      "source": [
        "# !head \"$TRAIN_PRED_FILE\""
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exgdCTXb68Vl"
      },
      "source": [
        "## Check Test score on CASP13 ##"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lda3hrM4dmWk"
      },
      "source": [
        "# Arguments for prediction PSSP dataset\n",
        "pred_args = (\"--bsize 1024 --test_set \" + TEST_FILE + \n",
        "\t\t\t\t\t\t \" --model ./saved_model/model.ckpt --dim \" +\n",
        "             str(HEIGHT) + \" \" + str(WIDTH) + \" \" + str(DEPTH)).split()"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3JCC5-2mk0rs",
        "outputId": "ae3327dc-474f-45c3-918b-4f47ee4919b4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "predict_model()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from ./saved_model/model.ckpt\n",
            "In test phase, average loss: 0.332 | average accuracy: 78.190%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PMY6ihMjG1iO"
      },
      "source": [
        "# !head \"$TEST_PRED_FILE\""
      ],
      "execution_count": 25,
      "outputs": []
    }
  ]
}