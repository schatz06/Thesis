{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Notebooks_CNN_HFO_v2.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "PYXlwVnXKhOJ",
        "CVtiFO19ymk_",
        "fzh5PNccJ28-",
        "gcdrh-r9JRdS",
        "2pQEA9LADkBP",
        "J5uoUeQBDq5Q",
        "vMCxn-8P5tsH",
        "UwH_nXJZDz2T",
        "NNNnI_mU6LN5"
      ],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/schatz06/Thesis/blob/main/Notebooks_CNN_HFO_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aa_PJwlt7zI2"
      },
      "source": [
        "##Mount drive## "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9DjpeG7t71KC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4113473-769d-4673-cba9-f6c956e3fa77"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IMo4HooFl0dv"
      },
      "source": [
        "##General Variables to use to load data##"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sJHXZcSlNqL9"
      },
      "source": [
        "fold = 2 # which fold to take \n",
        "embedding = \"protbert\"\n",
        "# embedding = \"seqvec\"\n",
        "# dataset=\"PISCES\" \n",
        "dataset=\"CB513\"\n",
        "USE_HFO=True"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PYXlwVnXKhOJ"
      },
      "source": [
        "## Imports ##"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WCuZCtMgg5ns"
      },
      "source": [
        "# Reload all modules (except those excluded by %aimport) every time before executing the Python code typed.\n",
        "%load_ext autoreload \n",
        "%autoreload 2\n",
        "# matplotlib graphs will be included in your notebook, next to the code. \n",
        "%matplotlib inline"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HKDaAU3F5Bgs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ed1fd0c-6004-48af-ad7e-3ea3cbd7e236"
      },
      "source": [
        "# install hdf5storage package \r\n",
        "!pip install hdf5storage"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting hdf5storage\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/79/e0/5dd25068a231cd817265529368aca2f918049b290dcb2fd9b24ce136adf4/hdf5storage-0.1.15-py2.py3-none-any.whl (56kB)\n",
            "\r\u001b[K     |█████▊                          | 10kB 20.0MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 20kB 15.3MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 30kB 12.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 40kB 12.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 51kB 8.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 61kB 5.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy; python_version >= \"3.4\" in /usr/local/lib/python3.6/dist-packages (from hdf5storage) (1.19.5)\n",
            "Requirement already satisfied: h5py>=2.1; python_version >= \"3.3\" in /usr/local/lib/python3.6/dist-packages (from hdf5storage) (2.10.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py>=2.1; python_version >= \"3.3\"->hdf5storage) (1.15.0)\n",
            "Installing collected packages: hdf5storage\n",
            "Successfully installed hdf5storage-0.1.15\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cjrANRzOFauh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c731edb0-2670-4108-dac3-5c2599d092ad"
      },
      "source": [
        "import pdb # python debugger\n",
        "import numpy as np # import numpy \n",
        "import tensorflow as tf # import tensorflow  \n",
        "tf.compat.v1.disable_eager_execution() # disable eager execution\n",
        "import time # import time\n",
        "import math # import math\n",
        "import argparse # import argparse\n",
        "import os # import os\n",
        "import scipy.io as sio # import scipy.io \n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior() # makes different behaviors betweem tf_v1 & tf_v2 behave the same \n",
        "from tensorflow.python.client import device_lib # package to find available gpus\n",
        "import pandas as pd # import padas\n",
        "import hdf5storage # import hdf5storage"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CVtiFO19ymk_"
      },
      "source": [
        "## Get data ##"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yfOssHbYEh-W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5b34844-0f06-4a9f-d205-dc0f5609447c"
      },
      "source": [
        "VALID_FILE = \"/content/drive/MyDrive/Datasets/{0}_{1}_testSet{2}.mat\".format(dataset.lower(),embedding,str(fold)) # validation set\n",
        "TRAIN_FILE = \"/content/drive/MyDrive/Datasets/{0}_{1}_trainSet{2}.mat\".format(dataset.lower(),embedding,str(fold)) # train set  \n",
        "TEST_FILE = \"/content/drive/MyDrive/Datasets/casp13_{0}.mat\".format(embedding) # test set CASP13\n",
        "print(VALID_FILE)\n",
        "print(TRAIN_FILE)\n",
        "print(TEST_FILE)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/Datasets/cb513_protbert_testSet2.mat\n",
            "/content/drive/MyDrive/Datasets/cb513_protbert_trainSet2.mat\n",
            "/content/drive/MyDrive/Datasets/casp13_protbert.mat\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QKVIpZww1-dw"
      },
      "source": [
        "HEIGHT = 32\n",
        "WIDTH = 32\n",
        "DEPTH = 1\n",
        "CATEGORIES = 3 # number of different classification categories"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fzh5PNccJ28-"
      },
      "source": [
        "## VGG ##"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NwaNt8clJ5Vu"
      },
      "source": [
        "\"\"\"\n",
        "Codes are modifeid from PyTorch and Tensorflow Versions of VGG: \n",
        "https://github.com/pytorch/vision/blob/master/torchvision/models/vgg.py, and\n",
        "https://github.com/keras-team/keras-applications/blob/master/keras_applications/vgg16.py\n",
        "\"\"\"\n",
        "\n",
        "# import tensorflow.compat.v1 as tf\n",
        "# tf.disable_v2_behavior()\n",
        "# import numpy as np \n",
        "# import pdb\n",
        "from tensorflow.keras.applications.vgg16 import VGG16 as vgg16 # import vgg16 convolutional network\n",
        "from tensorflow.keras.applications.vgg19 import VGG19 as vgg19 # import vgg19 convolutional network\n",
        "\n",
        "__all__ = ['VGG11', 'VGG13', 'VGG16','VGG19'] # array holds all vgg CNN's names\n",
        " \n",
        "def VGG(feature, num_cls): # define VGG \n",
        "\n",
        "\twith tf.variable_scope('fully_connected') as scope:\n",
        "\t\tdim =np.prod(feature.shape[1:]) # returns the product of the given array\n",
        "\t\tx = tf.reshape(feature, [-1, dim]) # reshape tensor\n",
        "\n",
        "\t\tx = tf.keras.layers.Dense(units=4096, activation='relu', name=scope.name)(x) # define layers \n",
        "\t\tx = tf.keras.layers.Dense(units=4096, activation='relu', name=scope.name)(x)\n",
        "\t\tx = tf.keras.layers.Dense(units=num_cls, name=scope.name)(x)\n",
        "\n",
        "\treturn x\n",
        "# make the layers of CNN \n",
        "def make_layers(x, cfg):\n",
        "\tfor v in cfg:\n",
        "\t\tif v == 'M':\n",
        "\t\t\tx = tf.keras.layers.MaxPool2D(pool_size=[2, 2], strides=2, padding='valid')(x)\n",
        "\t\telse:\n",
        "\t\t\tx = tf.keras.layers.Conv2D(\n",
        "\t\t\tfilters=v,\n",
        "\t\t\tkernel_size=[3, 3],\n",
        "\t\t\tpadding='SAME',\n",
        "\t\t\tactivation=tf.nn.relu\n",
        "\t\t\t)(x)\n",
        "\treturn x\n",
        "\n",
        "cfg = {\n",
        "\t'A': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
        "\t'B': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
        "\t'D': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
        "\t'E': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', \n",
        "\t\t  512, 512, 512, 512, 'M'],\n",
        "}\n",
        "\n",
        "def VGG11(x_images, num_cls):\n",
        "\tfeature = make_layers(x_images, cfg['A'])\n",
        "\treturn VGG(feature, num_cls)\n",
        "\n",
        "def VGG13(x_images, num_cls):\n",
        "\tfeature = make_layers(x_images, cfg['B'])\n",
        "\treturn VGG(feature, num_cls)\n",
        "\n",
        "def VGG16(x_images, num_cls):\n",
        "\tfeature = make_layers(x_images, cfg['D'])\n",
        "\treturn VGG(feature, num_cls)\n",
        "\n",
        "def VGG19(x_images, num_cls):\n",
        "\tfeature = make_layers(x_images, cfg['E'])\n",
        "\treturn VGG(feature, num_cls)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gcdrh-r9JRdS"
      },
      "source": [
        "## Net ##"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kHA5zxK1JRtg"
      },
      "source": [
        "# import tensorflow.compat.v1 as tf\n",
        "# tf.disable_v2_behavior()\n",
        "# import math\n",
        "# import pdb\n",
        "# from tensorflow.python.client import device_lib\n",
        "# import numpy as np\n",
        "# from net.vgg import *\n",
        "\n",
        "def CNN_4layers(x_image, num_cls, reuse=False):\n",
        "\t_NUM_CLASSES = num_cls\n",
        "\twith tf.variable_scope('conv1', reuse=reuse) as scope:\n",
        "\t\tconv = tf.keras.layers.Conv2D(\n",
        "\t\t\tfilters=64,\n",
        "\t\t\tkernel_size=[3, 3],\n",
        "\t\t\tpadding='SAME',\n",
        "\t\t\tactivation=tf.nn.relu\n",
        "\t\t)(x_image)\n",
        "\n",
        "\twith tf.variable_scope('conv2', reuse=reuse) as scope:\n",
        "\t\tconv = tf.keras.layers.Conv2D(\n",
        "\t\t\tfilters=64,\n",
        "\t\t\tkernel_size=[3, 3],\n",
        "\t\t\tpadding='SAME',\n",
        "\t\t\tactivation=tf.nn.relu\n",
        "\t\t)(conv)\n",
        "\t\t\n",
        "\twith tf.variable_scope('conv3', reuse=reuse) as scope:\n",
        "\t\tconv = tf.keras.layers.Conv2D(\n",
        "\t\t\tfilters=64,\n",
        "\t\t\tkernel_size=[3, 3],\n",
        "\t\t\tpadding='SAME',\n",
        "\t\t\tactivation=tf.nn.relu\n",
        "\t\t)(conv)\n",
        "\n",
        "\twith tf.variable_scope('fully_connected', reuse=reuse) as scope:\n",
        "\t\tdim =np.prod(conv.shape[1:])\n",
        "\t\tflat = tf.reshape(conv, [-1, dim])\n",
        "\t\toutputs = tf.keras.layers.Dense(units=_NUM_CLASSES, name=scope.name)(flat)\n",
        "\n",
        "\treturn outputs\n",
        "\n",
        "\n",
        "\t# with tf.variable_scope('conv1', reuse=reuse) as scope:\n",
        "\t# \tconv = tf.keras.layers.Conv2D(\n",
        "\t# \t\tfilters=32,\n",
        "\t# \t\tkernel_size=[5, 5],\n",
        "\t# \t\tpadding='SAME',\n",
        "\t# \t\tactivation=tf.nn.relu\n",
        "\t# \t)(x_image)\n",
        "\t# \tpool = tf.keras.layers.MaxPool2D(pool_size=[2, 2], strides=2, padding='valid')(conv)\n",
        "\t# \t# N x 16 x 16 x 32\n",
        "\n",
        "\t# with tf.variable_scope('conv2', reuse=reuse) as scope:\n",
        "\t# \tconv = tf.keras.layers.Conv2D(\n",
        "\t# \t\tfilters=64,\n",
        "\t# \t\tkernel_size=[3, 3],\n",
        "\t# \t\tpadding='SAME',\n",
        "\t# \t\tactivation=tf.nn.relu\n",
        "\t# \t)(pool)\n",
        "\t# \tpool = tf.keras.layers.MaxPool2D(pool_size=[2, 2], strides=2, padding='valid')(conv)\n",
        "\t# \t# N x 8 x 8 x 64\n",
        "\t\t\n",
        "\t# with tf.variable_scope('conv3', reuse=reuse) as scope:\n",
        "\t# \tconv = tf.keras.layers.Conv2D(\n",
        "\t# \t\tfilters=64,\n",
        "\t# \t\tkernel_size=[3, 3],\n",
        "\t# \t\tpadding='SAME',\n",
        "\t# \t\tactivation=tf.nn.relu\n",
        "\t# \t)(pool)\n",
        "\t# \tpool = tf.keras.layers.MaxPool2D(pool_size=[2, 2], strides=2, padding='valid')(conv)\n",
        "\t# \t# N x 4 x 4 x 64\n",
        "\n",
        "\t# with tf.variable_scope('fully_connected', reuse=reuse) as scope:\n",
        "\t# \tdim =np.prod(pool.shape[1:])\n",
        "\t# \tflat = tf.reshape(pool, [-1, dim])\n",
        "\t# \toutputs = tf.keras.layers.Dense(units=_NUM_CLASSES, name=scope.name)(flat)\n",
        "\n",
        "\t# return outputs\n",
        "\n",
        "def CNN_7layers(x_image, num_cls, reuse=False):\n",
        "\t_NUM_CLASSES = num_cls\n",
        "\twith tf.variable_scope('conv1', reuse=reuse) as scope:\n",
        "\t\tconv = tf.keras.layers.Conv2D(\n",
        "\t\t\tfilters=64,\n",
        "\t\t\tkernel_size=[3, 3],\n",
        "\t\t\tpadding='SAME',\n",
        "\t\t\tactivation=tf.nn.relu\n",
        "\t\t)(x_image)\n",
        "\t\tconv = tf.keras.layers.Conv2D(\n",
        "\t\t\tfilters=64,\n",
        "\t\t\tkernel_size=[3, 3],\n",
        "\t\t\tpadding='SAME',\n",
        "\t\t\tactivation=tf.nn.relu\n",
        "\t\t)(conv)\n",
        "\n",
        "\twith tf.variable_scope('conv2', reuse=reuse) as scope:\n",
        "\t\tconv = tf.keras.layers.Conv2D(\n",
        "\t\t\tfilters=64,\n",
        "\t\t\tkernel_size=[3, 3],\n",
        "\t\t\tpadding='SAME',\n",
        "\t\t\tactivation=tf.nn.relu\n",
        "\t\t)(conv)\n",
        "\t\tconv = tf.keras.layers.Conv2D(\n",
        "\t\t\tfilters=64,\n",
        "\t\t\tkernel_size=[3, 3],\n",
        "\t\t\tpadding='SAME',\n",
        "\t\t\tactivation=tf.nn.relu\n",
        "\t\t)(conv)\n",
        "\n",
        "\twith tf.variable_scope('conv3', reuse=reuse) as scope:\n",
        "\t\tconv = tf.keras.layers.Conv2D(\n",
        "\t\t\tfilters=64,\n",
        "\t\t\tkernel_size=[3, 3],\n",
        "\t\t\tpadding='SAME',\n",
        "\t\t\tactivation=tf.nn.relu\n",
        "\t\t)(conv)\n",
        "\t\tconv = tf.keras.layers.Conv2D(\n",
        "\t\t\tfilters=64,\n",
        "\t\t\tkernel_size=[3, 3],\n",
        "\t\t\tpadding='SAME',\n",
        "\t\t\tactivation=tf.nn.relu\n",
        "\t\t)(conv)\n",
        "\n",
        "\twith tf.variable_scope('fully_connected', reuse=reuse) as scope:\n",
        "\t\tdim = np.prod(conv.shape[1:])\n",
        "\t\tflat = tf.reshape(conv, [-1, dim])\n",
        "\t\toutputs = tf.keras.layers.Dense(units=_NUM_CLASSES, name=scope.name)(flat)\n",
        "\n",
        "\treturn outputs\n",
        "\n",
        "\t# with tf.variable_scope('conv1', reuse=reuse) as scope:\n",
        "\t# \tconv = tf.keras.layers.Conv2D(\n",
        "\t# \t\tfilters=32,\n",
        "\t# \t\tkernel_size=[5, 5],\n",
        "\t# \t\tpadding='SAME',\n",
        "\t# \t\tactivation=tf.nn.relu\n",
        "\t# \t)(x_image)\n",
        "\t# \tconv = tf.keras.layers.Conv2D(\n",
        "\t# \t\tfilters=32,\n",
        "\t# \t\tkernel_size=[3, 3],\n",
        "\t# \t\tpadding='SAME',\n",
        "\t# \t\tactivation=tf.nn.relu\n",
        "\t# \t)(conv)\n",
        "\t# \tpool = tf.keras.layers.MaxPool2D(pool_size=[2, 2], strides=2, padding='valid')(conv)\n",
        "\t# \t# N x 16 x 16 x 32\n",
        "\n",
        "\t# with tf.variable_scope('conv2', reuse=reuse) as scope:\n",
        "\t# \tconv = tf.keras.layers.Conv2D(\n",
        "\t# \t\tfilters=64,\n",
        "\t# \t\tkernel_size=[3, 3],\n",
        "\t# \t\tpadding='SAME',\n",
        "\t# \t\tactivation=tf.nn.relu\n",
        "\t# \t)(pool)\n",
        "\t# \tconv = tf.keras.layers.Conv2D(\n",
        "\t# \t\tfilters=64,\n",
        "\t# \t\tkernel_size=[3, 3],\n",
        "\t# \t\tpadding='SAME',\n",
        "\t# \t\tactivation=tf.nn.relu\n",
        "\t# \t)(conv)\n",
        "\t# \tpool = tf.keras.layers.MaxPool2D(pool_size=[2, 2], strides=2, padding='valid')(conv)\n",
        "\t# \t# N x 8 x 8 x 64\n",
        "\n",
        "\t# with tf.variable_scope('conv3', reuse=reuse) as scope:\n",
        "\t# \tconv = tf.keras.layers.Conv2D(\n",
        "\t# \t\tfilters=64,\n",
        "\t# \t\tkernel_size=[3, 3],\n",
        "\t# \t\tpadding='SAME',\n",
        "\t# \t\tactivation=tf.nn.relu\n",
        "\t# \t)(pool)\n",
        "\t# \tconv = tf.keras.layers.Conv2D(\n",
        "\t# \t\tfilters=128,\n",
        "\t# \t\tkernel_size=[3, 3],\n",
        "\t# \t\tpadding='SAME',\n",
        "\t# \t\tactivation=tf.nn.relu\n",
        "\t# \t)(conv)\n",
        "\t# \tpool = tf.keras.layers.MaxPool2D(pool_size=[2, 2], strides=2, padding='valid')(conv)\n",
        "\t# \t# pool = tf.layers.dropout(pool, rate=0.25, name=scope.name)\n",
        "\t# \t# N x 4 x 4 x 128\n",
        "\n",
        "\t# with tf.variable_scope('fully_connected', reuse=reuse) as scope:\n",
        "\t# \tdim = np.prod(pool.shape[1:])\n",
        "\t# \tflat = tf.reshape(pool, [-1, dim])\n",
        "\t# \toutputs = tf.keras.layers.Dense(units=_NUM_CLASSES, name=scope.name)(flat)\n",
        "\n",
        "\t# return outputs\n",
        "\n",
        "def CNN(net, num_cls, dim):\n",
        "\n",
        "\t_NUM_CLASSES = num_cls\n",
        "\t_IMAGE_HEIGHT, _IMAGE_WIDTH, _IMAGE_CHANNELS = dim\n",
        "\n",
        "\twith tf.name_scope('main_params'):\n",
        "\t\tx = tf.placeholder(tf.float32, shape=[None, _IMAGE_HEIGHT, _IMAGE_WIDTH, _IMAGE_CHANNELS], name='input_of_net')\n",
        "\t\ty = tf.placeholder(tf.float32, shape=[None, _NUM_CLASSES], name='labels')\n",
        "\n",
        "\t# call CNN structure according to string net\n",
        "\toutputs = globals()[net](x, _NUM_CLASSES)\n",
        "\toutputs = tf.identity(outputs, name='output_of_net')\n",
        "\n",
        "\treturn (x, y, outputs)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2pQEA9LADkBP"
      },
      "source": [
        "## Utilities ##"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JxQzW-93DIdN"
      },
      "source": [
        "# import numpy as np\n",
        "# import math\n",
        "# import scipy.io as sio\n",
        "# import os\n",
        "# import math\n",
        "# import pdb\n",
        "\n",
        "class ConfigClass(object):\n",
        "\tdef __init__(self, args, num_data, num_cls):\n",
        "\t\tsuper(ConfigClass, self).__init__()\n",
        "\t\tself.args = args\n",
        "\t\tself.iter_max = args.iter_max\n",
        "\t\t\n",
        "\t\t# Different notations of regularization term:\n",
        "\t\t# In SGD, weight decay:\n",
        "\t\t# \tweight_decay <- lr/(C*num_of_training_samples)\n",
        "\t\t# In Newton method:\n",
        "\t\t# \tC <- C * num_of_training_samples\n",
        "\n",
        "\t\tself.seed = args.seed\n",
        "\n",
        "\t\tif self.seed is None:\n",
        "\t\t\tprint('You choose not to specify a random seed.'+\\\n",
        "\t\t\t\t'A different result is produced after each run.')\n",
        "\t\telif isinstance(self.seed, int) and self.seed >= 0:\n",
        "\t\t\tprint('You specify random seed {}.'.format(self.seed))\n",
        "\t\telse:\n",
        "\t\t\traise ValueError('Only accept None type or nonnegative integers for'+\\\n",
        "\t\t\t\t\t' random seed argument!')\n",
        "\n",
        "\t\tself.train_set = args.train_set\n",
        "\t\tself.val_set = args.val_set\n",
        "\t\tself.num_cls = num_cls\n",
        "\t\tself.dim = args.dim\n",
        "\n",
        "\t\tself.num_data = num_data\n",
        "\t\tself.GNsize = min(args.GNsize, self.num_data)\n",
        "\t\tself.C = args.C * self.num_data\n",
        "\t\tself.net = args.net\n",
        "\n",
        "\t\tself.xi = 0.1\n",
        "\t\tself.CGmax = args.CGmax\n",
        "\t\tself._lambda = args._lambda\n",
        "\t\tself.drop = args.drop\n",
        "\t\tself.boost = args.boost\n",
        "\t\tself.eta = args.eta\n",
        "\t\tself.lr = args.lr\n",
        "\t\tself.lr_decay = args.lr_decay\n",
        "\n",
        "\t\tself.bsize = args.bsize\n",
        "\t\tif args.momentum < 0:\n",
        "\t\t\traise ValueError('Momentum needs to be larger than 0!')\n",
        "\t\tself.momentum = args.momentum\n",
        "\n",
        "\t\tself.loss = args.loss\n",
        "\t\tif self.loss not in ('MSELoss', 'CrossEntropy'):\n",
        "\t\t\traise ValueError('Unrecognized loss type!')\n",
        "\t\tself.optim = args.optim\n",
        "\t\tif self.optim not in ('SGD', 'NewtonCG', 'Adam'):\n",
        "\t\t\traise ValueError('Only support SGD, Adam & NewtonCG optimizer!')\n",
        "\t\t\n",
        "\t\tself.log_file = args.log_file\n",
        "\t\tself.model_file = args.model_file\n",
        "\t\tself.screen_log_only = args.screen_log_only\n",
        "\n",
        "\t\tif self.screen_log_only:\n",
        "\t\t\tprint('You choose not to store running log. Only store model to {}'.format(self.log_file))\n",
        "\t\telse:\n",
        "\t\t\tprint('Saving log to: {}'.format(self.log_file))\n",
        "\t\t\tdir_name, _ = os.path.split(self.log_file)\n",
        "\t\t\tif not os.path.isdir(dir_name):\n",
        "\t\t\t\tos.makedirs(dir_name, exist_ok=True)\n",
        "\n",
        "\t\tdir_name, _ = os.path.split(self.model_file)\n",
        "\t\tif not os.path.isdir(dir_name):\n",
        "\t\t\tos.makedirs(dir_name, exist_ok=True)\n",
        "\t\t\n",
        "\t\tself.elapsed_time = 0.0\n",
        "\n",
        "def read_data(filename, dim, label_enum=None):\n",
        "\t\"\"\"\n",
        "\targs:\n",
        "\t\tfilename: the path where .mat files are stored\n",
        "\t\tlabel_enum (default None): the list that stores the original labels. \n",
        "\t\t\tIf label_enum is None, the function will generate a new list which stores the \n",
        "\t\t\toriginal labels in a sequence, and map original labels to [0, 1, ... number_of_classes-1]. \n",
        "\t\t\tIf label_enum is a list, the function will use it to convert \n",
        "\t\t\toriginal labels to [0, 1,..., number_of_classes-1].\n",
        "\t\"\"\"\n",
        "\n",
        "\tmat_contents = sio.loadmat(filename)\n",
        "\t#mat_contents = hdf5storage.loadmat(filename)\n",
        "\timages, labels = mat_contents['x'], mat_contents['y']\n",
        "\t\n",
        "\tlabels = labels.reshape(-1)\n",
        "\timages = images.reshape(images.shape[0], -1)\n",
        "\n",
        "\t_IMAGE_HEIGHT, _IMAGE_WIDTH, _IMAGE_CHANNELS = dim\n",
        "\tzero_to_append = np.zeros((images.shape[0],\n",
        "\t\t\t_IMAGE_CHANNELS*_IMAGE_HEIGHT*_IMAGE_WIDTH-np.prod(images.shape[1:])))\n",
        "\timages = np.append(images, zero_to_append, axis=1)\n",
        "\n",
        "\t# check data validity\n",
        "\tif label_enum is None:\n",
        "\t\tlabel_enum, labels = np.unique(labels, return_inverse=True)\n",
        "\t\tnum_cls = labels.max() + 1\n",
        "\n",
        "\t\tif len(label_enum) != num_cls:\n",
        "\t\t\traise ValueError('The number of classes is not equal to the number of\\\n",
        "\t\t\t\t\t\t\tlabels in dataset. Please verify them.')\n",
        "\telse:\n",
        "\t\tnum_cls = len(label_enum)\n",
        "\t\tforward_map = dict(zip(label_enum, np.arange(num_cls)))\n",
        "\t\tlabels = np.expand_dims(labels, axis=1)\n",
        "\t\tlabels = np.apply_along_axis(lambda x:forward_map[x[0]], axis=1, arr=labels)\n",
        "\t\t\n",
        "\n",
        "\t# convert groundtruth to one-hot encoding\n",
        "\tlabels = np.eye(num_cls)[labels]\n",
        "\tlabels = labels.astype('float32')\n",
        "\n",
        "\treturn [images, labels], num_cls, label_enum\n",
        "\n",
        "def normalize_and_reshape(images, dim, mean_tr=None):\n",
        "\t_IMAGE_HEIGHT, _IMAGE_WIDTH, _IMAGE_CHANNELS = dim\n",
        "\timages_shape = [images.shape[0], _IMAGE_CHANNELS, _IMAGE_HEIGHT, _IMAGE_WIDTH]\n",
        "\n",
        "\t# images normalization and zero centering\n",
        "\timages = images.reshape(images_shape[0], -1)\n",
        "\n",
        "\timages = images/255.0\n",
        "\n",
        "\tif mean_tr is None:\n",
        "\t\tprint('No mean of data provided! Normalize images by their own mean.')\n",
        "\t\t# if no mean_tr is provided, we calculate it according to the current data\n",
        "\t\tmean_tr = images.mean(axis=0) \n",
        "\telse:\n",
        "\t\tprint('Normalize images according to the provided mean.')\n",
        "\t\tif np.prod(mean_tr.shape) != np.prod(dim):\n",
        "\t\t\traise ValueError('Dimension of provided mean does not agree with the data! Please verify them!')\n",
        "\n",
        "\timages = images - mean_tr\n",
        "\n",
        "\timages = images.reshape(images_shape)\n",
        "\t# Tensorflow accepts data shape: B x H x W x C\n",
        "\timages = np.transpose(images, (0, 2, 3, 1))\n",
        "\treturn images, mean_tr\n",
        "\n",
        "\n",
        "def predict(sess, network, test_batch, bsize):\n",
        "\tx, y, loss, outputs = network\n",
        "\n",
        "\ttest_inputs, test_labels = test_batch\n",
        "\tbatch_size = bsize\n",
        "\n",
        "\tnum_data = test_labels.shape[0]\n",
        "\tnum_batches = math.ceil(num_data/batch_size)\n",
        "\n",
        "\tresults = np.zeros(shape=num_data, dtype=np.int)\n",
        "\tinfer_loss = 0.0\n",
        "\n",
        "\tfor i in range(num_batches):\n",
        "\t\tbatch_idx = np.arange(i*batch_size, min((i+1)*batch_size, num_data))\n",
        "\n",
        "\t\tbatch_input = test_inputs[batch_idx]\n",
        "\t\tbatch_labels = test_labels[batch_idx]\n",
        "\n",
        "\t\tnet_outputs, _loss = sess.run(\n",
        "\t\t\t[outputs, loss], feed_dict={x: batch_input, y: batch_labels}\n",
        "\t\t\t)\n",
        "\t\t\n",
        "\t\tresults[batch_idx] = np.argmax(net_outputs, axis=1)\n",
        "\t\t# note that _loss was summed over batches\n",
        "\t\tinfer_loss = infer_loss + _loss\n",
        "\n",
        "\tavg_acc = (np.argmax(test_labels, axis=1) == results).mean()\n",
        "\tavg_loss = infer_loss/num_data\n",
        "\t\n",
        "\treturn avg_loss, avg_acc, results"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J5uoUeQBDq5Q"
      },
      "source": [
        "## Newton - CG ##"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IDug0aiKCqeG"
      },
      "source": [
        "# import pdb\n",
        "# import tensorflow as tf\n",
        "# import time\n",
        "# import numpy as np\n",
        "# import os\n",
        "# import math\n",
        "# from utilities import predict\n",
        "\n",
        "def Rop(f, weights, v):\n",
        "\t\"\"\"Implementation of R operator\n",
        "\tArgs:\n",
        "\t\tf: any function of weights\n",
        "\t\tweights: list of tensors.\n",
        "\t\tv: vector for right multiplication\n",
        "\tReturns:\n",
        "\t\tJv: Jaccobian vector product, length same as\n",
        "\t\t\tthe number of output of f\n",
        "\t\"\"\"\n",
        "\tif type(f) == list:\n",
        "\t\tu = [tf.zeros_like(ff) for ff in f]\n",
        "\telse:\n",
        "\t\tu = tf.zeros_like(f)  # dummy variable\n",
        "\tg = tf.gradients(ys=f, xs=weights, grad_ys=u)\n",
        "\treturn tf.gradients(ys=g, xs=u, grad_ys=v)\n",
        "\n",
        "def Gauss_Newton_vec(outputs, loss, weights, v):\n",
        "\t\"\"\"Implements Gauss-Newton vector product.\n",
        "\tArgs:\n",
        "\t\tloss: Loss function.\n",
        "\t\toutputs: outputs of the last layer (pre-softmax).\n",
        "\t\tweights: Weights, list of tensors.\n",
        "\t\tv: vector to be multiplied with Gauss Newton matrix\n",
        "\tReturns:\n",
        "\t\tJ'BJv: Guass-Newton vector product.\n",
        "\t\"\"\"\n",
        "\t# Validate the input\n",
        "\tif type(weights) == list:\n",
        "\t\tif len(v) != len(weights):\n",
        "\t\t\traise ValueError(\"weights and v must have the same length.\")\n",
        "\n",
        "\tgrads_outputs = tf.gradients(ys=loss, xs=outputs)\n",
        "\tBJv = Rop(grads_outputs, weights, v)\n",
        "\tJBJv = tf.gradients(ys=outputs, xs=weights, grad_ys=BJv)\n",
        "\treturn JBJv\n",
        "\t\n",
        "\n",
        "class newton_cg(object):\n",
        "\tdef __init__(self, config, sess, outputs, loss):\n",
        "\t\t\"\"\"\n",
        "\t\tinitialize operations and vairables that will be used in newton\n",
        "\t\targs:\n",
        "\t\t\tsess: tensorflow session\n",
        "\t\t\toutputs: output of the neural network (pre-softmax layer)\n",
        "\t\t\tloss: function to calculate loss\n",
        "\t\t\"\"\"\n",
        "\t\tsuper(newton_cg, self).__init__()\n",
        "\t\tself.sess = sess\n",
        "\t\tself.config = config\n",
        "\t\tself.outputs = outputs\n",
        "\t\tself.loss = loss\n",
        "\t\tself.param = tf.compat.v1.trainable_variables()\n",
        "\n",
        "\t\tself.CGiter = 0\n",
        "\t\tFLOAT = tf.float32\n",
        "\t\tmodel_weight = self.vectorize(self.param)\n",
        "\t\t\n",
        "\t\t# initial variable used in CG\n",
        "\t\tzeros = tf.zeros(model_weight.get_shape(), dtype=FLOAT)\n",
        "\t\tself.r = tf.Variable(zeros, dtype=FLOAT, trainable=False)\n",
        "\t\tself.v = tf.Variable(zeros, dtype=FLOAT, trainable=False)\n",
        "\t\tself.s = tf.Variable(zeros, dtype=FLOAT, trainable=False)\n",
        "\t\tself.g = tf.Variable(zeros, dtype=FLOAT, trainable=False)\n",
        "\t\t# initial Gv, f for method minibatch\n",
        "\t\tself.Gv = tf.Variable(zeros, dtype=FLOAT, trainable=False)\n",
        "\t\tself.f = tf.Variable(0., dtype=FLOAT, trainable=False)\n",
        "\n",
        "\t\t# rTr, cgtol and beta to be used in CG\n",
        "\t\tself.rTr = tf.Variable(0., dtype=FLOAT, trainable=False)\n",
        "\t\tself.cgtol = tf.Variable(0., dtype=FLOAT, trainable=False)\n",
        "\t\tself.beta = tf.Variable(0., dtype=FLOAT, trainable=False)\n",
        "\n",
        "\t\t# placeholder alpha, old_alpha and lambda\n",
        "\t\tself.alpha = tf.compat.v1.placeholder(FLOAT, shape=[])\n",
        "\t\tself.old_alpha = tf.compat.v1.placeholder(FLOAT, shape=[])\n",
        "\t\tself._lambda = tf.compat.v1.placeholder(FLOAT, shape=[])\n",
        "\n",
        "\t\tself.num_grad_segment = math.ceil(self.config.num_data/self.config.bsize)\n",
        "\t\tself.num_Gv_segment = math.ceil(self.config.GNsize/self.config.bsize)\n",
        "\n",
        "\t\tcal_loss, cal_lossgrad, cal_lossGv, \\\n",
        "\t\tadd_reg_avg_loss, add_reg_avg_grad, add_reg_avg_Gv, \\\n",
        "\t\tzero_loss, zero_grad, zero_Gv = self._ops_in_minibatch()\n",
        "\n",
        "\t\t# initial operations that will be used in minibatch and newton\n",
        "\t\tself.cal_loss = cal_loss\n",
        "\t\tself.cal_lossgrad = cal_lossgrad\n",
        "\t\tself.cal_lossGv = cal_lossGv\n",
        "\t\tself.add_reg_avg_loss = add_reg_avg_loss\n",
        "\t\tself.add_reg_avg_grad = add_reg_avg_grad\n",
        "\t\tself.add_reg_avg_Gv = add_reg_avg_Gv\n",
        "\t\tself.zero_loss = zero_loss\n",
        "\t\tself.zero_grad = zero_grad\n",
        "\t\tself.zero_Gv = zero_Gv\n",
        "\n",
        "\t\tself.CG, self.update_v = self._CG()\n",
        "\t\tself.init_cg_vars = self._init_cg_vars()\n",
        "\t\tself.update_gs = tf.tensordot(self.s, self.g, axes=1)\n",
        "\t\tself.update_sGs = 0.5*tf.tensordot(self.s, -self.g-self.r-self._lambda*self.s, axes=1)\n",
        "\t\tself.update_model = self._update_model()\n",
        "\t\tself.gnorm = self.calc_norm(self.g)\n",
        "\n",
        "\n",
        "\tdef vectorize(self, tensors):\n",
        "\t\tif isinstance(tensors, list) or isinstance(tensors, tuple):\n",
        "\t\t\tvector = [tf.reshape(tensor, [-1]) for tensor in tensors]\n",
        "\t\t\treturn tf.concat(vector, 0) \n",
        "\t\telse:\n",
        "\t\t\treturn tensors \n",
        "\t\n",
        "\tdef inverse_vectorize(self, vector, param):\n",
        "\t\tif isinstance(vector, list):\n",
        "\t\t\treturn vector\n",
        "\t\telse:\n",
        "\t\t\ttensors = []\n",
        "\t\t\toffset = 0\n",
        "\t\t\tnum_total_param = np.sum([np.prod(p.shape.as_list()) for p in param])\n",
        "\t\t\tfor p in param:\n",
        "\t\t\t\tnumel = np.prod(p.shape.as_list())\n",
        "\t\t\t\ttensors.append(tf.reshape(vector[offset: offset+numel], p.shape))\n",
        "\t\t\t\toffset += numel\n",
        "\n",
        "\t\t\tassert offset == num_total_param\n",
        "\t\t\treturn tensors\n",
        "\n",
        "\tdef calc_norm(self, v):\n",
        "\t\t# default: frobenius norm\n",
        "\t\tif isinstance(v, list):\n",
        "\t\t\tnorm = 0.\n",
        "\t\t\tfor p in v:\n",
        "\t\t\t\tnorm = norm + tf.norm(tensor=p)**2\n",
        "\t\t\treturn norm**0.5\n",
        "\t\telse:\n",
        "\t\t\treturn tf.norm(tensor=v)\n",
        "\n",
        "\tdef _ops_in_minibatch(self):\n",
        "\t\t\"\"\"\n",
        "\t\tDefine operations that will be used in method minibatch\n",
        "\t\tVectorization is already a deep copy operation.\n",
        "\t\tBefore using newton method, loss needs to be summed over training samples\n",
        "\t\tto make results consistent.\n",
        "\t\t\"\"\"\n",
        "\n",
        "\t\tdef cal_loss():\n",
        "\t\t\treturn tf.compat.v1.assign(self.f, self.f + self.loss)\n",
        "\n",
        "\t\tdef cal_lossgrad():\n",
        "\t\t\tupdate_f = tf.compat.v1.assign(self.f, self.f + self.loss)\n",
        "\n",
        "\t\t\tgrad = tf.gradients(ys=self.loss, xs=self.param)\n",
        "\t\t\tgrad = self.vectorize(grad)\n",
        "\t\t\tupdate_grad = tf.compat.v1.assign(self.g, self.g + grad)\n",
        "\n",
        "\t\t\treturn tf.group(*[update_f, update_grad])\n",
        "\n",
        "\t\tdef cal_lossGv():\n",
        "\t\t\tv = self.inverse_vectorize(self.v, self.param)\n",
        "\t\t\tGv = Gauss_Newton_vec(self.outputs, self.loss, self.param, v)\n",
        "\t\t\tGv = self.vectorize(Gv)\n",
        "\t\t\treturn tf.compat.v1.assign(self.Gv, self.Gv + Gv) \n",
        "\n",
        "\t\t# add regularization term to loss, gradient and Gv and further average over batches \n",
        "\t\tdef add_reg_avg_loss():\n",
        "\t\t\tmodel_weight = self.vectorize(self.param)\n",
        "\t\t\treg = (self.calc_norm(model_weight))**2\n",
        "\t\t\treg = 1.0/(2*self.config.C) * reg\n",
        "\t\t\treturn tf.compat.v1.assign(self.f, reg + self.f/self.config.num_data)\n",
        "\n",
        "\t\tdef add_reg_avg_lossgrad():\n",
        "\t\t\tmodel_weight = self.vectorize(self.param)\n",
        "\t\t\treg_grad = model_weight/self.config.C\n",
        "\t\t\treturn tf.compat.v1.assign(self.g, reg_grad + self.g/self.config.num_data)\n",
        "\n",
        "\t\tdef add_reg_avg_lossGv():\n",
        "\t\t\treturn tf.compat.v1.assign(self.Gv, (self._lambda + 1/self.config.C)*self.v\n",
        "\t\t\t + self.Gv/self.config.GNsize) \n",
        "\n",
        "\t\t# zero out loss, grad and Gv \n",
        "\t\tdef zero_loss():\n",
        "\t\t\treturn tf.compat.v1.assign(self.f, tf.zeros_like(self.f))\n",
        "\t\tdef zero_grad():\n",
        "\t\t\treturn tf.compat.v1.assign(self.g, tf.zeros_like(self.g))\n",
        "\t\tdef zero_Gv():\n",
        "\t\t\treturn tf.compat.v1.assign(self.Gv, tf.zeros_like(self.Gv))\n",
        "\n",
        "\t\treturn (cal_loss(), cal_lossgrad(), cal_lossGv(),\n",
        "\t\t\t\tadd_reg_avg_loss(), add_reg_avg_lossgrad(), add_reg_avg_lossGv(),\n",
        "\t\t\t\tzero_loss(), zero_grad(), zero_Gv())\n",
        "\n",
        "\tdef minibatch(self, data_batch, place_holder_x, place_holder_y, mode):\n",
        "\t\t\"\"\"\n",
        "\t\tA function to evaluate either function value, global gradient or sub-sampled Gv\n",
        "\t\t\"\"\"\n",
        "\t\tif mode not in ('funonly', 'fungrad', 'Gv'):\n",
        "\t\t\traise ValueError('Unknown mode other than funonly & fungrad & Gv!')\n",
        "\n",
        "\t\tinputs, labels = data_batch\n",
        "\t\tnum_data = labels.shape[0]\n",
        "\t\tnum_segment = math.ceil(num_data/self.config.bsize)\n",
        "\t\tx, y = place_holder_x, place_holder_y\n",
        "\n",
        "\t\t# before estimation starts, need to zero out f, grad and Gv according to the mode\n",
        "\n",
        "\t\tif mode == 'funonly':\n",
        "\t\t\tassert num_data == self.config.num_data\n",
        "\t\t\tassert num_segment == self.num_grad_segment\n",
        "\t\t\tself.sess.run(self.zero_loss)\n",
        "\t\telif mode == 'fungrad':\n",
        "\t\t\tassert num_data == self.config.num_data\n",
        "\t\t\tassert num_segment == self.num_grad_segment\n",
        "\t\t\tself.sess.run([self.zero_loss, self.zero_grad])\n",
        "\t\telse:\n",
        "\t\t\tassert num_data == self.config.GNsize\n",
        "\t\t\tassert num_segment == self.num_Gv_segment\n",
        "\t\t\tself.sess.run(self.zero_Gv)\n",
        "\n",
        "\t\tfor i in range(num_segment):\n",
        "\t\t\t\n",
        "\t\t\tload_time = time.time()\n",
        "\t\t\tidx = np.arange(i * self.config.bsize, min((i+1) * self.config.bsize, num_data))\n",
        "\t\t\tbatch_input = inputs[idx]\n",
        "\t\t\tbatch_labels = labels[idx]\n",
        "\t\t\tbatch_input = np.ascontiguousarray(batch_input)\n",
        "\t\t\tbatch_labels = np.ascontiguousarray(batch_labels)\n",
        "\t\t\tself.config.elapsed_time += time.time() - load_time\n",
        "\n",
        "\t\t\tif mode == 'funonly':\n",
        "\n",
        "\t\t\t\tself.sess.run(self.cal_loss, feed_dict={\n",
        "\t\t\t\t\t\t\tx: batch_input, \n",
        "\t\t\t\t\t\t\ty: batch_labels,})\n",
        "\n",
        "\t\t\telif mode == 'fungrad':\n",
        "\t\t\t\t\n",
        "\t\t\t\tself.sess.run(self.cal_lossgrad, feed_dict={\n",
        "\t\t\t\t\t\t\tx: batch_input, \n",
        "\t\t\t\t\t\t\ty: batch_labels,})\n",
        "\t\t\t\t\n",
        "\t\t\telse:\n",
        "\t\t\t\t\n",
        "\t\t\t\tself.sess.run(self.cal_lossGv, feed_dict={\n",
        "\t\t\t\t\t\t\tx: batch_input, \n",
        "\t\t\t\t\t\t\ty: batch_labels})\n",
        "\n",
        "\t\t# average over batches\n",
        "\t\tif mode == 'funonly':\n",
        "\t\t\tself.sess.run(self.add_reg_avg_loss)\n",
        "\t\telif mode == 'fungrad':\n",
        "\t\t\tself.sess.run([self.add_reg_avg_loss, self.add_reg_avg_grad])\n",
        "\t\telse:\n",
        "\t\t\tself.sess.run(self.add_reg_avg_Gv, \n",
        "\t\t\t\tfeed_dict={self._lambda: self.config._lambda})\n",
        "\n",
        "\n",
        "\tdef _update_model(self):\n",
        "\t\tupdate_model_ops = []\n",
        "\t\tx = self.inverse_vectorize(self.s, self.param)\n",
        "\t\tfor i, p in enumerate(self.param):\n",
        "\t\t\top = tf.compat.v1.assign(p, p + (self.alpha-self.old_alpha) * x[i])\n",
        "\t\t\tupdate_model_ops.append(op)\n",
        "\t\treturn tf.group(*update_model_ops)\n",
        "\n",
        "\tdef _init_cg_vars(self):\n",
        "\t\tinit_ops = []\n",
        "\n",
        "\t\tinit_r = tf.compat.v1.assign(self.r, -self.g)\n",
        "\t\tinit_v = tf.compat.v1.assign(self.v, -self.g)\n",
        "\t\tinit_s = tf.compat.v1.assign(self.s, tf.zeros_like(self.g))\n",
        "\t\tgnorm = self.calc_norm(self.g)\n",
        "\t\tinit_rTr = tf.compat.v1.assign(self.rTr, gnorm**2)\n",
        "\t\tinit_cgtol = tf.compat.v1.assign(self.cgtol, self.config.xi*gnorm)\n",
        "\n",
        "\t\tinit_ops = [init_r, init_v, init_s, init_rTr, init_cgtol]\n",
        "\n",
        "\t\treturn tf.group(*init_ops)\n",
        "\n",
        "\tdef _CG(self):\n",
        "\t\t\"\"\"\n",
        "\t\tCG:\n",
        "\t\t\tdefine operations that will be used in method newton\n",
        "\t\tSame as the previous loss calculation,\n",
        "\t\tGv has been summed over batches when samples were fed into Neural Network.\n",
        "\t\t\"\"\"\n",
        "\n",
        "\t\tdef CG_ops():\n",
        "\t\t\t\n",
        "\t\t\tvGv = tf.tensordot(self.v, self.Gv, axes=1)\n",
        "\n",
        "\t\t\talpha = self.rTr / vGv\n",
        "\t\t\twith tf.control_dependencies([alpha]):\n",
        "\t\t\t\tupdate_s = tf.compat.v1.assign(self.s, self.s + alpha * self.v, name='update_s_ops')\n",
        "\t\t\t\tupdate_r = tf.compat.v1.assign(self.r, self.r - alpha * self.Gv, name='update_r_ops')\n",
        "\n",
        "\t\t\t\twith tf.control_dependencies([update_s, update_r]):\n",
        "\t\t\t\t\trnewTrnew = self.calc_norm(update_r)**2\n",
        "\t\t\t\t\tupdate_beta = tf.compat.v1.assign(self.beta, rnewTrnew / self.rTr)\n",
        "\t\t\t\t\twith tf.control_dependencies([update_beta]):\n",
        "\t\t\t\t\t\tupdate_rTr = tf.compat.v1.assign(self.rTr, rnewTrnew, name='update_rTr_ops')\n",
        "\n",
        "\t\t\treturn tf.group(*[update_s, update_beta, update_rTr])\n",
        "\n",
        "\t\tdef update_v():\n",
        "\t\t\treturn tf.compat.v1.assign(self.v, self.r + self.beta*self.v, name='update_v')\n",
        "\n",
        "\t\treturn (CG_ops(), update_v())\n",
        "\n",
        "\n",
        "\tdef newton(self, full_batch, val_batch, saver, network, test_network=None):\n",
        "\t\t\"\"\"\n",
        "\t\tConduct newton steps for training\n",
        "\t\targs:\n",
        "\t\t\tfull_batch & val_batch: provide training set and validation set. The function will\n",
        "\t\t\t\tsave the best model evaluted on validation set for future prediction.\n",
        "\t\t\tnetwork: a tuple contains (x, y, loss, outputs).\n",
        "\t\t\ttest_network: a tuple similar to argument network. If you use layers which behave differently\n",
        "\t\t\t\tin test phase such as batchnorm, a separate test_network is needed.\n",
        "\t\treturn:\n",
        "\t\t\tNone\n",
        "\t\t\"\"\"\n",
        "\t\t# check whether data is valid\n",
        "\t\tfull_inputs, full_labels = full_batch\n",
        "\t\tassert full_inputs.shape[0] == full_labels.shape[0]\n",
        "\n",
        "\t\tif full_inputs.shape[0] != self.config.num_data:\n",
        "\t\t\traise ValueError('The number of full batch inputs does not agree with the config argument.\\\n",
        "\t\t\t\t\t\t\tThis is important because global loss is averaged over those inputs')\n",
        "\n",
        "\t\tx, y, _, outputs = network\n",
        "\n",
        "\t\ttf.compat.v1.summary.scalar('loss', self.f)\n",
        "\t\tmerged = tf.compat.v1.summary.merge_all()\n",
        "\t\ttrain_writer = tf.compat.v1.summary.FileWriter('./summary/train', self.sess.graph)\n",
        "\n",
        "\t\tprint(self.config.args)\n",
        "\t\tif not self.config.screen_log_only:\n",
        "\t\t\tlog_file = open(self.config.log_file, 'w')\n",
        "\t\t\tprint(self.config.args, file=log_file)\n",
        "\t\t\n",
        "\t\tself.minibatch(full_batch, x, y, mode='fungrad')\n",
        "\t\tf = self.sess.run(self.f)\n",
        "\t\toutput_str = 'initial f: {:.3f}'.format(f)\n",
        "\t\tprint(output_str)\n",
        "\t\tif not self.config.screen_log_only:\n",
        "\t\t\tprint(output_str, file=log_file)\n",
        "\t\t\n",
        "\t\tbest_acc = 0.0\n",
        "\n",
        "\t\ttotal_running_time = 0.0\n",
        "\t\tself.config.elapsed_time = 0.0\n",
        "\t\ttotal_CG = 0\n",
        "\t\t\n",
        "\t\tfor k in range(self.config.iter_max):\n",
        "\n",
        "\t\t\t# randomly select the batch for Gv estimation\n",
        "\t\t\tidx = np.random.choice(np.arange(0, full_labels.shape[0]),\n",
        "\t\t\t\t\tsize=self.config.GNsize, replace=False)\n",
        "\n",
        "\t\t\tmini_inputs = full_inputs[idx]\n",
        "\t\t\tmini_labels = full_labels[idx]\n",
        "\n",
        "\t\t\tstart = time.time()\n",
        "\n",
        "\t\t\tself.sess.run(self.init_cg_vars)\n",
        "\t\t\tcgtol = self.sess.run(self.cgtol)\n",
        "\n",
        "\t\t\tavg_cg_time = 0.0\n",
        "\t\t\tfor CGiter in range(1, self.config.CGmax+1):\n",
        "\t\t\t\t\n",
        "\t\t\t\tcg_time = time.time()\n",
        "\t\t\t\tself.minibatch((mini_inputs, mini_labels), x, y, mode='Gv')\n",
        "\t\t\t\tavg_cg_time += time.time() - cg_time\n",
        "\t\t\t\t\n",
        "\t\t\t\tself.sess.run(self.CG)\n",
        "\n",
        "\t\t\t\trnewTrnew = self.sess.run(self.rTr)\n",
        "\t\t\t\t\n",
        "\t\t\t\tif rnewTrnew**0.5 <= cgtol or CGiter == self.config.CGmax:\n",
        "\t\t\t\t\tbreak\n",
        "\n",
        "\t\t\t\tself.sess.run(self.update_v)\n",
        "\n",
        "\t\t\tprint('Avg time per Gv iteration: {:.5f} s\\r\\n'.format(avg_cg_time/CGiter))\n",
        "\n",
        "\t\t\tgs, sGs = self.sess.run([self.update_gs, self.update_sGs], feed_dict={\n",
        "\t\t\t\t\tself._lambda: self.config._lambda\n",
        "\t\t\t\t})\n",
        "\t\t\t\n",
        "\t\t\t# line_search\n",
        "\t\t\tf_old = f\n",
        "\t\t\talpha = 1\n",
        "\t\t\twhile True:\n",
        "\n",
        "\t\t\t\told_alpha = 0 if alpha == 1 else alpha/0.5\n",
        "\t\t\t\t\n",
        "\t\t\t\tself.sess.run(self.update_model, feed_dict={\n",
        "\t\t\t\t\tself.alpha:alpha, self.old_alpha:old_alpha\n",
        "\t\t\t\t\t})\n",
        "\n",
        "\t\t\t\tprered = alpha*gs + (alpha**2)*sGs\n",
        "\n",
        "\t\t\t\tself.minibatch(full_batch, x, y, mode='funonly')\n",
        "\t\t\t\tf = self.sess.run(self.f)\n",
        "\n",
        "\t\t\t\tactred = f - f_old\n",
        "\n",
        "\t\t\t\tif actred <= self.config.eta*alpha*gs:\n",
        "\t\t\t\t\tbreak\n",
        "\n",
        "\t\t\t\talpha *= 0.5\n",
        "\n",
        "\t\t\t# update lambda\n",
        "\t\t\tratio = actred / prered\n",
        "\t\t\tif ratio < 0.25:\n",
        "\t\t\t\tself.config._lambda *= self.config.boost\n",
        "\t\t\telif ratio >= 0.75:\n",
        "\t\t\t\tself.config._lambda *= self.config.drop\n",
        "\n",
        "\t\t\tself.minibatch(full_batch, x, y, mode='fungrad')\n",
        "\t\t\tf = self.sess.run(self.f)\n",
        "\n",
        "\t\t\tgnorm = self.sess.run(self.gnorm)\n",
        "\n",
        "\t\t\tsummary = self.sess.run(merged)\n",
        "\t\t\ttrain_writer.add_summary(summary, k)\n",
        "\n",
        "\t\t\t# exclude data loading time for fair comparison\n",
        "\t\t\tend = time.time() \n",
        "\t\t\t\n",
        "\t\t\tend = end - self.config.elapsed_time\n",
        "\t\t\ttotal_running_time += end-start\n",
        "\n",
        "\t\t\tself.config.elapsed_time = 0.0\n",
        "\t\t\t\n",
        "\t\t\ttotal_CG += CGiter\n",
        "\n",
        "\t\t\toutput_str = '{}-iter f: {:.3f} |g|: {:.5f} alpha: {:.3e} ratio: {:.3f} lambda: {:.5f} #CG: {} actred: {:.5f} prered: {:.5f} time: {:.3f}'.\\\n",
        "\t\t\t\t\t\t\tformat(k, f, gnorm, alpha, actred/prered, self.config._lambda, CGiter, actred, prered, end-start)\n",
        "\t\t\tprint(output_str)\n",
        "\t\t\tif not self.config.screen_log_only:\n",
        "\t\t\t\tprint(output_str, file=log_file)\n",
        "\n",
        "\t\t\tif val_batch is not None:\n",
        "\t\t\t\t# Evaluate the performance after every Newton Step\n",
        "\t\t\t\tif test_network == None:\n",
        "\t\t\t\t\tval_loss, val_acc, _ = predict(\n",
        "\t\t\t\t\t\tself.sess, \n",
        "\t\t\t\t\t\tnetwork=(x, y, self.loss, outputs),\n",
        "\t\t\t\t\t\ttest_batch=val_batch,\n",
        "\t\t\t\t\t\tbsize=self.config.bsize,\n",
        "\t\t\t\t\t\t)\n",
        "\t\t\t\telse:\n",
        "\t\t\t\t\t# A separat test network part has not been done...\n",
        "\t\t\t\t\tval_loss, val_acc, _ = predict(\n",
        "\t\t\t\t\t\tself.sess, \n",
        "\t\t\t\t\t\tnetwork=test_network,\n",
        "\t\t\t\t\t\ttest_batch=val_batch,\n",
        "\t\t\t\t\t\tbsize=self.config.bsize\n",
        "\t\t\t\t\t\t)\n",
        "\n",
        "\t\t\t\toutput_str = '\\r\\n {}-iter val_acc: {:.3f}% val_loss {:.3f}\\r\\n'.\\\n",
        "\t\t\t\t\tformat(k, val_acc*100, val_loss)\n",
        "\t\t\t\tprint(output_str)\n",
        "\t\t\t\tif not self.config.screen_log_only:\n",
        "\t\t\t\t\tprint(output_str, file=log_file)\n",
        "\n",
        "\t\t\t\tif val_acc > best_acc:\n",
        "\t\t\t\t\tbest_acc = val_acc\n",
        "\t\t\t\t\tcheckpoint_path = self.config.model_file\n",
        "\t\t\t\t\tsave_path = saver.save(self.sess, checkpoint_path)\n",
        "\t\t\t\t\tprint('Best model saved in {}\\r\\n'.format(save_path))\n",
        "\n",
        "\t\tif val_batch is None:\n",
        "\t\t\tcheckpoint_path = self.config.model_file\n",
        "\t\t\tsave_path = saver.save(self.sess, checkpoint_path)\n",
        "\t\t\tprint('Model at the last iteration saved in {}\\r\\n'.format(save_path))\n",
        "\t\t\toutput_str = 'total_#CG {} | total running time {:.3f}s'.format(total_CG, total_running_time)\n",
        "\t\telse:\n",
        "\t\t\toutput_str = 'Final acc: {:.3f}% | best acc {:.3f}% | total_#CG {} | total running time {:.3f}s'.\\\n",
        "\t\t\t\tformat(val_acc*100, best_acc*100, total_CG, total_running_time)\n",
        "\t\tprint(output_str)\n",
        "\t\tif not self.config.screen_log_only:\n",
        "\t\t\tprint(output_str, file=log_file)\n",
        "\t\t\tlog_file.close()"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xatdN2Zm7BOQ"
      },
      "source": [
        "##Set Train Arguments##"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v_DZPFd4m7Og"
      },
      "source": [
        "if USE_HFO:\n",
        "    # Arguments for HFO - PSSP dataset\n",
        "    train_args = (\"--optim NewtonCG --GNsize 4096 --C 0.01 --net CNN_7layers --bsize 1024 --iter_max 100 \" +\n",
        "              \"--train_set \" + TRAIN_FILE + \" --val_set \" + VALID_FILE + \" --dim \" + \n",
        "              str(HEIGHT) + \" \" + str(WIDTH) + \" \" + str(DEPTH)).split()\n",
        "else:\n",
        "    # Arguments for SGD - PSSP dataset\n",
        "    train_args = (\"--optim SGD --lr 0.05 --momentum 0.01 --C 0.01 --net CNN_4layers --bsize 1024 --epoch_max 1000 \" +\n",
        "              \"--train_set \" + TRAIN_FILE + \" --val_set \" + VALID_FILE + \" --dim \" +\n",
        "              str(HEIGHT) + \" \" + str(WIDTH) + \" \" + str(DEPTH)).split()"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vMCxn-8P5tsH"
      },
      "source": [
        "##Declare Train Function##"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cr528VD1EDj9"
      },
      "source": [
        "# import pdb\n",
        "# import numpy as np\n",
        "# import tensorflow as tf\n",
        "# tf.compat.v1.disable_eager_execution()\n",
        "# import time\n",
        "# import math\n",
        "# import argparse\n",
        "\n",
        "# from net.net import CNN\n",
        "# from newton_cg import newton_cg\n",
        "# from utilities import read_data, predict, ConfigClass, normalize_and_reshape\n",
        "\n",
        "def parse_args():\n",
        "\tparser = argparse.ArgumentParser(description='Newton method on DNN')\n",
        "\tparser.add_argument('--C', dest='C',\n",
        "\t\t\t\t\t  help='regularization term, or so-called weight decay where'+\\\n",
        "\t\t\t\t\t  \t\t'weight_decay = lr/(C*num_of_samples) in this implementation' ,\n",
        "\t\t\t\t\t  default=0.01, type=float)\n",
        "\n",
        "\t# Newton method arguments\n",
        "\tparser.add_argument('--GNsize', dest='GNsize',\n",
        "\t\t\t\t\t  help='number of samples for estimating Gauss-Newton matrix',\n",
        "\t\t\t\t\t  default=4096, type=int)\n",
        "\tparser.add_argument('--iter_max', dest='iter_max',\n",
        "\t\t\t\t\t  help='the maximal number of Newton iterations',\n",
        "\t\t\t\t\t  default=100, type=int)\n",
        "\tparser.add_argument('--xi', dest='xi',\n",
        "\t\t\t\t\t  help='the tolerance in the relative stopping condition for CG',\n",
        "\t\t\t\t\t  default=0.1, type=float)\n",
        "\tparser.add_argument('--drop', dest='drop',\n",
        "\t\t\t\t\t  help='the drop constants for the LM method',\n",
        "\t\t\t\t\t  default=2/3, type=float)\n",
        "\tparser.add_argument('--boost', dest='boost',\n",
        "\t\t\t\t\t  help='the boost constants for the LM method',\n",
        "\t\t\t\t\t  default=3/2, type=float)\n",
        "\tparser.add_argument('--eta', dest='eta',\n",
        "\t\t\t\t\t  help='the parameter for the line search stopping condition',\n",
        "\t\t\t\t\t  default=0.0001, type=float)\n",
        "\tparser.add_argument('--CGmax', dest='CGmax',\n",
        "\t\t\t\t\t  help='the maximal number of CG iterations',\n",
        "\t\t\t\t\t  default=250, type=int)\n",
        "\tparser.add_argument('--lambda', dest='_lambda',\n",
        "\t\t\t\t\t  help='the initial lambda for the LM method',\n",
        "\t\t\t\t\t  default=1, type=float)\n",
        "\n",
        "\t# SGD arguments\n",
        "\tparser.add_argument('--epoch_max', dest='epoch',\n",
        "\t\t\t\t\t  help='number of training epoch',\n",
        "\t\t\t\t\t  default=500, type=int)\n",
        "\tparser.add_argument('--lr', dest='lr',\n",
        "\t\t\t\t\t  help='learning rate',\n",
        "\t\t\t\t\t  default=0.01, type=float)\n",
        "\tparser.add_argument('--decay', dest='lr_decay',\n",
        "\t\t\t\t\t  help='learning rate decay over each mini-batch update',\n",
        "\t\t\t\t\t  default=0, type=float)\n",
        "\tparser.add_argument('--momentum', dest='momentum',\n",
        "\t\t\t\t\t  help='momentum of learning',\n",
        "\t\t\t\t\t  default=0, type=float)\n",
        "\n",
        "\t# Model training arguments\n",
        "\tparser.add_argument('--bsize', dest='bsize',\n",
        "\t\t\t\t\t  help='batch size to evaluate stochastic gradient, Gv, etc. Since the sampled data \\\n",
        "\t\t\t\t\t  for computing Gauss-Newton matrix and etc. might not fit into memeory \\\n",
        "\t\t\t\t\t  for one time, we will split the data into several segements and average\\\n",
        "\t\t\t\t\t  over them.',\n",
        "\t\t\t\t\t  default=1024, type=int)\n",
        "\tparser.add_argument('--net', dest='net',\n",
        "\t\t\t\t\t  help='classifier type',\n",
        "\t\t\t\t\t  default='CNN_4layers', type=str)\n",
        "\tparser.add_argument('--train_set', dest='train_set',\n",
        "\t\t\t\t\t  help='provide the directory of .mat file for training',\n",
        "\t\t\t\t\t  default=None, type=str)\n",
        "\tparser.add_argument('--val_set', dest='val_set',\n",
        "\t\t\t\t\t  help='provide the directory of .mat file for validation',\n",
        "\t\t\t\t\t  default=None, type=str)\n",
        "\tparser.add_argument('--model', dest='model_file',\n",
        "\t\t\t\t\t  help='model saving address',\n",
        "\t\t\t\t\t  default='./saved_model/model.ckpt', type=str)\n",
        "\tparser.add_argument('--log', dest='log_file',\n",
        "\t\t\t\t\t  help='log saving directory',\n",
        "\t\t\t\t\t  default='./running_log/logger.log', type=str)\n",
        "\tparser.add_argument('--screen_log_only', dest='screen_log_only',\n",
        "\t\t\t\t\t  help='screen printing running log instead of storing it',\n",
        "\t\t\t\t\t  action='store_true')\n",
        "\tparser.add_argument('--optim', '-optim', \n",
        "\t\t\t\t\t  help='which optimizer to use: SGD, Adam or NewtonCG',\n",
        "\t\t\t\t\t  default='NewtonCG', type=str)\n",
        "\tparser.add_argument('--loss', dest='loss', \n",
        "\t\t\t\t\t  help='which loss function to use: MSELoss or CrossEntropy',\n",
        "\t\t\t\t\t  default='MSELoss', type=str)\n",
        "\tparser.add_argument('--dim', dest='dim', nargs='+', help='input dimension of data,'+\\\n",
        "\t\t\t\t\t\t'shape must be:  height width num_channels',\n",
        "\t\t\t\t\t  default=[32, 32, 3], type=int)\n",
        "\tparser.add_argument('--seed', dest='seed', help='a nonnegative integer for \\\n",
        "\t\t\t\t\t\treproducibility', type=int)\t \n",
        "\t\n",
        "\targs = parser.parse_args(args=train_args)\n",
        "\treturn args\n",
        "\n",
        "\n",
        "args = parse_args()\n",
        "\n",
        "def init_model(param):\n",
        "\tinit_ops = []\n",
        "\tfor p in param:\n",
        "\t\tif 'kernel' in p.name:\n",
        "\t\t\tweight = np.random.standard_normal(p.shape)* np.sqrt(2.0 / ((np.prod(p.get_shape().as_list()[:-1]))))\n",
        "\t\t\topt = tf.compat.v1.assign(p, weight)\n",
        "\t\telif 'bias' in p.name:\n",
        "\t\t\tzeros = np.zeros(p.shape)\n",
        "\t\t\topt = tf.compat.v1.assign(p, zeros)\n",
        "\t\tinit_ops.append(opt)\n",
        "\treturn tf.group(*init_ops)\n",
        "\n",
        "def gradient_trainer(config, sess, network, full_batch, val_batch, saver, test_network):\n",
        "\tx, y, loss, outputs,  = network\n",
        "\t\n",
        "\tglobal_step = tf.Variable(initial_value=0, trainable=False, name='global_step')\n",
        "\tlearning_rate = tf.compat.v1.placeholder(tf.float32, shape=[], name='learning_rate')\n",
        "\n",
        "\t# Probably not a good way to add regularization.\n",
        "\t# Just to confirm the implementation is the same as MATLAB.\n",
        "\treg = 0.0\n",
        "\tparam = tf.compat.v1.trainable_variables()\n",
        "\tfor p in param:\n",
        "\t\treg = reg + tf.reduce_sum(input_tensor=tf.pow(p,2))\n",
        "\treg_const = 1/(2*config.C)\n",
        "\tbatch_size = tf.compat.v1.cast(tf.shape(x)[0], tf.float32)\n",
        "\tloss_with_reg = reg_const*reg + loss/batch_size\n",
        "\n",
        "\tif config.optim == 'SGD':\n",
        "\t\toptimizer = tf.compat.v1.train.MomentumOptimizer(\n",
        "\t\t\t\t\tlearning_rate=learning_rate, \n",
        "\t\t\t\t\tmomentum=config.momentum).minimize(\n",
        "\t\t\t\t\tloss_with_reg, \n",
        "\t\t\t\t\tglobal_step=global_step)\n",
        "\telif config.optim == 'Adam':\n",
        "\t\toptimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate,\n",
        "\t\t\t\t\t\t\t\tbeta1=0.9,\n",
        "\t\t\t\t\t\t\t\tbeta2=0.999,\n",
        "\t\t\t\t\t\t\t\tepsilon=1e-08).minimize(\n",
        "\t\t\t\t\t\t\t\tloss_with_reg, \n",
        "\t\t\t\t\t\t\t\tglobal_step=global_step)\n",
        "\n",
        "\ttrain_inputs, train_labels = full_batch\n",
        "\tnum_data = train_labels.shape[0]\n",
        "\tnum_iters = math.ceil(num_data/config.bsize)\n",
        "\n",
        "\tprint(config.args)\n",
        "\tif not config.screen_log_only:\n",
        "\t\tlog_file = open(config.log_file, 'w')\n",
        "\t\tprint(config.args, file=log_file)\n",
        "\tsess.run(tf.compat.v1.global_variables_initializer())\n",
        "\t\n",
        "\n",
        "\tprint('-------------- initializing network by methods in He et al. (2015) --------------')\n",
        "\tparam = tf.compat.v1.trainable_variables()\n",
        "\tsess.run(init_model(param))\n",
        "\n",
        "\ttotal_running_time = 0.0\n",
        "\tbest_acc = 0.0\n",
        "\tlr = config.lr\n",
        "\n",
        "\tfor epoch in range(0, args.epoch):\n",
        "\t\t\n",
        "\t\tloss_avg = 0.0\n",
        "\t\tstart = time.time()\n",
        "\n",
        "\t\tfor i in range(num_iters):\n",
        "\t\t\t\n",
        "\t\t\tload_time = time.time()\n",
        "\t\t\t# randomly select the batch\n",
        "\t\t\tidx = np.random.choice(np.arange(0, num_data), \n",
        "\t\t\t\t\tsize=config.bsize, replace=False)\n",
        "\n",
        "\t\t\tbatch_input = train_inputs[idx]\n",
        "\t\t\tbatch_labels = train_labels[idx]\n",
        "\t\t\tbatch_input = np.ascontiguousarray(batch_input)\n",
        "\t\t\tbatch_labels = np.ascontiguousarray(batch_labels)\n",
        "\t\t\tconfig.elapsed_time += time.time() - load_time\n",
        "\n",
        "\t\t\tstep, _, batch_loss= sess.run(\n",
        "\t\t\t\t[global_step, optimizer, loss_with_reg],\n",
        "\t\t\t\tfeed_dict = {x: batch_input, y: batch_labels, learning_rate: lr}\n",
        "\t\t\t\t)\n",
        "\n",
        "\t\t\t# print initial loss\n",
        "\t\t\tif epoch == 0 and i == 0:\n",
        "\t\t\t\toutput_str = 'initial f (reg + avg. loss of 1st batch): {:.3f}'.format(batch_loss)\n",
        "\t\t\t\tprint(output_str)\n",
        "\t\t\t\tif not config.screen_log_only:\n",
        "\t\t\t\t\tprint(output_str, file=log_file)\n",
        "\n",
        "\t\t\tloss_avg = loss_avg + batch_loss\n",
        "\t\t\t# print log every 10% of the iterations\n",
        "\t\t\tif i % math.ceil(num_iters/10) == 0:\n",
        "\t\t\t\tend = time.time()\n",
        "\t\t\t\toutput_str = 'Epoch {}: {}/{} | loss {:.4f} | lr {:.6} | elapsed time {:.3f}'\\\n",
        "\t\t\t\t\t.format(epoch, i, num_iters, batch_loss , lr, end-start)\n",
        "\t\t\t\tprint(output_str)\n",
        "\t\t\t\tif not config.screen_log_only:\n",
        "\t\t\t\t\tprint(output_str, file=log_file)\n",
        "\t\t\t\n",
        "\t\t\t# adjust learning rate for SGD by inverse time decay\n",
        "\t\t\tif args.optim != 'Adam':\n",
        "\t\t\t\tlr = config.lr/(1 + args.lr_decay*step)\n",
        "\n",
        "\t\t# exclude data loading time for fair comparison\n",
        "\t\tepoch_end = time.time() - config.elapsed_time\n",
        "\t\ttotal_running_time += epoch_end - start\n",
        "\t\tconfig.elapsed_time = 0.0\n",
        "\t\t\n",
        "\t\tif val_batch is None:\n",
        "\t\t\toutput_str = 'In epoch {} train loss: {:.3f} | epoch time {:.3f}'\\\n",
        "\t\t\t\t.format(epoch, loss_avg/(i+1), epoch_end-start)\t\t\t\n",
        "\t\telse:\n",
        "\t\t\tif test_network == None:\n",
        "\t\t\t\tval_loss, val_acc, _ = predict(\n",
        "\t\t\t\t\tsess, \n",
        "\t\t\t\t\tnetwork=(x, y, loss, outputs),\n",
        "\t\t\t\t\ttest_batch=val_batch,\n",
        "\t\t\t\t\tbsize=config.bsize\n",
        "\t\t\t\t\t)\n",
        "\t\t\telse:\n",
        "\t\t\t\t# A separat test network part have been done...\n",
        "\t\t\t\tval_loss, val_acc, _ = predict(\n",
        "\t\t\t\t\tsess, \n",
        "\t\t\t\t\tnetwork=test_network,\n",
        "\t\t\t\t\ttest_batch=val_batch,\n",
        "\t\t\t\t\tbsize=config.bsize\n",
        "\t\t\t\t\t)\n",
        "\t\t\t\n",
        "\t\t\toutput_str = 'In epoch {} train loss: {:.3f} | val loss: {:.3f} | val accuracy: {:.3f}% | epoch time {:.3f}'\\\n",
        "\t\t\t\t.format(epoch, loss_avg/(i+1), val_loss, val_acc*100, epoch_end-start)\n",
        "\t\t\n",
        "\t\t\tif val_acc > best_acc:\n",
        "\t\t\t\tbest_acc = val_acc\n",
        "\t\t\t\tcheckpoint_path = config.model_file \n",
        "\t\t\t\tsave_path = saver.save(sess, checkpoint_path)\n",
        "\t\t\t\tprint('Saved best model in {}'.format(save_path))\n",
        "\n",
        "\t\tprint(output_str)\n",
        "\t\tif not config.screen_log_only:\n",
        "\t\t\tprint(output_str, file=log_file)\n",
        "\n",
        "\tif val_batch is None:\n",
        "\t\tcheckpoint_path = config.model_file \n",
        "\t\tsave_path = saver.save(sess, checkpoint_path)\n",
        "\t\tprint('Model at the last iteration saved in {}\\r\\n'.format(save_path))\n",
        "\t\toutput_str = 'total running time {:.3f}s'.format(total_running_time)\n",
        "\telse:\n",
        "\t\toutput_str = 'Final acc: {:.3f}% | best acc {:.3f}% | total running time {:.3f}s'\\\n",
        "\t\t\t.format(val_acc*100, best_acc*100, total_running_time)\n",
        "\t\n",
        "\tprint(output_str)\n",
        "\tif not config.screen_log_only:\n",
        "\t\tprint(output_str, file=log_file)\n",
        "\t\tlog_file.close()\n",
        "\n",
        "def newton_trainer(config, sess, network, full_batch, val_batch, saver, test_network):\n",
        "\n",
        "\t_, _, loss, outputs = network\n",
        "\tnewton_solver = newton_cg(config, sess, outputs, loss)\n",
        "\tsess.run(tf.compat.v1.global_variables_initializer())\n",
        "\n",
        "\tprint('-------------- initializing network by methods in He et al. (2015) --------------')\n",
        "\tparam = tf.compat.v1.trainable_variables()\n",
        "\tsess.run(init_model(param))\n",
        "\tnewton_solver.newton(full_batch, val_batch, saver, network, test_network)\n",
        "\n",
        "\n",
        "def train_model():\n",
        "\tfull_batch, num_cls, label_enum = read_data(filename=args.train_set, dim=args.dim)\n",
        "\t\n",
        "\tif args.val_set is None:\n",
        "\t\tprint('No validation set is provided. Will output model at the last iteration.')\n",
        "\t\tval_batch = None\n",
        "\telse:\n",
        "\t\tval_batch, _, _ = read_data(filename=args.val_set, dim=args.dim, label_enum=label_enum)\n",
        "\n",
        "\tnum_data = full_batch[0].shape[0]\n",
        "\t\n",
        "\tconfig = ConfigClass(args, num_data, num_cls)\n",
        "\n",
        "\tif isinstance(config.seed, int):\n",
        "\t\ttf.compat.v1.random.set_random_seed(config.seed)\n",
        "\t\tnp.random.seed(config.seed)\n",
        "\n",
        "\tif config.net in ('CNN_4layers', 'CNN_7layers', 'VGG11', 'VGG13', 'VGG16','VGG19'):\n",
        "\t\tx, y, outputs = CNN(config.net, num_cls, config.dim)\n",
        "\t\ttest_network = None\n",
        "\telse:\n",
        "\t\traise ValueError('Unrecognized training model')\n",
        "\n",
        "\tif config.loss == 'MSELoss':\n",
        "\t\tloss = tf.reduce_sum(input_tensor=tf.pow(outputs-y, 2))\n",
        "\telse:\n",
        "\t\tloss = tf.reduce_sum(input_tensor=tf.nn.softmax_cross_entropy_with_logits(logits=outputs, labels=y))\n",
        "\t\n",
        "\tnetwork = (x, y, loss, outputs)\n",
        "\n",
        "\tsess_config = tf.compat.v1.ConfigProto()\n",
        "\tsess_config.gpu_options.allow_growth = True\n",
        "\n",
        "\twith tf.compat.v1.Session(config=sess_config) as sess:\n",
        "\t\t\n",
        "\t\tfull_batch[0], mean_tr = normalize_and_reshape(full_batch[0], dim=config.dim, mean_tr=None)\n",
        "\t\tif val_batch is not None:\n",
        "\t\t\tval_batch[0], _ = normalize_and_reshape(val_batch[0], dim=config.dim, mean_tr=mean_tr)\n",
        "\n",
        "\t\tparam = tf.compat.v1.trainable_variables()\n",
        "\n",
        "\t\tmean_param = tf.compat.v1.get_variable(name='mean_tr', initializer=mean_tr, trainable=False, \n",
        "\t\t\t\t\tvalidate_shape=True, use_resource=False)\n",
        "\t\tlabel_enum_var=tf.compat.v1.get_variable(name='label_enum', initializer=label_enum, trainable=False,\n",
        "\t\t\t\t\tvalidate_shape=True, use_resource=False)\n",
        "\t\tsaver = tf.compat.v1.train.Saver(var_list=param+[mean_param])\n",
        "\t\t\n",
        "\t\tif config.optim in ('SGD', 'Adam'):\n",
        "\t\t\tgradient_trainer(\n",
        "\t\t\t\tconfig, sess, network, full_batch, val_batch, saver, test_network)\n",
        "\t\telif config.optim == 'NewtonCG':\n",
        "\t\t\tnewton_trainer(\n",
        "\t\t\t\tconfig, sess, network, full_batch, val_batch, saver, test_network=test_network)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wi6UglYiD7Zd"
      },
      "source": [
        "## Train ##"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cSgKfh4r5lu7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bfdf00ef-38e0-4c29-d4f9-499eae00ebd0"
      },
      "source": [
        "train_model()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "You choose not to specify a random seed.A different result is produced after each run.\n",
            "Saving log to: ./running_log/logger.log\n",
            "No mean of data provided! Normalize images by their own mean.\n",
            "Normalize images according to the provided mean.\n",
            "-------------- initializing network by methods in He et al. (2015) --------------\n",
            "Namespace(C=0.01, CGmax=250, GNsize=4096, _lambda=1, boost=1.5, bsize=1024, dim=[32, 32, 1], drop=0.6666666666666666, epoch=500, eta=0.0001, iter_max=100, log_file='./running_log/logger.log', loss='MSELoss', lr=0.01, lr_decay=0, model_file='./saved_model/model.ckpt', momentum=0, net='CNN_7layers', optim='NewtonCG', screen_log_only=False, seed=None, train_set='/content/drive/MyDrive/Datasets/cb513_protbert_trainSet2.mat', val_set='/content/drive/MyDrive/Datasets/cb513_protbert_testSet2.mat', xi=0.1)\n",
            "initial f: 1.499\n",
            "Avg time per Gv iteration: 1.34551 s\n",
            "\n",
            "0-iter f: 1.223 |g|: 11.46467 alpha: 1.000e+00 ratio: 0.810 lambda: 0.66667 #CG: 2 actred: -0.27641 prered: -0.34142 time: 16.986\n",
            "\n",
            " 0-iter val_acc: 32.617% val_loss 0.731\n",
            "\n",
            "Best model saved in ./saved_model/model.ckpt\n",
            "\n",
            "Avg time per Gv iteration: 1.21330 s\n",
            "\n",
            "1-iter f: 1.146 |g|: 0.50599 alpha: 1.000e+00 ratio: 0.998 lambda: 0.44444 #CG: 1 actred: -0.07606 prered: -0.07621 time: 15.384\n",
            "\n",
            " 1-iter val_acc: 44.198% val_loss 0.646\n",
            "\n",
            "Best model saved in ./saved_model/model.ckpt\n",
            "\n",
            "Avg time per Gv iteration: 1.21164 s\n",
            "\n",
            "2-iter f: 1.145 |g|: 1.12534 alpha: 5.000e-01 ratio: 0.331 lambda: 0.44444 #CG: 4 actred: -0.00101 prered: -0.00305 time: 21.969\n",
            "\n",
            " 2-iter val_acc: 44.198% val_loss 0.647\n",
            "\n",
            "Avg time per Gv iteration: 1.21145 s\n",
            "\n",
            "3-iter f: 1.145 |g|: 0.04815 alpha: 1.000e+00 ratio: 1.000 lambda: 0.29630 #CG: 1 actred: -0.00086 prered: -0.00086 time: 15.361\n",
            "\n",
            " 3-iter val_acc: 44.198% val_loss 0.645\n",
            "\n",
            "Avg time per Gv iteration: 1.21157 s\n",
            "\n",
            "4-iter f: 1.144 |g|: 2.71172 alpha: 1.000e+00 ratio: 0.115 lambda: 0.44444 #CG: 3 actred: -0.00080 prered: -0.00693 time: 17.781\n",
            "\n",
            " 4-iter val_acc: 44.198% val_loss 0.649\n",
            "\n",
            "Avg time per Gv iteration: 1.21228 s\n",
            "\n",
            "5-iter f: 1.140 |g|: 0.05693 alpha: 1.000e+00 ratio: 1.000 lambda: 0.29630 #CG: 1 actred: -0.00408 prered: -0.00408 time: 15.366\n",
            "\n",
            " 5-iter val_acc: 44.198% val_loss 0.644\n",
            "\n",
            "Avg time per Gv iteration: 1.21168 s\n",
            "\n",
            "6-iter f: 1.138 |g|: 1.26666 alpha: 5.000e-01 ratio: 0.503 lambda: 0.29630 #CG: 4 actred: -0.00200 prered: -0.00398 time: 21.946\n",
            "\n",
            " 6-iter val_acc: 44.198% val_loss 0.644\n",
            "\n",
            "Avg time per Gv iteration: 1.21042 s\n",
            "\n",
            "7-iter f: 1.137 |g|: 0.06535 alpha: 1.000e+00 ratio: 0.999 lambda: 0.19753 #CG: 1 actred: -0.00079 prered: -0.00079 time: 15.358\n",
            "\n",
            " 7-iter val_acc: 44.198% val_loss 0.644\n",
            "\n",
            "Avg time per Gv iteration: 1.21202 s\n",
            "\n",
            "8-iter f: 1.132 |g|: 2.05927 alpha: 1.000e+00 ratio: 0.339 lambda: 0.19753 #CG: 5 actred: -0.00479 prered: -0.01414 time: 20.166\n",
            "\n",
            " 8-iter val_acc: 44.198% val_loss 0.646\n",
            "\n",
            "Avg time per Gv iteration: 1.21043 s\n",
            "\n",
            "9-iter f: 1.131 |g|: 0.08245 alpha: 1.000e+00 ratio: 1.000 lambda: 0.13169 #CG: 1 actred: -0.00152 prered: -0.00152 time: 15.366\n",
            "\n",
            " 9-iter val_acc: 44.198% val_loss 0.644\n",
            "\n",
            "Avg time per Gv iteration: 1.21136 s\n",
            "\n",
            "10-iter f: 1.128 |g|: 6.92595 alpha: 1.000e+00 ratio: 0.091 lambda: 0.19753 #CG: 5 actred: -0.00302 prered: -0.03300 time: 20.170\n",
            "\n",
            " 10-iter val_acc: 44.198% val_loss 0.651\n",
            "\n",
            "Avg time per Gv iteration: 1.21062 s\n",
            "\n",
            "11-iter f: 1.121 |g|: 0.10281 alpha: 1.000e+00 ratio: 1.000 lambda: 0.13169 #CG: 1 actred: -0.00663 prered: -0.00663 time: 15.363\n",
            "\n",
            " 11-iter val_acc: 44.198% val_loss 0.643\n",
            "\n",
            "Avg time per Gv iteration: 1.21120 s\n",
            "\n",
            "12-iter f: 1.117 |g|: 8.58416 alpha: 1.000e+00 ratio: 0.158 lambda: 0.19753 #CG: 7 actred: -0.00373 prered: -0.02357 time: 22.579\n",
            "\n",
            " 12-iter val_acc: 44.198% val_loss 0.649\n",
            "\n",
            "Avg time per Gv iteration: 1.21112 s\n",
            "\n",
            "13-iter f: 1.111 |g|: 0.06695 alpha: 1.000e+00 ratio: 1.000 lambda: 0.13169 #CG: 1 actred: -0.00661 prered: -0.00661 time: 15.368\n",
            "\n",
            " 13-iter val_acc: 44.198% val_loss 0.642\n",
            "\n",
            "Avg time per Gv iteration: 1.21098 s\n",
            "\n",
            "14-iter f: 1.107 |g|: 5.35684 alpha: 5.000e-01 ratio: 0.369 lambda: 0.13169 #CG: 4 actred: -0.00317 prered: -0.00860 time: 21.922\n",
            "\n",
            " 14-iter val_acc: 44.198% val_loss 0.644\n",
            "\n",
            "Avg time per Gv iteration: 1.21132 s\n",
            "\n",
            "15-iter f: 1.105 |g|: 0.19012 alpha: 1.000e+00 ratio: 1.000 lambda: 0.08779 #CG: 1 actred: -0.00247 prered: -0.00247 time: 15.361\n",
            "\n",
            " 15-iter val_acc: 44.198% val_loss 0.640\n",
            "\n",
            "Avg time per Gv iteration: 1.21201 s\n",
            "\n",
            "16-iter f: 1.105 |g|: 10.08674 alpha: 5.000e-01 ratio: 0.004 lambda: 0.13169 #CG: 7 actred: -0.00013 prered: -0.03062 time: 25.544\n",
            "\n",
            " 16-iter val_acc: 44.198% val_loss 0.646\n",
            "\n",
            "Avg time per Gv iteration: 1.21155 s\n",
            "\n",
            "17-iter f: 1.099 |g|: 0.10191 alpha: 1.000e+00 ratio: 1.000 lambda: 0.08779 #CG: 1 actred: -0.00578 prered: -0.00578 time: 15.382\n",
            "\n",
            " 17-iter val_acc: 44.198% val_loss 0.640\n",
            "\n",
            "Avg time per Gv iteration: 1.21257 s\n",
            "\n",
            "18-iter f: 1.097 |g|: 4.04515 alpha: 1.250e-01 ratio: 0.258 lambda: 0.08779 #CG: 8 actred: -0.00168 prered: -0.00652 time: 32.751\n",
            "\n",
            " 18-iter val_acc: 44.198% val_loss 0.639\n",
            "\n",
            "Avg time per Gv iteration: 1.21177 s\n",
            "\n",
            "19-iter f: 1.096 |g|: 0.05822 alpha: 1.000e+00 ratio: 1.000 lambda: 0.05853 #CG: 1 actred: -0.00089 prered: -0.00089 time: 15.357\n",
            "\n",
            " 19-iter val_acc: 44.198% val_loss 0.639\n",
            "\n",
            "Avg time per Gv iteration: 1.21217 s\n",
            "\n",
            "20-iter f: 1.096 |g|: 20.59230 alpha: 1.000e+00 ratio: 0.010 lambda: 0.08779 #CG: 6 actred: -0.00037 prered: -0.03742 time: 21.393\n",
            "\n",
            " 20-iter val_acc: 32.617% val_loss 0.660\n",
            "\n",
            "Avg time per Gv iteration: 1.21139 s\n",
            "\n",
            "21-iter f: 1.077 |g|: 0.28638 alpha: 1.000e+00 ratio: 1.000 lambda: 0.05853 #CG: 1 actred: -0.01888 prered: -0.01888 time: 15.365\n",
            "\n",
            " 21-iter val_acc: 44.198% val_loss 0.638\n",
            "\n",
            "Avg time per Gv iteration: 1.21195 s\n",
            "\n",
            "22-iter f: 1.074 |g|: 5.93202 alpha: 2.500e-01 ratio: 0.090 lambda: 0.08779 #CG: 5 actred: -0.00348 prered: -0.03856 time: 26.081\n",
            "\n",
            " 22-iter val_acc: 44.198% val_loss 0.636\n",
            "\n",
            "Avg time per Gv iteration: 1.21280 s\n",
            "\n",
            "23-iter f: 1.072 |g|: 0.17701 alpha: 1.000e+00 ratio: 1.000 lambda: 0.05853 #CG: 1 actred: -0.00147 prered: -0.00147 time: 15.368\n",
            "\n",
            " 23-iter val_acc: 44.198% val_loss 0.636\n",
            "\n",
            "Avg time per Gv iteration: 1.21139 s\n",
            "\n",
            "24-iter f: 1.071 |g|: 9.59462 alpha: 2.500e-01 ratio: 0.062 lambda: 0.08779 #CG: 5 actred: -0.00165 prered: -0.02686 time: 26.107\n",
            "\n",
            " 24-iter val_acc: 46.601% val_loss 0.641\n",
            "\n",
            "Best model saved in ./saved_model/model.ckpt\n",
            "\n",
            "Avg time per Gv iteration: 1.21171 s\n",
            "\n",
            "25-iter f: 1.067 |g|: 0.07841 alpha: 1.000e+00 ratio: 1.000 lambda: 0.05853 #CG: 1 actred: -0.00327 prered: -0.00327 time: 15.378\n",
            "\n",
            " 25-iter val_acc: 44.198% val_loss 0.635\n",
            "\n",
            "Avg time per Gv iteration: 1.21212 s\n",
            "\n",
            "26-iter f: 1.063 |g|: 14.88257 alpha: 5.000e-01 ratio: 0.170 lambda: 0.08779 #CG: 8 actred: -0.00451 prered: -0.02659 time: 26.780\n",
            "\n",
            " 26-iter val_acc: 34.935% val_loss 0.644\n",
            "\n",
            "Avg time per Gv iteration: 1.21181 s\n",
            "\n",
            "27-iter f: 1.055 |g|: 0.63789 alpha: 1.000e+00 ratio: 1.000 lambda: 0.05853 #CG: 1 actred: -0.00787 prered: -0.00787 time: 15.376\n",
            "\n",
            " 27-iter val_acc: 44.198% val_loss 0.632\n",
            "\n",
            "Avg time per Gv iteration: 1.21173 s\n",
            "\n",
            "28-iter f: 1.054 |g|: 1.98328 alpha: 1.250e-01 ratio: 0.034 lambda: 0.08779 #CG: 3 actred: -0.00065 prered: -0.01887 time: 26.680\n",
            "\n",
            " 28-iter val_acc: 44.198% val_loss 0.631\n",
            "\n",
            "Avg time per Gv iteration: 1.21362 s\n",
            "\n",
            "29-iter f: 1.054 |g|: 0.15781 alpha: 1.000e+00 ratio: 1.000 lambda: 0.05853 #CG: 1 actred: -0.00014 prered: -0.00014 time: 15.373\n",
            "\n",
            " 29-iter val_acc: 44.198% val_loss 0.631\n",
            "\n",
            "Avg time per Gv iteration: 1.21164 s\n",
            "\n",
            "30-iter f: 1.052 |g|: 4.52567 alpha: 6.250e-02 ratio: 0.413 lambda: 0.05853 #CG: 5 actred: -0.00180 prered: -0.00436 time: 32.070\n",
            "\n",
            " 30-iter val_acc: 44.198% val_loss 0.629\n",
            "\n",
            "Avg time per Gv iteration: 1.21150 s\n",
            "\n",
            "31-iter f: 1.052 |g|: 0.16213 alpha: 1.000e+00 ratio: 1.000 lambda: 0.03902 #CG: 1 actred: -0.00073 prered: -0.00073 time: 15.380\n",
            "\n",
            " 31-iter val_acc: 44.294% val_loss 0.629\n",
            "\n",
            "Avg time per Gv iteration: 1.21176 s\n",
            "\n",
            "32-iter f: 1.044 |g|: 17.27300 alpha: 5.000e-01 ratio: 0.131 lambda: 0.05853 #CG: 5 actred: -0.00768 prered: -0.05877 time: 23.171\n",
            "\n",
            " 32-iter val_acc: 44.572% val_loss 0.637\n",
            "\n",
            "Avg time per Gv iteration: 1.21343 s\n",
            "\n",
            "33-iter f: 1.036 |g|: 0.80360 alpha: 1.000e+00 ratio: 1.000 lambda: 0.03902 #CG: 1 actred: -0.00823 prered: -0.00823 time: 15.390\n",
            "\n",
            " 33-iter val_acc: 44.499% val_loss 0.625\n",
            "\n",
            "Avg time per Gv iteration: 1.21173 s\n",
            "\n",
            "34-iter f: 1.035 |g|: 3.40956 alpha: 6.250e-02 ratio: 0.034 lambda: 0.05853 #CG: 5 actred: -0.00037 prered: -0.01084 time: 32.053\n",
            "\n",
            " 34-iter val_acc: 47.229% val_loss 0.626\n",
            "\n",
            "Best model saved in ./saved_model/model.ckpt\n",
            "\n",
            "Avg time per Gv iteration: 1.21159 s\n",
            "\n",
            "35-iter f: 1.035 |g|: 0.08267 alpha: 1.000e+00 ratio: 1.000 lambda: 0.03902 #CG: 1 actred: -0.00032 prered: -0.00032 time: 15.382\n",
            "\n",
            " 35-iter val_acc: 44.608% val_loss 0.625\n",
            "\n",
            "Avg time per Gv iteration: 1.21231 s\n",
            "\n",
            "36-iter f: 1.035 |g|: 18.15976 alpha: 1.250e-01 ratio: 0.008 lambda: 0.05853 #CG: 8 actred: -0.00008 prered: -0.00990 time: 32.726\n",
            "\n",
            " 36-iter val_acc: 52.119% val_loss 0.631\n",
            "\n",
            "Best model saved in ./saved_model/model.ckpt\n",
            "\n",
            "Avg time per Gv iteration: 1.21179 s\n",
            "\n",
            "37-iter f: 1.028 |g|: 0.19424 alpha: 1.000e+00 ratio: 1.000 lambda: 0.03902 #CG: 1 actred: -0.00736 prered: -0.00736 time: 15.387\n",
            "\n",
            " 37-iter val_acc: 46.540% val_loss 0.620\n",
            "\n",
            "Avg time per Gv iteration: 1.21158 s\n",
            "\n",
            "38-iter f: 1.023 |g|: 16.56602 alpha: 2.500e-01 ratio: 0.136 lambda: 0.05853 #CG: 11 actred: -0.00412 prered: -0.03018 time: 33.366\n",
            "\n",
            " 38-iter val_acc: 45.719% val_loss 0.621\n",
            "\n",
            "Avg time per Gv iteration: 1.21146 s\n",
            "\n",
            "39-iter f: 1.018 |g|: 0.09876 alpha: 1.000e+00 ratio: 1.000 lambda: 0.03902 #CG: 1 actred: -0.00528 prered: -0.00528 time: 15.380\n",
            "\n",
            " 39-iter val_acc: 48.412% val_loss 0.616\n",
            "\n",
            "Avg time per Gv iteration: 1.21226 s\n",
            "\n",
            "40-iter f: 1.000 |g|: 34.98455 alpha: 5.000e-01 ratio: 0.411 lambda: 0.03902 #CG: 9 actred: -0.01785 prered: -0.04339 time: 27.989\n",
            "\n",
            " 40-iter val_acc: 47.180% val_loss 0.614\n",
            "\n",
            "Avg time per Gv iteration: 1.21175 s\n",
            "\n",
            "41-iter f: 0.986 |g|: 0.31613 alpha: 1.000e+00 ratio: 1.000 lambda: 0.02601 #CG: 1 actred: -0.01453 prered: -0.01453 time: 15.401\n",
            "\n",
            " 41-iter val_acc: 55.549% val_loss 0.594\n",
            "\n",
            "Best model saved in ./saved_model/model.ckpt\n",
            "\n",
            "Avg time per Gv iteration: 1.21197 s\n",
            "\n",
            "42-iter f: 0.967 |g|: 20.91807 alpha: 2.500e-01 ratio: 0.391 lambda: 0.02601 #CG: 10 actred: -0.01909 prered: -0.04882 time: 32.182\n",
            "\n",
            " 42-iter val_acc: 59.220% val_loss 0.583\n",
            "\n",
            "Best model saved in ./saved_model/model.ckpt\n",
            "\n",
            "Avg time per Gv iteration: 1.21164 s\n",
            "\n",
            "43-iter f: 0.962 |g|: 0.81304 alpha: 1.000e+00 ratio: 1.000 lambda: 0.01734 #CG: 1 actred: -0.00441 prered: -0.00441 time: 15.384\n",
            "\n",
            " 43-iter val_acc: 59.244% val_loss 0.577\n",
            "\n",
            "Best model saved in ./saved_model/model.ckpt\n",
            "\n",
            "Avg time per Gv iteration: 1.21231 s\n",
            "\n",
            "44-iter f: 0.960 |g|: 24.19407 alpha: 1.250e-01 ratio: 0.103 lambda: 0.02601 #CG: 5 actred: -0.00192 prered: -0.01876 time: 29.111\n",
            "\n",
            " 44-iter val_acc: 55.162% val_loss 0.573\n",
            "\n",
            "Avg time per Gv iteration: 1.21198 s\n",
            "\n",
            "45-iter f: 0.954 |g|: 0.17036 alpha: 1.000e+00 ratio: 1.000 lambda: 0.01734 #CG: 1 actred: -0.00624 prered: -0.00624 time: 15.395\n",
            "\n",
            " 45-iter val_acc: 60.005% val_loss 0.570\n",
            "\n",
            "Best model saved in ./saved_model/model.ckpt\n",
            "\n",
            "Avg time per Gv iteration: 1.21282 s\n",
            "\n",
            "46-iter f: 0.854 |g|: 48.25637 alpha: 5.000e-01 ratio: 1.111 lambda: 0.01156 #CG: 9 actred: -0.09980 prered: -0.08983 time: 28.021\n",
            "\n",
            " 46-iter val_acc: 63.664% val_loss 0.488\n",
            "\n",
            "Best model saved in ./saved_model/model.ckpt\n",
            "\n",
            "Avg time per Gv iteration: 1.21317 s\n",
            "\n",
            "47-iter f: 0.835 |g|: 0.71336 alpha: 1.000e+00 ratio: 1.000 lambda: 0.00771 #CG: 1 actred: -0.01952 prered: -0.01952 time: 15.387\n",
            "\n",
            " 47-iter val_acc: 67.866% val_loss 0.467\n",
            "\n",
            "Best model saved in ./saved_model/model.ckpt\n",
            "\n",
            "Avg time per Gv iteration: 1.21251 s\n",
            "\n",
            "48-iter f: 0.802 |g|: 27.53933 alpha: 2.500e-01 ratio: 0.849 lambda: 0.00514 #CG: 16 actred: -0.03296 prered: -0.03884 time: 39.411\n",
            "\n",
            " 48-iter val_acc: 72.358% val_loss 0.442\n",
            "\n",
            "Best model saved in ./saved_model/model.ckpt\n",
            "\n",
            "Avg time per Gv iteration: 1.21233 s\n",
            "\n",
            "49-iter f: 0.796 |g|: 0.53351 alpha: 1.000e+00 ratio: 1.000 lambda: 0.00343 #CG: 1 actred: -0.00541 prered: -0.00541 time: 15.408\n",
            "\n",
            " 49-iter val_acc: 71.694% val_loss 0.437\n",
            "\n",
            "Avg time per Gv iteration: 1.21215 s\n",
            "\n",
            "50-iter f: 0.674 |g|: 24.22828 alpha: 5.000e-01 ratio: 1.001 lambda: 0.00228 #CG: 38 actred: -0.12230 prered: -0.12218 time: 62.983\n",
            "\n",
            " 50-iter val_acc: 74.931% val_loss 0.375\n",
            "\n",
            "Best model saved in ./saved_model/model.ckpt\n",
            "\n",
            "Avg time per Gv iteration: 1.21233 s\n",
            "\n",
            "51-iter f: 0.668 |g|: 1.24333 alpha: 1.000e+00 ratio: 1.000 lambda: 0.00152 #CG: 1 actred: -0.00613 prered: -0.00613 time: 15.395\n",
            "\n",
            " 51-iter val_acc: 76.199% val_loss 0.372\n",
            "\n",
            "Best model saved in ./saved_model/model.ckpt\n",
            "\n",
            "Avg time per Gv iteration: 1.21174 s\n",
            "\n",
            "52-iter f: 0.665 |g|: 23.74899 alpha: 1.000e+00 ratio: 0.241 lambda: 0.00228 #CG: 4 actred: -0.00291 prered: -0.01205 time: 18.996\n",
            "\n",
            " 52-iter val_acc: 75.752% val_loss 0.366\n",
            "\n",
            "Avg time per Gv iteration: 1.21180 s\n",
            "\n",
            "53-iter f: 0.660 |g|: 1.04968 alpha: 1.000e+00 ratio: 1.000 lambda: 0.00152 #CG: 1 actred: -0.00503 prered: -0.00503 time: 15.377\n",
            "\n",
            " 53-iter val_acc: 76.428% val_loss 0.364\n",
            "\n",
            "Best model saved in ./saved_model/model.ckpt\n",
            "\n",
            "Avg time per Gv iteration: 1.21182 s\n",
            "\n",
            "54-iter f: 0.658 |g|: 10.66122 alpha: 1.000e+00 ratio: 0.206 lambda: 0.00228 #CG: 5 actred: -0.00177 prered: -0.00859 time: 20.201\n",
            "\n",
            " 54-iter val_acc: 76.537% val_loss 0.362\n",
            "\n",
            "Best model saved in ./saved_model/model.ckpt\n",
            "\n",
            "Avg time per Gv iteration: 1.21235 s\n",
            "\n",
            "55-iter f: 0.657 |g|: 0.95379 alpha: 1.000e+00 ratio: 1.000 lambda: 0.00152 #CG: 1 actred: -0.00118 prered: -0.00118 time: 15.381\n",
            "\n",
            " 55-iter val_acc: 76.742% val_loss 0.362\n",
            "\n",
            "Best model saved in ./saved_model/model.ckpt\n",
            "\n",
            "Avg time per Gv iteration: 1.21169 s\n",
            "\n",
            "56-iter f: 0.653 |g|: 1.76930 alpha: 1.000e+00 ratio: 0.969 lambda: 0.00101 #CG: 4 actred: -0.00421 prered: -0.00435 time: 18.996\n",
            "\n",
            " 56-iter val_acc: 76.488% val_loss 0.358\n",
            "\n",
            "Avg time per Gv iteration: 1.21095 s\n",
            "\n",
            "57-iter f: 0.653 |g|: 0.10123 alpha: 1.000e+00 ratio: 0.998 lambda: 0.00068 #CG: 1 actred: -0.00003 prered: -0.00003 time: 15.373\n",
            "\n",
            " 57-iter val_acc: 76.597% val_loss 0.358\n",
            "\n",
            "Avg time per Gv iteration: 1.21214 s\n",
            "\n",
            "58-iter f: 0.606 |g|: 36.18867 alpha: 2.500e-01 ratio: 0.501 lambda: 0.00068 #CG: 109 actred: -0.04675 prered: -0.09329 time: 151.533\n",
            "\n",
            " 58-iter val_acc: 74.641% val_loss 0.387\n",
            "\n",
            "Avg time per Gv iteration: 1.21279 s\n",
            "\n",
            "59-iter f: 0.561 |g|: 0.66784 alpha: 1.000e+00 ratio: 1.000 lambda: 0.00045 #CG: 2 actred: -0.04472 prered: -0.04474 time: 16.573\n",
            "\n",
            " 59-iter val_acc: 77.793% val_loss 0.348\n",
            "\n",
            "Best model saved in ./saved_model/model.ckpt\n",
            "\n",
            "Avg time per Gv iteration: 1.21171 s\n",
            "\n",
            "60-iter f: 0.530 |g|: 17.14250 alpha: 2.500e-01 ratio: 0.530 lambda: 0.00045 #CG: 56 actred: -0.03101 prered: -0.05852 time: 87.595\n",
            "\n",
            " 60-iter val_acc: 76.730% val_loss 0.363\n",
            "\n",
            "Avg time per Gv iteration: 1.21195 s\n",
            "\n",
            "61-iter f: 0.509 |g|: 0.31969 alpha: 1.000e+00 ratio: 1.000 lambda: 0.00030 #CG: 2 actred: -0.02095 prered: -0.02095 time: 16.579\n",
            "\n",
            " 61-iter val_acc: 78.300% val_loss 0.343\n",
            "\n",
            "Best model saved in ./saved_model/model.ckpt\n",
            "\n",
            "Avg time per Gv iteration: 1.21191 s\n",
            "\n",
            "62-iter f: 0.486 |g|: 11.88863 alpha: 1.250e-01 ratio: 0.769 lambda: 0.00020 #CG: 88 actred: -0.02331 prered: -0.03032 time: 129.169\n",
            "\n",
            " 62-iter val_acc: 78.384% val_loss 0.347\n",
            "\n",
            "Best model saved in ./saved_model/model.ckpt\n",
            "\n",
            "Avg time per Gv iteration: 1.21073 s\n",
            "\n",
            "63-iter f: 0.480 |g|: 0.29327 alpha: 1.000e+00 ratio: 0.998 lambda: 0.00013 #CG: 2 actred: -0.00592 prered: -0.00593 time: 16.535\n",
            "\n",
            " 63-iter val_acc: 78.686% val_loss 0.338\n",
            "\n",
            "Best model saved in ./saved_model/model.ckpt\n",
            "\n",
            "Avg time per Gv iteration: 1.21176 s\n",
            "\n",
            "64-iter f: 0.475 |g|: 35.80779 alpha: 2.500e-01 ratio: 0.107 lambda: 0.00020 #CG: 92 actred: -0.00552 prered: -0.05146 time: 130.897\n",
            "\n",
            " 64-iter val_acc: 75.196% val_loss 0.369\n",
            "\n",
            "Avg time per Gv iteration: 1.21249 s\n",
            "\n",
            "65-iter f: 0.433 |g|: 2.05795 alpha: 1.000e+00 ratio: 0.982 lambda: 0.00013 #CG: 2 actred: -0.04209 prered: -0.04287 time: 16.547\n",
            "\n",
            " 65-iter val_acc: 79.085% val_loss 0.335\n",
            "\n",
            "Best model saved in ./saved_model/model.ckpt\n",
            "\n",
            "Avg time per Gv iteration: 1.21177 s\n",
            "\n",
            "66-iter f: 0.431 |g|: 4.13240 alpha: 1.000e+00 ratio: 0.473 lambda: 0.00013 #CG: 3 actred: -0.00215 prered: -0.00454 time: 17.763\n",
            "\n",
            " 66-iter val_acc: 78.988% val_loss 0.332\n",
            "\n",
            "Avg time per Gv iteration: 1.21107 s\n",
            "\n",
            "67-iter f: 0.430 |g|: 0.11607 alpha: 1.000e+00 ratio: 0.995 lambda: 0.00009 #CG: 2 actred: -0.00051 prered: -0.00051 time: 16.550\n",
            "\n",
            " 67-iter val_acc: 78.940% val_loss 0.332\n",
            "\n",
            "Avg time per Gv iteration: 1.21164 s\n",
            "\n",
            "68-iter f: 0.417 |g|: 13.47865 alpha: 1.250e-01 ratio: 0.689 lambda: 0.00009 #CG: 137 actred: -0.01282 prered: -0.01861 time: 188.041\n",
            "\n",
            " 68-iter val_acc: 78.686% val_loss 0.338\n",
            "\n",
            "Avg time per Gv iteration: 1.21135 s\n",
            "\n",
            "69-iter f: 0.412 |g|: 0.16125 alpha: 1.000e+00 ratio: 0.998 lambda: 0.00006 #CG: 2 actred: -0.00471 prered: -0.00472 time: 16.535\n",
            "\n",
            " 69-iter val_acc: 79.193% val_loss 0.331\n",
            "\n",
            "Best model saved in ./saved_model/model.ckpt\n",
            "\n",
            "Avg time per Gv iteration: 1.21026 s\n",
            "\n",
            "70-iter f: 0.403 |g|: 31.66569 alpha: 2.500e-01 ratio: 0.335 lambda: 0.00006 #CG: 134 actred: -0.00939 prered: -0.02803 time: 181.400\n",
            "\n",
            " 70-iter val_acc: 78.107% val_loss 0.342\n",
            "\n",
            "Avg time per Gv iteration: 1.21142 s\n",
            "\n",
            "71-iter f: 0.392 |g|: 1.94183 alpha: 1.000e+00 ratio: 0.987 lambda: 0.00004 #CG: 1 actred: -0.01135 prered: -0.01150 time: 15.339\n",
            "\n",
            " 71-iter val_acc: 79.145% val_loss 0.334\n",
            "\n",
            "Avg time per Gv iteration: 1.21152 s\n",
            "\n",
            "72-iter f: 0.389 |g|: 3.35678 alpha: 5.000e-01 ratio: 0.743 lambda: 0.00004 #CG: 5 actred: -0.00263 prered: -0.00354 time: 23.092\n",
            "\n",
            " 72-iter val_acc: 79.254% val_loss 0.331\n",
            "\n",
            "Best model saved in ./saved_model/model.ckpt\n",
            "\n",
            "Avg time per Gv iteration: 1.21226 s\n",
            "\n",
            "73-iter f: 0.389 |g|: 0.19665 alpha: 1.000e+00 ratio: 0.997 lambda: 0.00003 #CG: 2 actred: -0.00039 prered: -0.00039 time: 16.546\n",
            "\n",
            " 73-iter val_acc: 79.145% val_loss 0.330\n",
            "\n",
            "Avg time per Gv iteration: 1.21101 s\n",
            "\n",
            "74-iter f: 0.388 |g|: 20.82586 alpha: 2.500e-01 ratio: 0.037 lambda: 0.00004 #CG: 147 actred: -0.00069 prered: -0.01877 time: 197.149\n",
            "\n",
            " 74-iter val_acc: 78.940% val_loss 0.346\n",
            "\n",
            "Avg time per Gv iteration: 1.21256 s\n",
            "\n",
            "75-iter f: 0.382 |g|: 0.60406 alpha: 1.000e+00 ratio: 0.991 lambda: 0.00003 #CG: 2 actred: -0.00622 prered: -0.00628 time: 16.581\n",
            "\n",
            " 75-iter val_acc: 79.193% val_loss 0.336\n",
            "\n",
            "Avg time per Gv iteration: 1.21045 s\n",
            "\n",
            "76-iter f: 0.378 |g|: 3.59508 alpha: 2.500e-01 ratio: 0.716 lambda: 0.00003 #CG: 11 actred: -0.00360 prered: -0.00502 time: 33.270\n",
            "\n",
            " 76-iter val_acc: 79.133% val_loss 0.332\n",
            "\n",
            "Avg time per Gv iteration: 1.21126 s\n",
            "\n",
            "77-iter f: 0.378 |g|: 0.26346 alpha: 1.000e+00 ratio: 1.000 lambda: 0.00002 #CG: 2 actred: -0.00034 prered: -0.00034 time: 16.538\n",
            "\n",
            " 77-iter val_acc: 79.290% val_loss 0.333\n",
            "\n",
            "Best model saved in ./saved_model/model.ckpt\n",
            "\n",
            "Avg time per Gv iteration: 1.21065 s\n",
            "\n",
            "78-iter f: 0.374 |g|: 21.97079 alpha: 1.250e-01 ratio: 0.545 lambda: 0.00002 #CG: 111 actred: -0.00391 prered: -0.00717 time: 156.682\n",
            "\n",
            " 78-iter val_acc: 79.024% val_loss 0.331\n",
            "\n",
            "Avg time per Gv iteration: 1.21178 s\n",
            "\n",
            "79-iter f: 0.369 |g|: 1.11220 alpha: 1.000e+00 ratio: 0.995 lambda: 0.00001 #CG: 1 actred: -0.00505 prered: -0.00508 time: 15.350\n",
            "\n",
            " 79-iter val_acc: 79.242% val_loss 0.328\n",
            "\n",
            "Avg time per Gv iteration: 1.21042 s\n",
            "\n",
            "80-iter f: 0.368 |g|: 8.71216 alpha: 1.000e+00 ratio: 0.234 lambda: 0.00002 #CG: 4 actred: -0.00045 prered: -0.00191 time: 18.948\n",
            "\n",
            " 80-iter val_acc: 79.097% val_loss 0.329\n",
            "\n",
            "Avg time per Gv iteration: 1.21108 s\n",
            "\n",
            "81-iter f: 0.368 |g|: 0.50644 alpha: 1.000e+00 ratio: 0.994 lambda: 0.00001 #CG: 1 actred: -0.00077 prered: -0.00077 time: 15.343\n",
            "\n",
            " 81-iter val_acc: 79.326% val_loss 0.327\n",
            "\n",
            "Best model saved in ./saved_model/model.ckpt\n",
            "\n",
            "Avg time per Gv iteration: 1.21026 s\n",
            "\n",
            "82-iter f: 0.366 |g|: 4.18832 alpha: 5.000e-01 ratio: 0.844 lambda: 0.00001 #CG: 6 actred: -0.00151 prered: -0.00179 time: 24.291\n",
            "\n",
            " 82-iter val_acc: 79.338% val_loss 0.326\n",
            "\n",
            "Best model saved in ./saved_model/model.ckpt\n",
            "\n",
            "Avg time per Gv iteration: 1.21069 s\n",
            "\n",
            "83-iter f: 0.366 |g|: 0.27225 alpha: 1.000e+00 ratio: 0.993 lambda: 0.00001 #CG: 1 actred: -0.00015 prered: -0.00015 time: 15.360\n",
            "\n",
            " 83-iter val_acc: 79.302% val_loss 0.326\n",
            "\n",
            "Avg time per Gv iteration: 1.21170 s\n",
            "\n",
            "84-iter f: 0.366 |g|: 5.83975 alpha: 5.000e-01 ratio: 0.229 lambda: 0.00001 #CG: 9 actred: -0.00019 prered: -0.00081 time: 27.924\n",
            "\n",
            " 84-iter val_acc: 79.399% val_loss 0.325\n",
            "\n",
            "Best model saved in ./saved_model/model.ckpt\n",
            "\n",
            "Avg time per Gv iteration: 1.21170 s\n",
            "\n",
            "85-iter f: 0.366 |g|: 0.32661 alpha: 1.000e+00 ratio: 0.994 lambda: 0.00001 #CG: 1 actred: -0.00033 prered: -0.00033 time: 15.343\n",
            "\n",
            " 85-iter val_acc: 79.290% val_loss 0.325\n",
            "\n",
            "Avg time per Gv iteration: 1.21066 s\n",
            "\n",
            "86-iter f: 0.365 |g|: 4.40578 alpha: 2.500e-01 ratio: 0.182 lambda: 0.00001 #CG: 6 actred: -0.00008 prered: -0.00046 time: 27.247\n",
            "\n",
            " 86-iter val_acc: 79.338% val_loss 0.325\n",
            "\n",
            "Avg time per Gv iteration: 1.21281 s\n",
            "\n",
            "87-iter f: 0.365 |g|: 0.29467 alpha: 1.000e+00 ratio: 0.998 lambda: 0.00001 #CG: 1 actred: -0.00016 prered: -0.00016 time: 15.346\n",
            "\n",
            " 87-iter val_acc: 79.338% val_loss 0.325\n",
            "\n",
            "Avg time per Gv iteration: 1.21023 s\n",
            "\n",
            "88-iter f: 0.365 |g|: 0.02775 alpha: 1.000e+00 ratio: 0.997 lambda: 0.00000 #CG: 2 actred: -0.00002 prered: -0.00002 time: 16.539\n",
            "\n",
            " 88-iter val_acc: 79.326% val_loss 0.325\n",
            "\n",
            "Avg time per Gv iteration: 1.21189 s\n",
            "\n",
            "89-iter f: 0.363 |g|: 15.59670 alpha: 5.000e-01 ratio: 0.135 lambda: 0.00001 #CG: 211 actred: -0.00218 prered: -0.01620 time: 271.333\n",
            "\n",
            " 89-iter val_acc: 79.278% val_loss 0.336\n",
            "\n",
            "Avg time per Gv iteration: 1.21040 s\n",
            "\n",
            "90-iter f: 0.362 |g|: 1.14641 alpha: 1.000e+00 ratio: 0.995 lambda: 0.00000 #CG: 1 actred: -0.00131 prered: -0.00131 time: 15.340\n",
            "\n",
            " 90-iter val_acc: 79.278% val_loss 0.333\n",
            "\n",
            "Avg time per Gv iteration: 1.21226 s\n",
            "\n",
            "91-iter f: 0.359 |g|: 10.71891 alpha: 5.000e-01 ratio: 0.439 lambda: 0.00000 #CG: 6 actred: -0.00314 prered: -0.00716 time: 24.332\n",
            "\n",
            " 91-iter val_acc: 79.302% val_loss 0.329\n",
            "\n",
            "Avg time per Gv iteration: 1.21173 s\n",
            "\n",
            "92-iter f: 0.358 |g|: 0.80089 alpha: 1.000e+00 ratio: 0.987 lambda: 0.00000 #CG: 1 actred: -0.00075 prered: -0.00075 time: 15.356\n",
            "\n",
            " 92-iter val_acc: 79.145% val_loss 0.329\n",
            "\n",
            "Avg time per Gv iteration: 1.21175 s\n",
            "\n",
            "93-iter f: 0.357 |g|: 10.05485 alpha: 2.500e-01 ratio: 0.524 lambda: 0.00000 #CG: 6 actred: -0.00121 prered: -0.00230 time: 27.294\n",
            "\n",
            " 93-iter val_acc: 79.435% val_loss 0.327\n",
            "\n",
            "Best model saved in ./saved_model/model.ckpt\n",
            "\n",
            "Avg time per Gv iteration: 1.21088 s\n",
            "\n",
            "94-iter f: 0.356 |g|: 0.54739 alpha: 1.000e+00 ratio: 0.995 lambda: 0.00000 #CG: 1 actred: -0.00056 prered: -0.00056 time: 15.347\n",
            "\n",
            " 94-iter val_acc: 79.157% val_loss 0.327\n",
            "\n",
            "Avg time per Gv iteration: 1.21292 s\n",
            "\n",
            "95-iter f: 0.355 |g|: 7.23655 alpha: 2.500e-01 ratio: 0.748 lambda: 0.00000 #CG: 6 actred: -0.00125 prered: -0.00167 time: 27.284\n",
            "\n",
            " 95-iter val_acc: 79.314% val_loss 0.326\n",
            "\n",
            "Avg time per Gv iteration: 1.21224 s\n",
            "\n",
            "96-iter f: 0.355 |g|: 0.35223 alpha: 1.000e+00 ratio: 0.999 lambda: 0.00000 #CG: 1 actred: -0.00026 prered: -0.00026 time: 15.344\n",
            "\n",
            " 96-iter val_acc: 79.266% val_loss 0.326\n",
            "\n",
            "Avg time per Gv iteration: 1.21216 s\n",
            "\n",
            "97-iter f: 0.352 |g|: 2.22341 alpha: 1.000e+00 ratio: 0.904 lambda: 0.00000 #CG: 10 actred: -0.00293 prered: -0.00324 time: 26.198\n",
            "\n",
            " 97-iter val_acc: 79.471% val_loss 0.323\n",
            "\n",
            "Best model saved in ./saved_model/model.ckpt\n",
            "\n",
            "Avg time per Gv iteration: 1.21174 s\n",
            "\n",
            "98-iter f: 0.351 |g|: 0.05242 alpha: 1.000e+00 ratio: 0.999 lambda: 0.00000 #CG: 2 actred: -0.00025 prered: -0.00025 time: 16.550\n",
            "\n",
            " 98-iter val_acc: 79.556% val_loss 0.324\n",
            "\n",
            "Best model saved in ./saved_model/model.ckpt\n",
            "\n",
            "Avg time per Gv iteration: 1.21270 s\n",
            "\n",
            "99-iter f: 0.349 |g|: 7.44626 alpha: 2.500e-01 ratio: 0.444 lambda: 0.00000 #CG: 195 actred: -0.00209 prered: -0.00470 time: 255.097\n",
            "\n",
            " 99-iter val_acc: 79.338% val_loss 0.324\n",
            "\n",
            "Final acc: 79.338% | best acc 79.556% | total_#CG 1623 | total running time 3588.157s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UwH_nXJZDz2T"
      },
      "source": [
        "## Predict ##"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9zyzmNHFnOwD"
      },
      "source": [
        "# Arguments for prediction PSSP dataset\n",
        "pred_args = (\"--bsize 1024 --valid_set \" + VALID_FILE + \" --train_set \" + TRAIN_FILE + \n",
        "\t\t\t\t\t\t \" --model ./saved_model/model.ckpt --dim \" +\n",
        "             str(HEIGHT) + \" \" + str(WIDTH) + \" \" + str(DEPTH)).split()"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s6MctxH5_nTR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "915d5b5d-b814-406b-8463-a7f9cf2d9f26"
      },
      "source": [
        "valid_f =  \"/content/drive/MyDrive/Datasets/{0}_test_fold{1}.txt\".format(dataset.lower(),str(fold)) # train set  \n",
        "train_f = \"/content/drive/MyDrive/Datasets/{0}_train_fold{1}.txt\".format(dataset.lower(),str(fold)) # validation set\n",
        "test_f = \"/content/drive/MyDrive/Datasets/CASP13_3class.txt\" # test set CASP13\n",
        "print(valid_f)\n",
        "print(train_f)\n",
        "print(test_f)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/Datasets/cb513_test_fold2.txt\n",
            "/content/drive/MyDrive/Datasets/cb513_train_fold2.txt\n",
            "/content/drive/MyDrive/Datasets/CASP13_3class.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IeQqT96h_NHr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a28cab27-cd25-4fd3-ce35-5dc4f27ecbf0"
      },
      "source": [
        "VALID_PRED_FILE=\"pred_test_fold{0}.txt\".format(fold)\n",
        "TRAIN_PRED_FILE=\"pred_train_fold{0}.txt\".format(fold)\n",
        "TEST_PRED_FILE=\"pred_casp13_fold{0}.txt\".format(fold)\n",
        "print(VALID_PRED_FILE)\n",
        "print(TRAIN_PRED_FILE)\n",
        "print(TEST_PRED_FILE)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "pred_test_fold2.txt\n",
            "pred_train_fold2.txt\n",
            "pred_casp13_fold2.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NNNnI_mU6LN5"
      },
      "source": [
        "##Declare Predict Methods##"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kYGPhdqk5wYp"
      },
      "source": [
        "def create_output_pred(pred, origin_f, outFileName):\n",
        "    labels = ['C','H','E']\n",
        "    read_file = open(origin_f,\"r\")\n",
        "    output_file = open(outFileName,\"w\")\n",
        "    count =1 \n",
        "    target_name =1\n",
        "    target_secondary = 3\n",
        "    counter = 0\n",
        "    while True:\n",
        "        line = read_file.readline()\n",
        "        if not line:\n",
        "            break\n",
        "        if count == target_name:\n",
        "            output_file.write(line)\n",
        "            target_name+=3\n",
        "        if count == target_secondary:\n",
        "            output_file.write(line)\n",
        "            target_secondary+=3\n",
        "            line = line.replace(\"\\n\",\"\") \n",
        "            prediction = \"\"\n",
        "            for c in line:\n",
        "                if (c!='!'):\n",
        "                    prediction = prediction + labels[pred[counter]]\n",
        "                    counter +=1\n",
        "            output_file.write(prediction + \"\\n\")        \n",
        "        count+=1\n",
        "        "
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EA8Pq5a0C3m5"
      },
      "source": [
        "# import tensorflow as tf \n",
        "# tf.compat.v1.disable_eager_execution()\n",
        "# from utilities import predict, read_data, normalize_and_reshape\n",
        "# from net.net import CNN\n",
        "# import numpy as np \n",
        "# import argparse\n",
        "# import pdb\n",
        "\n",
        "def parse_args():\n",
        "        parser = argparse.ArgumentParser(description='prediction')\n",
        "        parser.add_argument('--test_set', dest='test_set',\n",
        "                            help='provide the directory of .mat file for testing',\n",
        "                            default=None, type=str)\n",
        "        parser.add_argument('--valid_set', dest='valid_set',\n",
        "                            help='provide the directory of .mat file for validation',\n",
        "                            default=None, type=str)\n",
        "        parser.add_argument('--train_set', dest='train_set',\n",
        "                            help='provide the directory of .mat file for training',\n",
        "                            default=None, type=str)\n",
        "        parser.add_argument('--model', dest='model_file',\n",
        "                            help='provide file storing network parameters, i.e. ./dir/model.ckpt',\n",
        "                            default='./saved_model/model.ckpt', type=str)\n",
        "        parser.add_argument('--bsize', dest='bsize',\n",
        "                            help='batch size',\n",
        "                            default=1024, type=int)\n",
        "        parser.add_argument('--loss', dest='loss', \n",
        "                            help='which loss function to use: MSELoss or CrossEntropy',\n",
        "                            default='MSELoss', type=str)\n",
        "        parser.add_argument('--dim', dest='dim', nargs='+', help='input dimension of data,'+\\\n",
        "                            'shape must be:  height width num_channels',\n",
        "                            default=[32, 32, 3], type=int)\n",
        "\n",
        "        args = parser.parse_args(args=pred_args)\n",
        "        return args\n",
        "\n",
        "def predict_model():\n",
        "        args = parse_args()\n",
        "\n",
        "        sess_config = tf.compat.v1.ConfigProto()\n",
        "        sess_config.gpu_options.allow_growth = True\n",
        "\n",
        "        with tf.compat.v1.Session(config=sess_config) as sess:\n",
        "                graph_address = args.model_file + '.meta'\n",
        "                imported_graph = tf.compat.v1.train.import_meta_graph(graph_address)\n",
        "                imported_graph.restore(sess, args.model_file)\n",
        "                mean_param = [v for v in tf.compat.v1.global_variables() if 'mean_tr:0' in v.name][0]\n",
        "                label_enum_var = [v for v in tf.compat.v1.global_variables() if 'label_enum:0' in v.name][0]\n",
        "\n",
        "                sess.run(tf.compat.v1.variables_initializer([mean_param, label_enum_var]))\n",
        "                mean_tr = sess.run(mean_param)\n",
        "                label_enum = sess.run(label_enum_var)\n",
        "\n",
        "                x = tf.compat.v1.get_default_graph().get_tensor_by_name('main_params/input_of_net:0')\n",
        "                y = tf.compat.v1.get_default_graph().get_tensor_by_name('main_params/labels:0')\n",
        "                outputs = tf.compat.v1.get_default_graph().get_tensor_by_name('output_of_net:0')\n",
        "\n",
        "                if args.loss == 'MSELoss':\n",
        "                        loss = tf.reduce_sum(input_tensor=tf.pow(outputs-y, 2))\n",
        "                else:\n",
        "                        loss = tf.reduce_sum(input_tensor=\n",
        "                            tf.nn.softmax_cross_entropy_with_logits(logits=outputs, labels=tf.stop_gradient(y)))\n",
        "                \n",
        "                network = (x, y, loss, outputs)\n",
        "\n",
        "                if args.valid_set is not None:\n",
        "                        valid_batch, num_cls, _ = read_data(args.valid_set, dim=args.dim, label_enum=label_enum)\n",
        "                        valid_batch[0], _ = normalize_and_reshape(valid_batch[0], dim=args.dim, mean_tr=mean_tr)\n",
        "            \n",
        "                        avg_loss_valid, avg_acc_valid, results_valid = predict(sess, network, valid_batch, args.bsize)\n",
        "\n",
        "                        # convert results back to the original labels\n",
        "                        inverse_map = dict(zip(np.arange(num_cls), label_enum))\n",
        "                        results_valid = np.expand_dims(results_valid, axis=1)\n",
        "                        results_valid = np.apply_along_axis(lambda x: inverse_map[x[0]], axis=1, arr=results_valid)\n",
        "                        create_output_pred(results_valid, valid_f, VALID_PRED_FILE)\n",
        "                        print('In valid phase, average loss: {:.3f} | average accuracy: {:.3f}%'.\\\n",
        "                            format(avg_loss_valid, avg_acc_valid*100))\n",
        "                \n",
        "                if args.train_set is not None:\n",
        "                        train_batch, num_cls, _ = read_data(args.train_set, dim=args.dim, label_enum=label_enum)\n",
        "                        train_batch[0], _ = normalize_and_reshape(train_batch[0], dim=args.dim, mean_tr=mean_tr)\n",
        "\n",
        "                        avg_loss_train, avg_acc_train, results_train = predict(sess, network, train_batch, args.bsize)\n",
        "                        # convert results back to the original labels\n",
        "                        inverse_map = dict(zip(np.arange(num_cls), label_enum))\n",
        "                        results_train = np.expand_dims(results_train, axis=1)\n",
        "                        results_train = np.apply_along_axis(lambda x: inverse_map[x[0]], axis=1, arr=results_train)\n",
        "                        # create_output_pred(results, results_train)\n",
        "\n",
        "                        create_output_pred(results_train, train_f, TRAIN_PRED_FILE)\n",
        "                        print('In train phase, average loss: {:.3f} | average accuracy: {:.3f}%'.\\\n",
        "                            format(avg_loss_train, avg_acc_train*100))\n",
        "\n",
        "                if args.test_set is not None:\n",
        "                        test_batch, num_cls, _ = read_data(args.test_set, dim=args.dim, label_enum=label_enum)\n",
        "                        test_batch[0], _ = normalize_and_reshape(test_batch[0], dim=args.dim, mean_tr=mean_tr)\n",
        "\n",
        "                        avg_loss_test, avg_acc_test, results_test = predict(sess, network, test_batch, args.bsize)\n",
        "                        # convert results back to the original labels\n",
        "                        inverse_map = dict(zip(np.arange(num_cls), label_enum))\n",
        "                        results_test = np.expand_dims(results_test, axis=1)\n",
        "                        results_test = np.apply_along_axis(lambda x: inverse_map[x[0]], axis=1, arr=results_test)\n",
        "                        # create_output_pred(results, results_train)\n",
        "\n",
        "                        create_output_pred(results_test, test_f, TEST_PRED_FILE)\n",
        "                        print('In test phase, average loss: {:.3f} | average accuracy: {:.3f}%'.\\\n",
        "                            format(avg_loss_test, avg_acc_test*100))\n",
        "            "
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aN8BTASh6eSL"
      },
      "source": [
        "##Run Predict and Display output##"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gCInY5uB6Y3G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5fc11d8-c192-4496-9d44-91060c870d55"
      },
      "source": [
        "predict_model()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from ./saved_model/model.ckpt\n",
            "Normalize images according to the provided mean.\n",
            "In valid phase, average loss: 0.324 | average accuracy: 79.556%\n",
            "Normalize images according to the provided mean.\n",
            "In train phase, average loss: 0.309 | average accuracy: 80.385%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "loR25EiOE4cQ"
      },
      "source": [
        "# !head \"$VALID_PRED_FILE\""
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NOnpS-N3DwhK"
      },
      "source": [
        "# !head \"$TRAIN_PRED_FILE\""
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exgdCTXb68Vl"
      },
      "source": [
        "## Check Test score on CASP13 ##"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lda3hrM4dmWk"
      },
      "source": [
        "# Arguments for prediction PSSP dataset\n",
        "pred_args = (\"--bsize 1024 --test_set \" + TEST_FILE + \n",
        "\t\t\t\t\t\t \" --model ./saved_model/model.ckpt --dim \" +\n",
        "             str(HEIGHT) + \" \" + str(WIDTH) + \" \" + str(DEPTH)).split()"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3JCC5-2mk0rs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "edc75183-3e0f-4a78-f807-515c48db4355"
      },
      "source": [
        "predict_model()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from ./saved_model/model.ckpt\n",
            "Normalize images according to the provided mean.\n",
            "In test phase, average loss: 0.333 | average accuracy: 78.012%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PMY6ihMjG1iO"
      },
      "source": [
        "# !head \"$TEST_PRED_FILE\""
      ],
      "execution_count": 25,
      "outputs": []
    }
  ]
}