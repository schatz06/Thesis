{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Notebooks_CNN_HFO_v2.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "PYXlwVnXKhOJ",
        "CVtiFO19ymk_",
        "fzh5PNccJ28-",
        "gcdrh-r9JRdS",
        "2pQEA9LADkBP",
        "J5uoUeQBDq5Q",
        "vMCxn-8P5tsH",
        "UwH_nXJZDz2T",
        "NNNnI_mU6LN5"
      ],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/schatz06/Thesis/blob/main/Notebooks_CNN_HFO_v3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aa_PJwlt7zI2"
      },
      "source": [
        "##Mount drive## "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9DjpeG7t71KC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f2d4330-a26c-4784-a9bc-d0d8f074c5d6"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IMo4HooFl0dv"
      },
      "source": [
        "##General Variables to use to load data##"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sJHXZcSlNqL9"
      },
      "source": [
        "fold = 0 # which fold to take \n",
        "embedding = \"protbert\"\n",
        "# embedding = \"seqvec\"\n",
        "# dataset=\"PISCES\" \n",
        "dataset=\"CB513\"\n",
        "USE_HFO=True"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PYXlwVnXKhOJ"
      },
      "source": [
        "## Imports ##"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WCuZCtMgg5ns"
      },
      "source": [
        "# Reload all modules (except those excluded by %aimport) every time before executing the Python code typed.\n",
        "%load_ext autoreload \n",
        "%autoreload 2\n",
        "# matplotlib graphs will be included in your notebook, next to the code. \n",
        "%matplotlib inline"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HKDaAU3F5Bgs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d986f79c-4f95-4c57-8b1f-876c86ff8252"
      },
      "source": [
        "# install hdf5storage package \r\n",
        "!pip install hdf5storage"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting hdf5storage\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/79/e0/5dd25068a231cd817265529368aca2f918049b290dcb2fd9b24ce136adf4/hdf5storage-0.1.15-py2.py3-none-any.whl (56kB)\n",
            "\r\u001b[K     |█████▊                          | 10kB 15.3MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 20kB 9.1MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 30kB 7.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 40kB 6.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 51kB 4.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 61kB 3.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: h5py>=2.1; python_version >= \"3.3\" in /usr/local/lib/python3.6/dist-packages (from hdf5storage) (2.10.0)\n",
            "Requirement already satisfied: numpy; python_version >= \"3.4\" in /usr/local/lib/python3.6/dist-packages (from hdf5storage) (1.19.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py>=2.1; python_version >= \"3.3\"->hdf5storage) (1.15.0)\n",
            "Installing collected packages: hdf5storage\n",
            "Successfully installed hdf5storage-0.1.15\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cjrANRzOFauh",
        "outputId": "0dcebfc9-7e3c-44f3-a6d4-7d5e30e4a9dd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import pdb # python debugger\n",
        "import numpy as np # import numpy \n",
        "import tensorflow as tf # import tensorflow  \n",
        "tf.compat.v1.disable_eager_execution() # disable eager execution\n",
        "import time # import time\n",
        "import math # import math\n",
        "import argparse # import argparse\n",
        "import os # import os\n",
        "import scipy.io as sio # import scipy.io \n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior() # makes different behaviors betweem tf_v1 & tf_v2 behave the same \n",
        "from tensorflow.python.client import device_lib # package to find available gpus\n",
        "import pandas as pd # import padas\n",
        "import hdf5storage # import hdf5storage"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CVtiFO19ymk_"
      },
      "source": [
        "## Get data ##"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yfOssHbYEh-W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e4bdd4e-f801-4b9a-9e63-11e726077ef6"
      },
      "source": [
        "VALID_FILE = \"/content/drive/MyDrive/Datasets/{0}_{1}_testSet{2}.mat\".format(dataset.lower(),embedding,str(fold)) # validation set\n",
        "TRAIN_FILE = \"/content/drive/MyDrive/Datasets/{0}_{1}_trainSet{2}.mat\".format(dataset.lower(),embedding,str(fold)) # train set  \n",
        "TEST_FILE = \"/content/drive/MyDrive/Datasets/casp13_{0}.mat\".format(embedding) # test set CASP13\n",
        "print(VALID_FILE)\n",
        "print(TRAIN_FILE)\n",
        "print(TEST_FILE)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/Datasets/cb513_protbert_testSet0.mat\n",
            "/content/drive/MyDrive/Datasets/cb513_protbert_trainSet0.mat\n",
            "/content/drive/MyDrive/Datasets/casp13_protbert.mat\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QKVIpZww1-dw"
      },
      "source": [
        "HEIGHT = 1538\n",
        "WIDTH = 1024\n",
        "DEPTH = 1\n",
        "CATEGORIES = 3 # number of different classification categories"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fzh5PNccJ28-"
      },
      "source": [
        "## VGG ##"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NwaNt8clJ5Vu"
      },
      "source": [
        "\"\"\"\n",
        "Codes are modifeid from PyTorch and Tensorflow Versions of VGG: \n",
        "https://github.com/pytorch/vision/blob/master/torchvision/models/vgg.py, and\n",
        "https://github.com/keras-team/keras-applications/blob/master/keras_applications/vgg16.py\n",
        "\"\"\"\n",
        "\n",
        "# import tensorflow.compat.v1 as tf\n",
        "# tf.disable_v2_behavior()\n",
        "# import numpy as np \n",
        "# import pdb\n",
        "from tensorflow.keras.applications.vgg16 import VGG16 as vgg16 # import vgg16 convolutional network\n",
        "from tensorflow.keras.applications.vgg19 import VGG19 as vgg19 # import vgg19 convolutional network\n",
        "\n",
        "__all__ = ['VGG11', 'VGG13', 'VGG16','VGG19'] # array holds all vgg CNN's names\n",
        " \n",
        "def VGG(feature, num_cls): # define VGG \n",
        "\n",
        "\twith tf.variable_scope('fully_connected') as scope:\n",
        "\t\tdim =np.prod(feature.shape[1:]) # returns the product of the given array\n",
        "\t\tx = tf.reshape(feature, [-1, dim]) # reshape tensor\n",
        "\n",
        "\t\tx = tf.keras.layers.Dense(units=4096, activation='relu', name=scope.name)(x) # define layers \n",
        "\t\tx = tf.keras.layers.Dense(units=4096, activation='relu', name=scope.name)(x)\n",
        "\t\tx = tf.keras.layers.Dense(units=num_cls, name=scope.name)(x)\n",
        "\n",
        "\treturn x\n",
        "# make the layers of CNN \n",
        "def make_layers(x, cfg):\n",
        "\tfor v in cfg:\n",
        "\t\tif v == 'M':\n",
        "\t\t\tx = tf.keras.layers.MaxPool2D(pool_size=[2, 2], strides=2, padding='valid')(x)\n",
        "\t\telse:\n",
        "\t\t\tx = tf.keras.layers.Conv2D(\n",
        "\t\t\tfilters=v,\n",
        "\t\t\tkernel_size=[3, 3],\n",
        "\t\t\tpadding='SAME',\n",
        "\t\t\tactivation=tf.nn.relu\n",
        "\t\t\t)(x)\n",
        "\treturn x\n",
        "\n",
        "cfg = {\n",
        "\t'A': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
        "\t'B': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
        "\t'D': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
        "\t'E': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', \n",
        "\t\t  512, 512, 512, 512, 'M'],\n",
        "}\n",
        "\n",
        "def VGG11(x_images, num_cls):\n",
        "\tfeature = make_layers(x_images, cfg['A'])\n",
        "\treturn VGG(feature, num_cls)\n",
        "\n",
        "def VGG13(x_images, num_cls):\n",
        "\tfeature = make_layers(x_images, cfg['B'])\n",
        "\treturn VGG(feature, num_cls)\n",
        "\n",
        "def VGG16(x_images, num_cls):\n",
        "\tfeature = make_layers(x_images, cfg['D'])\n",
        "\treturn VGG(feature, num_cls)\n",
        "\n",
        "def VGG19(x_images, num_cls):\n",
        "\tfeature = make_layers(x_images, cfg['E'])\n",
        "\treturn VGG(feature, num_cls)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gcdrh-r9JRdS"
      },
      "source": [
        "## Net ##"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kHA5zxK1JRtg"
      },
      "source": [
        "# import tensorflow.compat.v1 as tf\n",
        "# tf.disable_v2_behavior()\n",
        "# import math\n",
        "# import pdb\n",
        "# from tensorflow.python.client import device_lib\n",
        "# import numpy as np\n",
        "# from net.vgg import *\n",
        "\n",
        "def CNN_4layers(x_image, num_cls, reuse=False):\n",
        "\t_NUM_CLASSES = num_cls\n",
        "\twith tf.variable_scope('conv1', reuse=reuse) as scope:\n",
        "\t\tconv = tf.keras.layers.Conv2D(\n",
        "\t\t\tfilters=64,\n",
        "\t\t\tkernel_size=[3, 3],\n",
        "\t\t\tpadding='SAME',\n",
        "\t\t\tactivation=tf.nn.relu\n",
        "\t\t)(x_image)\n",
        "\n",
        "\twith tf.variable_scope('conv2', reuse=reuse) as scope:\n",
        "\t\tconv = tf.keras.layers.Conv2D(\n",
        "\t\t\tfilters=64,\n",
        "\t\t\tkernel_size=[3, 3],\n",
        "\t\t\tpadding='SAME',\n",
        "\t\t\tactivation=tf.nn.relu\n",
        "\t\t)(conv)\n",
        "\t\t\n",
        "\twith tf.variable_scope('conv3', reuse=reuse) as scope:\n",
        "\t\tconv = tf.keras.layers.Conv2D(\n",
        "\t\t\tfilters=64,\n",
        "\t\t\tkernel_size=[3, 3],\n",
        "\t\t\tpadding='SAME',\n",
        "\t\t\tactivation=tf.nn.relu\n",
        "\t\t)(conv)\n",
        "\n",
        "\twith tf.variable_scope('fully_connected', reuse=reuse) as scope:\n",
        "\t\tdim =np.prod(conv.shape[1:])\n",
        "\t\tflat = tf.reshape(conv, [-1, dim])\n",
        "\t\toutputs = tf.keras.layers.Dense(units=_NUM_CLASSES, name=scope.name)(flat)\n",
        "\n",
        "\treturn outputs\n",
        "\n",
        "\n",
        "\t# with tf.variable_scope('conv1', reuse=reuse) as scope:\n",
        "\t# \tconv = tf.keras.layers.Conv2D(\n",
        "\t# \t\tfilters=32,\n",
        "\t# \t\tkernel_size=[5, 5],\n",
        "\t# \t\tpadding='SAME',\n",
        "\t# \t\tactivation=tf.nn.relu\n",
        "\t# \t)(x_image)\n",
        "\t# \tpool = tf.keras.layers.MaxPool2D(pool_size=[2, 2], strides=2, padding='valid')(conv)\n",
        "\t# \t# N x 16 x 16 x 32\n",
        "\n",
        "\t# with tf.variable_scope('conv2', reuse=reuse) as scope:\n",
        "\t# \tconv = tf.keras.layers.Conv2D(\n",
        "\t# \t\tfilters=64,\n",
        "\t# \t\tkernel_size=[3, 3],\n",
        "\t# \t\tpadding='SAME',\n",
        "\t# \t\tactivation=tf.nn.relu\n",
        "\t# \t)(pool)\n",
        "\t# \tpool = tf.keras.layers.MaxPool2D(pool_size=[2, 2], strides=2, padding='valid')(conv)\n",
        "\t# \t# N x 8 x 8 x 64\n",
        "\t\t\n",
        "\t# with tf.variable_scope('conv3', reuse=reuse) as scope:\n",
        "\t# \tconv = tf.keras.layers.Conv2D(\n",
        "\t# \t\tfilters=64,\n",
        "\t# \t\tkernel_size=[3, 3],\n",
        "\t# \t\tpadding='SAME',\n",
        "\t# \t\tactivation=tf.nn.relu\n",
        "\t# \t)(pool)\n",
        "\t# \tpool = tf.keras.layers.MaxPool2D(pool_size=[2, 2], strides=2, padding='valid')(conv)\n",
        "\t# \t# N x 4 x 4 x 64\n",
        "\n",
        "\t# with tf.variable_scope('fully_connected', reuse=reuse) as scope:\n",
        "\t# \tdim =np.prod(pool.shape[1:])\n",
        "\t# \tflat = tf.reshape(pool, [-1, dim])\n",
        "\t# \toutputs = tf.keras.layers.Dense(units=_NUM_CLASSES, name=scope.name)(flat)\n",
        "\n",
        "\t# return outputs\n",
        "\n",
        "def CNN_7layers(x_image, num_cls, reuse=False):\n",
        "\t_NUM_CLASSES = num_cls\n",
        "\twith tf.variable_scope('conv1', reuse=reuse) as scope:\n",
        "\t\tconv = tf.keras.layers.Conv2D(\n",
        "\t\t\tfilters=64,\n",
        "\t\t\tkernel_size=[3, 3],\n",
        "\t\t\tpadding='SAME',\n",
        "\t\t\tactivation=tf.nn.relu\n",
        "\t\t)(x_image)\n",
        "\t\tconv = tf.keras.layers.Conv2D(\n",
        "\t\t\tfilters=64,\n",
        "\t\t\tkernel_size=[3, 3],\n",
        "\t\t\tpadding='SAME',\n",
        "\t\t\tactivation=tf.nn.relu\n",
        "\t\t)(conv)\n",
        "\n",
        "\twith tf.variable_scope('conv2', reuse=reuse) as scope:\n",
        "\t\tconv = tf.keras.layers.Conv2D(\n",
        "\t\t\tfilters=64,\n",
        "\t\t\tkernel_size=[3, 3],\n",
        "\t\t\tpadding='SAME',\n",
        "\t\t\tactivation=tf.nn.relu\n",
        "\t\t)(conv)\n",
        "\t\tconv = tf.keras.layers.Conv2D(\n",
        "\t\t\tfilters=64,\n",
        "\t\t\tkernel_size=[3, 3],\n",
        "\t\t\tpadding='SAME',\n",
        "\t\t\tactivation=tf.nn.relu\n",
        "\t\t)(conv)\n",
        "\n",
        "\twith tf.variable_scope('conv3', reuse=reuse) as scope:\n",
        "\t\tconv = tf.keras.layers.Conv2D(\n",
        "\t\t\tfilters=64,\n",
        "\t\t\tkernel_size=[3, 3],\n",
        "\t\t\tpadding='SAME',\n",
        "\t\t\tactivation=tf.nn.relu\n",
        "\t\t)(conv)\n",
        "\t\tconv = tf.keras.layers.Conv2D(\n",
        "\t\t\tfilters=64,\n",
        "\t\t\tkernel_size=[3, 3],\n",
        "\t\t\tpadding='SAME',\n",
        "\t\t\tactivation=tf.nn.relu\n",
        "\t\t)(conv)\n",
        "\n",
        "\twith tf.variable_scope('fully_connected', reuse=reuse) as scope:\n",
        "\t\tdim = np.prod(conv.shape[1:])\n",
        "\t\tflat = tf.reshape(conv, [-1, dim])\n",
        "\t\toutputs = tf.keras.layers.Dense(units=_NUM_CLASSES, name=scope.name)(flat)\n",
        "\n",
        "\treturn outputs\n",
        "\n",
        "\t# with tf.variable_scope('conv1', reuse=reuse) as scope:\n",
        "\t# \tconv = tf.keras.layers.Conv2D(\n",
        "\t# \t\tfilters=32,\n",
        "\t# \t\tkernel_size=[5, 5],\n",
        "\t# \t\tpadding='SAME',\n",
        "\t# \t\tactivation=tf.nn.relu\n",
        "\t# \t)(x_image)\n",
        "\t# \tconv = tf.keras.layers.Conv2D(\n",
        "\t# \t\tfilters=32,\n",
        "\t# \t\tkernel_size=[3, 3],\n",
        "\t# \t\tpadding='SAME',\n",
        "\t# \t\tactivation=tf.nn.relu\n",
        "\t# \t)(conv)\n",
        "\t# \tpool = tf.keras.layers.MaxPool2D(pool_size=[2, 2], strides=2, padding='valid')(conv)\n",
        "\t# \t# N x 16 x 16 x 32\n",
        "\n",
        "\t# with tf.variable_scope('conv2', reuse=reuse) as scope:\n",
        "\t# \tconv = tf.keras.layers.Conv2D(\n",
        "\t# \t\tfilters=64,\n",
        "\t# \t\tkernel_size=[3, 3],\n",
        "\t# \t\tpadding='SAME',\n",
        "\t# \t\tactivation=tf.nn.relu\n",
        "\t# \t)(pool)\n",
        "\t# \tconv = tf.keras.layers.Conv2D(\n",
        "\t# \t\tfilters=64,\n",
        "\t# \t\tkernel_size=[3, 3],\n",
        "\t# \t\tpadding='SAME',\n",
        "\t# \t\tactivation=tf.nn.relu\n",
        "\t# \t)(conv)\n",
        "\t# \tpool = tf.keras.layers.MaxPool2D(pool_size=[2, 2], strides=2, padding='valid')(conv)\n",
        "\t# \t# N x 8 x 8 x 64\n",
        "\n",
        "\t# with tf.variable_scope('conv3', reuse=reuse) as scope:\n",
        "\t# \tconv = tf.keras.layers.Conv2D(\n",
        "\t# \t\tfilters=64,\n",
        "\t# \t\tkernel_size=[3, 3],\n",
        "\t# \t\tpadding='SAME',\n",
        "\t# \t\tactivation=tf.nn.relu\n",
        "\t# \t)(pool)\n",
        "\t# \tconv = tf.keras.layers.Conv2D(\n",
        "\t# \t\tfilters=128,\n",
        "\t# \t\tkernel_size=[3, 3],\n",
        "\t# \t\tpadding='SAME',\n",
        "\t# \t\tactivation=tf.nn.relu\n",
        "\t# \t)(conv)\n",
        "\t# \tpool = tf.keras.layers.MaxPool2D(pool_size=[2, 2], strides=2, padding='valid')(conv)\n",
        "\t# \t# pool = tf.layers.dropout(pool, rate=0.25, name=scope.name)\n",
        "\t# \t# N x 4 x 4 x 128\n",
        "\n",
        "\t# with tf.variable_scope('fully_connected', reuse=reuse) as scope:\n",
        "\t# \tdim = np.prod(pool.shape[1:])\n",
        "\t# \tflat = tf.reshape(pool, [-1, dim])\n",
        "\t# \toutputs = tf.keras.layers.Dense(units=_NUM_CLASSES, name=scope.name)(flat)\n",
        "\n",
        "\t# return outputs\n",
        "\n",
        "def CNN(net, num_cls, dim):\n",
        "\n",
        "\t_NUM_CLASSES = num_cls\n",
        "\t_IMAGE_HEIGHT, _IMAGE_WIDTH, _IMAGE_CHANNELS = dim\n",
        "\n",
        "\twith tf.name_scope('main_params'):\n",
        "\t\tx = tf.placeholder(tf.float32, shape=[None, _IMAGE_HEIGHT, _IMAGE_WIDTH, _IMAGE_CHANNELS], name='input_of_net')\n",
        "\t\ty = tf.placeholder(tf.float32, shape=[None, _NUM_CLASSES], name='labels')\n",
        "\n",
        "\t# call CNN structure according to string net\n",
        "\toutputs = globals()[net](x, _NUM_CLASSES)\n",
        "\toutputs = tf.identity(outputs, name='output_of_net')\n",
        "\n",
        "\treturn (x, y, outputs)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2pQEA9LADkBP"
      },
      "source": [
        "## Utilities ##"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JxQzW-93DIdN"
      },
      "source": [
        "# import numpy as np\n",
        "# import math\n",
        "# import scipy.io as sio\n",
        "# import os\n",
        "# import math\n",
        "# import pdb\n",
        "\n",
        "class ConfigClass(object):\n",
        "    def __init__(self, args, num_data, num_cls):\n",
        "        super(ConfigClass, self).__init__()\n",
        "        self.args = args\n",
        "        self.iter_max = args.iter_max\n",
        "        \n",
        "        # Different notations of regularization term:\n",
        "        # In SGD, weight decay:\n",
        "        #     weight_decay <- lr/(C*num_of_training_samples)\n",
        "        # In Newton method:\n",
        "        #     C <- C * num_of_training_samples\n",
        "    \n",
        "\n",
        "        self.seed = args.seed\n",
        "\n",
        "        if self.seed is None:\n",
        "            print('You choose not to specify a random seed.'+\\\n",
        "                'A different result is produced after each run.')\n",
        "        elif isinstance(self.seed, int) and self.seed >= 0:\n",
        "            print('You specify random seed {}.'.format(self.seed))\n",
        "        else:\n",
        "            raise ValueError('Only accept None type or nonnegative integers for'+\\\n",
        "                    ' random seed argument!')\n",
        "\n",
        "        self.train_set = args.train_set\n",
        "        self.val_set = args.val_set\n",
        "        self.num_cls = num_cls\n",
        "        self.dim = args.dim\n",
        "\n",
        "        self.num_data = num_data\n",
        "        self.GNsize = min(args.GNsize, self.num_data)\n",
        "        self.C = args.C * self.num_data\n",
        "        self.net = args.net\n",
        "\n",
        "        self.xi = 0.1\n",
        "        self.CGmax = args.CGmax\n",
        "        self._lambda = args._lambda\n",
        "        self.drop = args.drop\n",
        "        self.boost = args.boost\n",
        "        self.eta = args.eta\n",
        "        self.lr = args.lr\n",
        "        self.lr_decay = args.lr_decay\n",
        "\n",
        "        self.bsize = args.bsize\n",
        "        if args.momentum < 0:\n",
        "            raise ValueError('Momentum needs to be larger than 0!')\n",
        "        self.momentum = args.momentum\n",
        "\n",
        "        self.loss = args.loss\n",
        "        if self.loss not in ('MSELoss', 'CrossEntropy'):\n",
        "            raise ValueError('Unrecognized loss type!')\n",
        "        self.optim = args.optim\n",
        "        if self.optim not in ('SGD', 'NewtonCG', 'Adam'):\n",
        "            raise ValueError('Only support SGD, Adam & NewtonCG optimizer!')\n",
        "        \n",
        "        self.log_file = args.log_file\n",
        "        self.model_file = args.model_file\n",
        "        self.screen_log_only = args.screen_log_only\n",
        "\n",
        "        if self.screen_log_only:\n",
        "            print('You choose not to store running log. Only store model to {}'.format(self.log_file))\n",
        "        else:\n",
        "            print('Saving log to: {}'.format(self.log_file))\n",
        "            dir_name, _ = os.path.split(self.log_file)\n",
        "            if not os.path.isdir(dir_name):\n",
        "                os.makedirs(dir_name, exist_ok=True)\n",
        "\n",
        "        dir_name, _ = os.path.split(self.model_file)\n",
        "        if not os.path.isdir(dir_name):\n",
        "            os.makedirs(dir_name, exist_ok=True)\n",
        "        \n",
        "        self.elapsed_time = 0.0\n",
        "\n",
        "def read_data(filename, dim, label_enum=None):\n",
        "    \"\"\"\n",
        "    args:\n",
        "    filename: the path where .mat files are stored\n",
        "    label_enum (default None): the list that stores the original labels. \n",
        "    If label_enum is None, the function will generate a new list which stores the \n",
        "    original labels in a sequence, and map original labels to [0, 1, ... number_of_classes-1]. \n",
        "    If label_enum is a list, the function will use it to convert \n",
        "    original labels to [0, 1,..., number_of_classes-1].\n",
        "    \"\"\"\n",
        "    mat_contents = sio.loadmat(filename)\n",
        "    #mat_contents = hdf5storage.loadmat(filename)\n",
        "    images, labels = mat_contents['x'], mat_contents['y']\n",
        "    labels = labels.reshape(-1)\n",
        "    print(labels.shape)\n",
        "    print(images.shape)\n",
        "    images = images.reshape(images.shape[0], -1)\n",
        "    _IMAGE_HEIGHT, _IMAGE_WIDTH, _IMAGE_CHANNELS = dim\n",
        "    print(dim)\n",
        "    print(images.shape[0])\n",
        "    print(_IMAGE_CHANNELS,_IMAGE_HEIGHT,_IMAGE_WIDTH)\n",
        "    print(np.prod(images.shape[1:]))\n",
        "    #zero_to_append = np.zeros((images.shape[0],_IMAGE_CHANNELS*_IMAGE_HEIGHT*_IMAGE_WIDTH-np.prod(images.shape[1:])))\n",
        "    #print(zero_to_append.shape)\n",
        "    #images = np.append(images, zero_to_append, axis=1)\n",
        "    #print(images_shape,labels.shape)\n",
        "    # check data validity\n",
        "    if label_enum is None:\n",
        "        label_enum, labels = np.unique(labels, return_inverse=True)\n",
        "        num_cls = labels.max() + 1\n",
        "\n",
        "        if len(label_enum) != num_cls:\n",
        "            raise ValueError('The number of classes is not equal to the number of\\\n",
        "            labels in dataset. Please verify them.')\n",
        "    else:\n",
        "        num_cls = len(label_enum)\n",
        "        forward_map = dict(zip(label_enum, np.arange(num_cls)))\n",
        "        labels = np.expand_dims(labels, axis=1)\n",
        "        labels = np.apply_along_axis(lambda x:forward_map[x[0]], axis=1, arr=labels)\n",
        "      \n",
        "    # convert groundtruth to one-hot encoding\n",
        "    labels = np.eye(num_cls)[labels]\n",
        "    labels = labels.astype('float32')\n",
        "\n",
        "    return [images, labels], num_cls, label_enum\n",
        "\n",
        "def normalize_and_reshape(images, dim, mean_tr=None):\n",
        "    _IMAGE_HEIGHT, _IMAGE_WIDTH, _IMAGE_CHANNELS = dim\n",
        "    images_shape = [images.shape[0], _IMAGE_CHANNELS, _IMAGE_HEIGHT, _IMAGE_WIDTH]\n",
        "    print(images_shape)\n",
        "    # images normalization and zero centering\n",
        "    images = images.reshape(images_shape[0], -1)\n",
        "\n",
        "    images = images/255.0\n",
        "\n",
        "    if mean_tr is None:\n",
        "        print('No mean of data provided! Normalize images by their own mean.')\n",
        "        # if no mean_tr is provided, we calculate it according to the current data\n",
        "        mean_tr = images.mean(axis=0) \n",
        "    else:\n",
        "        print('Normalize images according to the provided mean.')\n",
        "        if np.prod(mean_tr.shape) != np.prod(dim):\n",
        "            raise ValueError('Dimension of provided mean does not agree with the data! Please verify them!')\n",
        "\n",
        "    images = images - mean_tr\n",
        "\n",
        "    images = images.reshape(images_shape)\n",
        "    # Tensorflow accepts data shape: B x H x W x C\n",
        "    images = np.transpose(images, (0, 2, 3, 1))\n",
        "    return images, mean_tr\n",
        "\n",
        "\n",
        "def predict(sess, network, test_batch, bsize):\n",
        "    x, y, loss, outputs = network\n",
        "\n",
        "    test_inputs, test_labels = test_batch\n",
        "    batch_size = bsize\n",
        "\n",
        "    num_data = test_labels.shape[0]\n",
        "    num_batches = math.ceil(num_data/batch_size)\n",
        "\n",
        "    results = np.zeros(shape=num_data, dtype=np.int)\n",
        "    infer_loss = 0.0\n",
        "\n",
        "    for i in range(num_batches):\n",
        "        batch_idx = np.arange(i*batch_size, min((i+1)*batch_size, num_data))\n",
        "\n",
        "        batch_input = test_inputs[batch_idx]\n",
        "        batch_labels = test_labels[batch_idx]\n",
        "\n",
        "        net_outputs, _loss = sess.run(\n",
        "            [outputs, loss], feed_dict={x: batch_input, y: batch_labels}\n",
        "            )\n",
        "        \n",
        "        results[batch_idx] = np.argmax(net_outputs, axis=1)\n",
        "        # note that _loss was summed over batches\n",
        "        infer_loss = infer_loss + _loss\n",
        "\n",
        "    avg_acc = (np.argmax(test_labels, axis=1) == results).mean()\n",
        "    avg_loss = infer_loss/num_data\n",
        "    \n",
        "    return avg_loss, avg_acc, results"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J5uoUeQBDq5Q"
      },
      "source": [
        "## Newton - CG ##"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IDug0aiKCqeG"
      },
      "source": [
        "# import pdb\n",
        "# import tensorflow as tf\n",
        "# import time\n",
        "# import numpy as np\n",
        "# import os\n",
        "# import math\n",
        "# from utilities import predict\n",
        "\n",
        "def Rop(f, weights, v):\n",
        "\t\"\"\"Implementation of R operator\n",
        "\tArgs:\n",
        "\t\tf: any function of weights\n",
        "\t\tweights: list of tensors.\n",
        "\t\tv: vector for right multiplication\n",
        "\tReturns:\n",
        "\t\tJv: Jaccobian vector product, length same as\n",
        "\t\t\tthe number of output of f\n",
        "\t\"\"\"\n",
        "\tif type(f) == list:\n",
        "\t\tu = [tf.zeros_like(ff) for ff in f]\n",
        "\telse:\n",
        "\t\tu = tf.zeros_like(f)  # dummy variable\n",
        "\tg = tf.gradients(ys=f, xs=weights, grad_ys=u)\n",
        "\treturn tf.gradients(ys=g, xs=u, grad_ys=v)\n",
        "\n",
        "def Gauss_Newton_vec(outputs, loss, weights, v):\n",
        "\t\"\"\"Implements Gauss-Newton vector product.\n",
        "\tArgs:\n",
        "\t\tloss: Loss function.\n",
        "\t\toutputs: outputs of the last layer (pre-softmax).\n",
        "\t\tweights: Weights, list of tensors.\n",
        "\t\tv: vector to be multiplied with Gauss Newton matrix\n",
        "\tReturns:\n",
        "\t\tJ'BJv: Guass-Newton vector product.\n",
        "\t\"\"\"\n",
        "\t# Validate the input\n",
        "\tif type(weights) == list:\n",
        "\t\tif len(v) != len(weights):\n",
        "\t\t\traise ValueError(\"weights and v must have the same length.\")\n",
        "\n",
        "\tgrads_outputs = tf.gradients(ys=loss, xs=outputs)\n",
        "\tBJv = Rop(grads_outputs, weights, v)\n",
        "\tJBJv = tf.gradients(ys=outputs, xs=weights, grad_ys=BJv)\n",
        "\treturn JBJv\n",
        "\t\n",
        "\n",
        "class newton_cg(object):\n",
        "\tdef __init__(self, config, sess, outputs, loss):\n",
        "\t\t\"\"\"\n",
        "\t\tinitialize operations and vairables that will be used in newton\n",
        "\t\targs:\n",
        "\t\t\tsess: tensorflow session\n",
        "\t\t\toutputs: output of the neural network (pre-softmax layer)\n",
        "\t\t\tloss: function to calculate loss\n",
        "\t\t\"\"\"\n",
        "\t\tsuper(newton_cg, self).__init__()\n",
        "\t\tself.sess = sess\n",
        "\t\tself.config = config\n",
        "\t\tself.outputs = outputs\n",
        "\t\tself.loss = loss\n",
        "\t\tself.param = tf.compat.v1.trainable_variables()\n",
        "\n",
        "\t\tself.CGiter = 0\n",
        "\t\tFLOAT = tf.float32\n",
        "\t\tmodel_weight = self.vectorize(self.param)\n",
        "\t\t\n",
        "\t\t# initial variable used in CG\n",
        "\t\tzeros = tf.zeros(model_weight.get_shape(), dtype=FLOAT)\n",
        "\t\tself.r = tf.Variable(zeros, dtype=FLOAT, trainable=False)\n",
        "\t\tself.v = tf.Variable(zeros, dtype=FLOAT, trainable=False)\n",
        "\t\tself.s = tf.Variable(zeros, dtype=FLOAT, trainable=False)\n",
        "\t\tself.g = tf.Variable(zeros, dtype=FLOAT, trainable=False)\n",
        "\t\t# initial Gv, f for method minibatch\n",
        "\t\tself.Gv = tf.Variable(zeros, dtype=FLOAT, trainable=False)\n",
        "\t\tself.f = tf.Variable(0., dtype=FLOAT, trainable=False)\n",
        "\n",
        "\t\t# rTr, cgtol and beta to be used in CG\n",
        "\t\tself.rTr = tf.Variable(0., dtype=FLOAT, trainable=False)\n",
        "\t\tself.cgtol = tf.Variable(0., dtype=FLOAT, trainable=False)\n",
        "\t\tself.beta = tf.Variable(0., dtype=FLOAT, trainable=False)\n",
        "\n",
        "\t\t# placeholder alpha, old_alpha and lambda\n",
        "\t\tself.alpha = tf.compat.v1.placeholder(FLOAT, shape=[])\n",
        "\t\tself.old_alpha = tf.compat.v1.placeholder(FLOAT, shape=[])\n",
        "\t\tself._lambda = tf.compat.v1.placeholder(FLOAT, shape=[])\n",
        "\n",
        "\t\tself.num_grad_segment = math.ceil(self.config.num_data/self.config.bsize)\n",
        "\t\tself.num_Gv_segment = math.ceil(self.config.GNsize/self.config.bsize)\n",
        "\n",
        "\t\tcal_loss, cal_lossgrad, cal_lossGv, \\\n",
        "\t\tadd_reg_avg_loss, add_reg_avg_grad, add_reg_avg_Gv, \\\n",
        "\t\tzero_loss, zero_grad, zero_Gv = self._ops_in_minibatch()\n",
        "\n",
        "\t\t# initial operations that will be used in minibatch and newton\n",
        "\t\tself.cal_loss = cal_loss\n",
        "\t\tself.cal_lossgrad = cal_lossgrad\n",
        "\t\tself.cal_lossGv = cal_lossGv\n",
        "\t\tself.add_reg_avg_loss = add_reg_avg_loss\n",
        "\t\tself.add_reg_avg_grad = add_reg_avg_grad\n",
        "\t\tself.add_reg_avg_Gv = add_reg_avg_Gv\n",
        "\t\tself.zero_loss = zero_loss\n",
        "\t\tself.zero_grad = zero_grad\n",
        "\t\tself.zero_Gv = zero_Gv\n",
        "\n",
        "\t\tself.CG, self.update_v = self._CG()\n",
        "\t\tself.init_cg_vars = self._init_cg_vars()\n",
        "\t\tself.update_gs = tf.tensordot(self.s, self.g, axes=1)\n",
        "\t\tself.update_sGs = 0.5*tf.tensordot(self.s, -self.g-self.r-self._lambda*self.s, axes=1)\n",
        "\t\tself.update_model = self._update_model()\n",
        "\t\tself.gnorm = self.calc_norm(self.g)\n",
        "\n",
        "\n",
        "\tdef vectorize(self, tensors):\n",
        "\t\tif isinstance(tensors, list) or isinstance(tensors, tuple):\n",
        "\t\t\tvector = [tf.reshape(tensor, [-1]) for tensor in tensors]\n",
        "\t\t\treturn tf.concat(vector, 0) \n",
        "\t\telse:\n",
        "\t\t\treturn tensors \n",
        "\t\n",
        "\tdef inverse_vectorize(self, vector, param):\n",
        "\t\tif isinstance(vector, list):\n",
        "\t\t\treturn vector\n",
        "\t\telse:\n",
        "\t\t\ttensors = []\n",
        "\t\t\toffset = 0\n",
        "\t\t\tnum_total_param = np.sum([np.prod(p.shape.as_list()) for p in param])\n",
        "\t\t\tfor p in param:\n",
        "\t\t\t\tnumel = np.prod(p.shape.as_list())\n",
        "\t\t\t\ttensors.append(tf.reshape(vector[offset: offset+numel], p.shape))\n",
        "\t\t\t\toffset += numel\n",
        "\n",
        "\t\t\tassert offset == num_total_param\n",
        "\t\t\treturn tensors\n",
        "\n",
        "\tdef calc_norm(self, v):\n",
        "\t\t# default: frobenius norm\n",
        "\t\tif isinstance(v, list):\n",
        "\t\t\tnorm = 0.\n",
        "\t\t\tfor p in v:\n",
        "\t\t\t\tnorm = norm + tf.norm(tensor=p)**2\n",
        "\t\t\treturn norm**0.5\n",
        "\t\telse:\n",
        "\t\t\treturn tf.norm(tensor=v)\n",
        "\n",
        "\tdef _ops_in_minibatch(self):\n",
        "\t\t\"\"\"\n",
        "\t\tDefine operations that will be used in method minibatch\n",
        "\t\tVectorization is already a deep copy operation.\n",
        "\t\tBefore using newton method, loss needs to be summed over training samples\n",
        "\t\tto make results consistent.\n",
        "\t\t\"\"\"\n",
        "\n",
        "\t\tdef cal_loss():\n",
        "\t\t\treturn tf.compat.v1.assign(self.f, self.f + self.loss)\n",
        "\n",
        "\t\tdef cal_lossgrad():\n",
        "\t\t\tupdate_f = tf.compat.v1.assign(self.f, self.f + self.loss)\n",
        "\n",
        "\t\t\tgrad = tf.gradients(ys=self.loss, xs=self.param)\n",
        "\t\t\tgrad = self.vectorize(grad)\n",
        "\t\t\tupdate_grad = tf.compat.v1.assign(self.g, self.g + grad)\n",
        "\n",
        "\t\t\treturn tf.group(*[update_f, update_grad])\n",
        "\n",
        "\t\tdef cal_lossGv():\n",
        "\t\t\tv = self.inverse_vectorize(self.v, self.param)\n",
        "\t\t\tGv = Gauss_Newton_vec(self.outputs, self.loss, self.param, v)\n",
        "\t\t\tGv = self.vectorize(Gv)\n",
        "\t\t\treturn tf.compat.v1.assign(self.Gv, self.Gv + Gv) \n",
        "\n",
        "\t\t# add regularization term to loss, gradient and Gv and further average over batches \n",
        "\t\tdef add_reg_avg_loss():\n",
        "\t\t\tmodel_weight = self.vectorize(self.param)\n",
        "\t\t\treg = (self.calc_norm(model_weight))**2\n",
        "\t\t\treg = 1.0/(2*self.config.C) * reg\n",
        "\t\t\treturn tf.compat.v1.assign(self.f, reg + self.f/self.config.num_data)\n",
        "\n",
        "\t\tdef add_reg_avg_lossgrad():\n",
        "\t\t\tmodel_weight = self.vectorize(self.param)\n",
        "\t\t\treg_grad = model_weight/self.config.C\n",
        "\t\t\treturn tf.compat.v1.assign(self.g, reg_grad + self.g/self.config.num_data)\n",
        "\n",
        "\t\tdef add_reg_avg_lossGv():\n",
        "\t\t\treturn tf.compat.v1.assign(self.Gv, (self._lambda + 1/self.config.C)*self.v\n",
        "\t\t\t + self.Gv/self.config.GNsize) \n",
        "\n",
        "\t\t# zero out loss, grad and Gv \n",
        "\t\tdef zero_loss():\n",
        "\t\t\treturn tf.compat.v1.assign(self.f, tf.zeros_like(self.f))\n",
        "\t\tdef zero_grad():\n",
        "\t\t\treturn tf.compat.v1.assign(self.g, tf.zeros_like(self.g))\n",
        "\t\tdef zero_Gv():\n",
        "\t\t\treturn tf.compat.v1.assign(self.Gv, tf.zeros_like(self.Gv))\n",
        "\n",
        "\t\treturn (cal_loss(), cal_lossgrad(), cal_lossGv(),\n",
        "\t\t\t\tadd_reg_avg_loss(), add_reg_avg_lossgrad(), add_reg_avg_lossGv(),\n",
        "\t\t\t\tzero_loss(), zero_grad(), zero_Gv())\n",
        "\n",
        "\tdef minibatch(self, data_batch, place_holder_x, place_holder_y, mode):\n",
        "\t\t\"\"\"\n",
        "\t\tA function to evaluate either function value, global gradient or sub-sampled Gv\n",
        "\t\t\"\"\"\n",
        "\t\tif mode not in ('funonly', 'fungrad', 'Gv'):\n",
        "\t\t\traise ValueError('Unknown mode other than funonly & fungrad & Gv!')\n",
        "\n",
        "\t\tinputs, labels = data_batch\n",
        "\t\tnum_data = labels.shape[0]\n",
        "\t\tnum_segment = math.ceil(num_data/self.config.bsize)\n",
        "\t\tx, y = place_holder_x, place_holder_y\n",
        "\n",
        "\t\t# before estimation starts, need to zero out f, grad and Gv according to the mode\n",
        "\n",
        "\t\tif mode == 'funonly':\n",
        "\t\t\tassert num_data == self.config.num_data\n",
        "\t\t\tassert num_segment == self.num_grad_segment\n",
        "\t\t\tself.sess.run(self.zero_loss)\n",
        "\t\telif mode == 'fungrad':\n",
        "\t\t\tassert num_data == self.config.num_data\n",
        "\t\t\tassert num_segment == self.num_grad_segment\n",
        "\t\t\tself.sess.run([self.zero_loss, self.zero_grad])\n",
        "\t\telse:\n",
        "\t\t\tassert num_data == self.config.GNsize\n",
        "\t\t\tassert num_segment == self.num_Gv_segment\n",
        "\t\t\tself.sess.run(self.zero_Gv)\n",
        "\n",
        "\t\tfor i in range(num_segment):\n",
        "\t\t\t\n",
        "\t\t\tload_time = time.time()\n",
        "\t\t\tidx = np.arange(i * self.config.bsize, min((i+1) * self.config.bsize, num_data))\n",
        "\t\t\tbatch_input = inputs[idx]\n",
        "\t\t\tbatch_labels = labels[idx]\n",
        "\t\t\tbatch_input = np.ascontiguousarray(batch_input)\n",
        "\t\t\tbatch_labels = np.ascontiguousarray(batch_labels)\n",
        "\t\t\tself.config.elapsed_time += time.time() - load_time\n",
        "\n",
        "\t\t\tif mode == 'funonly':\n",
        "\n",
        "\t\t\t\tself.sess.run(self.cal_loss, feed_dict={\n",
        "\t\t\t\t\t\t\tx: batch_input, \n",
        "\t\t\t\t\t\t\ty: batch_labels,})\n",
        "\n",
        "\t\t\telif mode == 'fungrad':\n",
        "\t\t\t\t\n",
        "\t\t\t\tself.sess.run(self.cal_lossgrad, feed_dict={\n",
        "\t\t\t\t\t\t\tx: batch_input, \n",
        "\t\t\t\t\t\t\ty: batch_labels,})\n",
        "\t\t\t\t\n",
        "\t\t\telse:\n",
        "\t\t\t\t\n",
        "\t\t\t\tself.sess.run(self.cal_lossGv, feed_dict={\n",
        "\t\t\t\t\t\t\tx: batch_input, \n",
        "\t\t\t\t\t\t\ty: batch_labels})\n",
        "\n",
        "\t\t# average over batches\n",
        "\t\tif mode == 'funonly':\n",
        "\t\t\tself.sess.run(self.add_reg_avg_loss)\n",
        "\t\telif mode == 'fungrad':\n",
        "\t\t\tself.sess.run([self.add_reg_avg_loss, self.add_reg_avg_grad])\n",
        "\t\telse:\n",
        "\t\t\tself.sess.run(self.add_reg_avg_Gv, \n",
        "\t\t\t\tfeed_dict={self._lambda: self.config._lambda})\n",
        "\n",
        "\n",
        "\tdef _update_model(self):\n",
        "\t\tupdate_model_ops = []\n",
        "\t\tx = self.inverse_vectorize(self.s, self.param)\n",
        "\t\tfor i, p in enumerate(self.param):\n",
        "\t\t\top = tf.compat.v1.assign(p, p + (self.alpha-self.old_alpha) * x[i])\n",
        "\t\t\tupdate_model_ops.append(op)\n",
        "\t\treturn tf.group(*update_model_ops)\n",
        "\n",
        "\tdef _init_cg_vars(self):\n",
        "\t\tinit_ops = []\n",
        "\n",
        "\t\tinit_r = tf.compat.v1.assign(self.r, -self.g)\n",
        "\t\tinit_v = tf.compat.v1.assign(self.v, -self.g)\n",
        "\t\tinit_s = tf.compat.v1.assign(self.s, tf.zeros_like(self.g))\n",
        "\t\tgnorm = self.calc_norm(self.g)\n",
        "\t\tinit_rTr = tf.compat.v1.assign(self.rTr, gnorm**2)\n",
        "\t\tinit_cgtol = tf.compat.v1.assign(self.cgtol, self.config.xi*gnorm)\n",
        "\n",
        "\t\tinit_ops = [init_r, init_v, init_s, init_rTr, init_cgtol]\n",
        "\n",
        "\t\treturn tf.group(*init_ops)\n",
        "\n",
        "\tdef _CG(self):\n",
        "\t\t\"\"\"\n",
        "\t\tCG:\n",
        "\t\t\tdefine operations that will be used in method newton\n",
        "\t\tSame as the previous loss calculation,\n",
        "\t\tGv has been summed over batches when samples were fed into Neural Network.\n",
        "\t\t\"\"\"\n",
        "\n",
        "\t\tdef CG_ops():\n",
        "\t\t\t\n",
        "\t\t\tvGv = tf.tensordot(self.v, self.Gv, axes=1)\n",
        "\n",
        "\t\t\talpha = self.rTr / vGv\n",
        "\t\t\twith tf.control_dependencies([alpha]):\n",
        "\t\t\t\tupdate_s = tf.compat.v1.assign(self.s, self.s + alpha * self.v, name='update_s_ops')\n",
        "\t\t\t\tupdate_r = tf.compat.v1.assign(self.r, self.r - alpha * self.Gv, name='update_r_ops')\n",
        "\n",
        "\t\t\t\twith tf.control_dependencies([update_s, update_r]):\n",
        "\t\t\t\t\trnewTrnew = self.calc_norm(update_r)**2\n",
        "\t\t\t\t\tupdate_beta = tf.compat.v1.assign(self.beta, rnewTrnew / self.rTr)\n",
        "\t\t\t\t\twith tf.control_dependencies([update_beta]):\n",
        "\t\t\t\t\t\tupdate_rTr = tf.compat.v1.assign(self.rTr, rnewTrnew, name='update_rTr_ops')\n",
        "\n",
        "\t\t\treturn tf.group(*[update_s, update_beta, update_rTr])\n",
        "\n",
        "\t\tdef update_v():\n",
        "\t\t\treturn tf.compat.v1.assign(self.v, self.r + self.beta*self.v, name='update_v')\n",
        "\n",
        "\t\treturn (CG_ops(), update_v())\n",
        "\n",
        "\n",
        "\tdef newton(self, full_batch, val_batch, saver, network, test_network=None):\n",
        "\t\t\"\"\"\n",
        "\t\tConduct newton steps for training\n",
        "\t\targs:\n",
        "\t\t\tfull_batch & val_batch: provide training set and validation set. The function will\n",
        "\t\t\t\tsave the best model evaluted on validation set for future prediction.\n",
        "\t\t\tnetwork: a tuple contains (x, y, loss, outputs).\n",
        "\t\t\ttest_network: a tuple similar to argument network. If you use layers which behave differently\n",
        "\t\t\t\tin test phase such as batchnorm, a separate test_network is needed.\n",
        "\t\treturn:\n",
        "\t\t\tNone\n",
        "\t\t\"\"\"\n",
        "\t\t# check whether data is valid\n",
        "\t\tfull_inputs, full_labels = full_batch\n",
        "\t\tassert full_inputs.shape[0] == full_labels.shape[0]\n",
        "\n",
        "\t\tif full_inputs.shape[0] != self.config.num_data:\n",
        "\t\t\traise ValueError('The number of full batch inputs does not agree with the config argument.\\\n",
        "\t\t\t\t\t\t\tThis is important because global loss is averaged over those inputs')\n",
        "\n",
        "\t\tx, y, _, outputs = network\n",
        "\n",
        "\t\ttf.compat.v1.summary.scalar('loss', self.f)\n",
        "\t\tmerged = tf.compat.v1.summary.merge_all()\n",
        "\t\ttrain_writer = tf.compat.v1.summary.FileWriter('./summary/train', self.sess.graph)\n",
        "\n",
        "\t\tprint(self.config.args)\n",
        "\t\tif not self.config.screen_log_only:\n",
        "\t\t\tlog_file = open(self.config.log_file, 'w')\n",
        "\t\t\tprint(self.config.args, file=log_file)\n",
        "\t\t\n",
        "\t\tself.minibatch(full_batch, x, y, mode='fungrad')\n",
        "\t\tf = self.sess.run(self.f)\n",
        "\t\toutput_str = 'initial f: {:.3f}'.format(f)\n",
        "\t\tprint(output_str)\n",
        "\t\tif not self.config.screen_log_only:\n",
        "\t\t\tprint(output_str, file=log_file)\n",
        "\t\t\n",
        "\t\tbest_acc = 0.0\n",
        "\n",
        "\t\ttotal_running_time = 0.0\n",
        "\t\tself.config.elapsed_time = 0.0\n",
        "\t\ttotal_CG = 0\n",
        "\t\t\n",
        "\t\tfor k in range(self.config.iter_max):\n",
        "\n",
        "\t\t\t# randomly select the batch for Gv estimation\n",
        "\t\t\tidx = np.random.choice(np.arange(0, full_labels.shape[0]),\n",
        "\t\t\t\t\tsize=self.config.GNsize, replace=False)\n",
        "\n",
        "\t\t\tmini_inputs = full_inputs[idx]\n",
        "\t\t\tmini_labels = full_labels[idx]\n",
        "\n",
        "\t\t\tstart = time.time()\n",
        "\n",
        "\t\t\tself.sess.run(self.init_cg_vars)\n",
        "\t\t\tcgtol = self.sess.run(self.cgtol)\n",
        "\n",
        "\t\t\tavg_cg_time = 0.0\n",
        "\t\t\tfor CGiter in range(1, self.config.CGmax+1):\n",
        "\t\t\t\t\n",
        "\t\t\t\tcg_time = time.time()\n",
        "\t\t\t\tself.minibatch((mini_inputs, mini_labels), x, y, mode='Gv')\n",
        "\t\t\t\tavg_cg_time += time.time() - cg_time\n",
        "\t\t\t\t\n",
        "\t\t\t\tself.sess.run(self.CG)\n",
        "\n",
        "\t\t\t\trnewTrnew = self.sess.run(self.rTr)\n",
        "\t\t\t\t\n",
        "\t\t\t\tif rnewTrnew**0.5 <= cgtol or CGiter == self.config.CGmax:\n",
        "\t\t\t\t\tbreak\n",
        "\n",
        "\t\t\t\tself.sess.run(self.update_v)\n",
        "\n",
        "\t\t\tprint('Avg time per Gv iteration: {:.5f} s\\r\\n'.format(avg_cg_time/CGiter))\n",
        "\n",
        "\t\t\tgs, sGs = self.sess.run([self.update_gs, self.update_sGs], feed_dict={\n",
        "\t\t\t\t\tself._lambda: self.config._lambda\n",
        "\t\t\t\t})\n",
        "\t\t\t\n",
        "\t\t\t# line_search\n",
        "\t\t\tf_old = f\n",
        "\t\t\talpha = 1\n",
        "\t\t\twhile True:\n",
        "\n",
        "\t\t\t\told_alpha = 0 if alpha == 1 else alpha/0.5\n",
        "\t\t\t\t\n",
        "\t\t\t\tself.sess.run(self.update_model, feed_dict={\n",
        "\t\t\t\t\tself.alpha:alpha, self.old_alpha:old_alpha\n",
        "\t\t\t\t\t})\n",
        "\n",
        "\t\t\t\tprered = alpha*gs + (alpha**2)*sGs\n",
        "\n",
        "\t\t\t\tself.minibatch(full_batch, x, y, mode='funonly')\n",
        "\t\t\t\tf = self.sess.run(self.f)\n",
        "\n",
        "\t\t\t\tactred = f - f_old\n",
        "\n",
        "\t\t\t\tif actred <= self.config.eta*alpha*gs:\n",
        "\t\t\t\t\tbreak\n",
        "\n",
        "\t\t\t\talpha *= 0.5\n",
        "\n",
        "\t\t\t# update lambda\n",
        "\t\t\tratio = actred / prered\n",
        "\t\t\tif ratio < 0.25:\n",
        "\t\t\t\tself.config._lambda *= self.config.boost\n",
        "\t\t\telif ratio >= 0.75:\n",
        "\t\t\t\tself.config._lambda *= self.config.drop\n",
        "\n",
        "\t\t\tself.minibatch(full_batch, x, y, mode='fungrad')\n",
        "\t\t\tf = self.sess.run(self.f)\n",
        "\n",
        "\t\t\tgnorm = self.sess.run(self.gnorm)\n",
        "\n",
        "\t\t\tsummary = self.sess.run(merged)\n",
        "\t\t\ttrain_writer.add_summary(summary, k)\n",
        "\n",
        "\t\t\t# exclude data loading time for fair comparison\n",
        "\t\t\tend = time.time() \n",
        "\t\t\t\n",
        "\t\t\tend = end - self.config.elapsed_time\n",
        "\t\t\ttotal_running_time += end-start\n",
        "\n",
        "\t\t\tself.config.elapsed_time = 0.0\n",
        "\t\t\t\n",
        "\t\t\ttotal_CG += CGiter\n",
        "\n",
        "\t\t\toutput_str = '{}-iter f: {:.3f} |g|: {:.5f} alpha: {:.3e} ratio: {:.3f} lambda: {:.5f} #CG: {} actred: {:.5f} prered: {:.5f} time: {:.3f}'.\\\n",
        "\t\t\t\t\t\t\tformat(k, f, gnorm, alpha, actred/prered, self.config._lambda, CGiter, actred, prered, end-start)\n",
        "\t\t\tprint(output_str)\n",
        "\t\t\tif not self.config.screen_log_only:\n",
        "\t\t\t\tprint(output_str, file=log_file)\n",
        "\n",
        "\t\t\tif val_batch is not None:\n",
        "\t\t\t\t# Evaluate the performance after every Newton Step\n",
        "\t\t\t\tif test_network == None:\n",
        "\t\t\t\t\tval_loss, val_acc, _ = predict(\n",
        "\t\t\t\t\t\tself.sess, \n",
        "\t\t\t\t\t\tnetwork=(x, y, self.loss, outputs),\n",
        "\t\t\t\t\t\ttest_batch=val_batch,\n",
        "\t\t\t\t\t\tbsize=self.config.bsize,\n",
        "\t\t\t\t\t\t)\n",
        "\t\t\t\telse:\n",
        "\t\t\t\t\t# A separat test network part has not been done...\n",
        "\t\t\t\t\tval_loss, val_acc, _ = predict(\n",
        "\t\t\t\t\t\tself.sess, \n",
        "\t\t\t\t\t\tnetwork=test_network,\n",
        "\t\t\t\t\t\ttest_batch=val_batch,\n",
        "\t\t\t\t\t\tbsize=self.config.bsize\n",
        "\t\t\t\t\t\t)\n",
        "\n",
        "\t\t\t\toutput_str = '\\r\\n {}-iter val_acc: {:.3f}% val_loss {:.3f}\\r\\n'.\\\n",
        "\t\t\t\t\tformat(k, val_acc*100, val_loss)\n",
        "\t\t\t\tprint(output_str)\n",
        "\t\t\t\tif not self.config.screen_log_only:\n",
        "\t\t\t\t\tprint(output_str, file=log_file)\n",
        "\n",
        "\t\t\t\tif val_acc > best_acc:\n",
        "\t\t\t\t\tbest_acc = val_acc\n",
        "\t\t\t\t\tcheckpoint_path = self.config.model_file\n",
        "\t\t\t\t\tsave_path = saver.save(self.sess, checkpoint_path)\n",
        "\t\t\t\t\tprint('Best model saved in {}\\r\\n'.format(save_path))\n",
        "\n",
        "\t\tif val_batch is None:\n",
        "\t\t\tcheckpoint_path = self.config.model_file\n",
        "\t\t\tsave_path = saver.save(self.sess, checkpoint_path)\n",
        "\t\t\tprint('Model at the last iteration saved in {}\\r\\n'.format(save_path))\n",
        "\t\t\toutput_str = 'total_#CG {} | total running time {:.3f}s'.format(total_CG, total_running_time)\n",
        "\t\telse:\n",
        "\t\t\toutput_str = 'Final acc: {:.3f}% | best acc {:.3f}% | total_#CG {} | total running time {:.3f}s'.\\\n",
        "\t\t\t\tformat(val_acc*100, best_acc*100, total_CG, total_running_time)\n",
        "\t\tprint(output_str)\n",
        "\t\tif not self.config.screen_log_only:\n",
        "\t\t\tprint(output_str, file=log_file)\n",
        "\t\t\tlog_file.close()"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xatdN2Zm7BOQ"
      },
      "source": [
        "##Set Train Arguments##"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v_DZPFd4m7Og"
      },
      "source": [
        "if USE_HFO:\n",
        "    # Arguments for HFO - PSSP dataset\n",
        "    train_args = (\"--optim NewtonCG --GNsize 512 --C 0.01 --net CNN_4layers --bsize 8 --iter_max 100 \" +\n",
        "              \"--train_set \" + TRAIN_FILE + \" --val_set \" + VALID_FILE + \" --dim \" + \n",
        "              str(HEIGHT) + \" \" + str(WIDTH) + \" \" + str(DEPTH)).split()\n",
        "else:\n",
        "    # Arguments for SGD - PSSP dataset\n",
        "    train_args = (\"--optim SGD --lr 0.05 --momentum 0.01 --C 0.01 --net CNN_4layers --bsize 8 --epoch_max 1000 \" +\n",
        "              \"--train_set \" + TRAIN_FILE + \" --val_set \" + VALID_FILE + \" --dim \" +\n",
        "              str(HEIGHT) + \" \" + str(WIDTH) + \" \" + str(DEPTH)).split()"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vMCxn-8P5tsH"
      },
      "source": [
        "##Declare Train Function##"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cr528VD1EDj9"
      },
      "source": [
        "# import pdb\n",
        "# import numpy as np\n",
        "# import tensorflow as tf\n",
        "# tf.compat.v1.disable_eager_execution()\n",
        "# import time\n",
        "# import math\n",
        "# import argparse\n",
        "\n",
        "# from net.net import CNN\n",
        "# from newton_cg import newton_cg\n",
        "# from utilities import read_data, predict, ConfigClass, normalize_and_reshape\n",
        "\n",
        "def parse_args():\n",
        "\tparser = argparse.ArgumentParser(description='Newton method on DNN')\n",
        "\tparser.add_argument('--C', dest='C',\n",
        "\t\t\t\t\t  help='regularization term, or so-called weight decay where'+\\\n",
        "\t\t\t\t\t  \t\t'weight_decay = lr/(C*num_of_samples) in this implementation' ,\n",
        "\t\t\t\t\t  default=0.01, type=float)\n",
        "\n",
        "\t# Newton method arguments\n",
        "\tparser.add_argument('--GNsize', dest='GNsize',\n",
        "\t\t\t\t\t  help='number of samples for estimating Gauss-Newton matrix',\n",
        "\t\t\t\t\t  default=4096, type=int)\n",
        "\tparser.add_argument('--iter_max', dest='iter_max',\n",
        "\t\t\t\t\t  help='the maximal number of Newton iterations',\n",
        "\t\t\t\t\t  default=100, type=int)\n",
        "\tparser.add_argument('--xi', dest='xi',\n",
        "\t\t\t\t\t  help='the tolerance in the relative stopping condition for CG',\n",
        "\t\t\t\t\t  default=0.1, type=float)\n",
        "\tparser.add_argument('--drop', dest='drop',\n",
        "\t\t\t\t\t  help='the drop constants for the LM method',\n",
        "\t\t\t\t\t  default=2/3, type=float)\n",
        "\tparser.add_argument('--boost', dest='boost',\n",
        "\t\t\t\t\t  help='the boost constants for the LM method',\n",
        "\t\t\t\t\t  default=3/2, type=float)\n",
        "\tparser.add_argument('--eta', dest='eta',\n",
        "\t\t\t\t\t  help='the parameter for the line search stopping condition',\n",
        "\t\t\t\t\t  default=0.0001, type=float)\n",
        "\tparser.add_argument('--CGmax', dest='CGmax',\n",
        "\t\t\t\t\t  help='the maximal number of CG iterations',\n",
        "\t\t\t\t\t  default=250, type=int)\n",
        "\tparser.add_argument('--lambda', dest='_lambda',\n",
        "\t\t\t\t\t  help='the initial lambda for the LM method',\n",
        "\t\t\t\t\t  default=1, type=float)\n",
        "\n",
        "\t# SGD arguments\n",
        "\tparser.add_argument('--epoch_max', dest='epoch',\n",
        "\t\t\t\t\t  help='number of training epoch',\n",
        "\t\t\t\t\t  default=500, type=int)\n",
        "\tparser.add_argument('--lr', dest='lr',\n",
        "\t\t\t\t\t  help='learning rate',\n",
        "\t\t\t\t\t  default=0.01, type=float)\n",
        "\tparser.add_argument('--decay', dest='lr_decay',\n",
        "\t\t\t\t\t  help='learning rate decay over each mini-batch update',\n",
        "\t\t\t\t\t  default=0, type=float)\n",
        "\tparser.add_argument('--momentum', dest='momentum',\n",
        "\t\t\t\t\t  help='momentum of learning',\n",
        "\t\t\t\t\t  default=0, type=float)\n",
        "\n",
        "\t# Model training arguments\n",
        "\tparser.add_argument('--bsize', dest='bsize',\n",
        "\t\t\t\t\t  help='batch size to evaluate stochastic gradient, Gv, etc. Since the sampled data \\\n",
        "\t\t\t\t\t  for computing Gauss-Newton matrix and etc. might not fit into memeory \\\n",
        "\t\t\t\t\t  for one time, we will split the data into several segements and average\\\n",
        "\t\t\t\t\t  over them.',\n",
        "\t\t\t\t\t  default=1024, type=int)\n",
        "\tparser.add_argument('--net', dest='net',\n",
        "\t\t\t\t\t  help='classifier type',\n",
        "\t\t\t\t\t  default='CNN_4layers', type=str)\n",
        "\tparser.add_argument('--train_set', dest='train_set',\n",
        "\t\t\t\t\t  help='provide the directory of .mat file for training',\n",
        "\t\t\t\t\t  default=None, type=str)\n",
        "\tparser.add_argument('--val_set', dest='val_set',\n",
        "\t\t\t\t\t  help='provide the directory of .mat file for validation',\n",
        "\t\t\t\t\t  default=None, type=str)\n",
        "\tparser.add_argument('--model', dest='model_file',\n",
        "\t\t\t\t\t  help='model saving address',\n",
        "\t\t\t\t\t  default='./saved_model/model.ckpt', type=str)\n",
        "\tparser.add_argument('--log', dest='log_file',\n",
        "\t\t\t\t\t  help='log saving directory',\n",
        "\t\t\t\t\t  default='./running_log/logger.log', type=str)\n",
        "\tparser.add_argument('--screen_log_only', dest='screen_log_only',\n",
        "\t\t\t\t\t  help='screen printing running log instead of storing it',\n",
        "\t\t\t\t\t  action='store_true')\n",
        "\tparser.add_argument('--optim', '-optim', \n",
        "\t\t\t\t\t  help='which optimizer to use: SGD, Adam or NewtonCG',\n",
        "\t\t\t\t\t  default='NewtonCG', type=str)\n",
        "\tparser.add_argument('--loss', dest='loss', \n",
        "\t\t\t\t\t  help='which loss function to use: MSELoss or CrossEntropy',\n",
        "\t\t\t\t\t  default='MSELoss', type=str)\n",
        "\tparser.add_argument('--dim', dest='dim', nargs='+', help='input dimension of data,'+\\\n",
        "\t\t\t\t\t\t'shape must be:  height width num_channels',\n",
        "\t\t\t\t\t  default=[32, 32, 3], type=int)\n",
        "\tparser.add_argument('--seed', dest='seed', help='a nonnegative integer for \\\n",
        "\t\t\t\t\t\treproducibility', type=int)\t \n",
        "\t\n",
        "\targs = parser.parse_args(args=train_args)\n",
        "\treturn args\n",
        "\n",
        "\n",
        "args = parse_args()\n",
        "\n",
        "def init_model(param):\n",
        "\tinit_ops = []\n",
        "\tfor p in param:\n",
        "\t\tif 'kernel' in p.name:\n",
        "\t\t\tweight = np.random.standard_normal(p.shape)* np.sqrt(2.0 / ((np.prod(p.get_shape().as_list()[:-1]))))\n",
        "\t\t\topt = tf.compat.v1.assign(p, weight)\n",
        "\t\telif 'bias' in p.name:\n",
        "\t\t\tzeros = np.zeros(p.shape)\n",
        "\t\t\topt = tf.compat.v1.assign(p, zeros)\n",
        "\t\tinit_ops.append(opt)\n",
        "\treturn tf.group(*init_ops)\n",
        "\n",
        "def gradient_trainer(config, sess, network, full_batch, val_batch, saver, test_network):\n",
        "\tx, y, loss, outputs,  = network\n",
        "\t\n",
        "\tglobal_step = tf.Variable(initial_value=0, trainable=False, name='global_step')\n",
        "\tlearning_rate = tf.compat.v1.placeholder(tf.float32, shape=[], name='learning_rate')\n",
        "\n",
        "\t# Probably not a good way to add regularization.\n",
        "\t# Just to confirm the implementation is the same as MATLAB.\n",
        "\treg = 0.0\n",
        "\tparam = tf.compat.v1.trainable_variables()\n",
        "\tfor p in param:\n",
        "\t\treg = reg + tf.reduce_sum(input_tensor=tf.pow(p,2))\n",
        "\treg_const = 1/(2*config.C)\n",
        "\tbatch_size = tf.compat.v1.cast(tf.shape(x)[0], tf.float32)\n",
        "\tloss_with_reg = reg_const*reg + loss/batch_size\n",
        "\n",
        "\tif config.optim == 'SGD':\n",
        "\t\toptimizer = tf.compat.v1.train.MomentumOptimizer(\n",
        "\t\t\t\t\tlearning_rate=learning_rate, \n",
        "\t\t\t\t\tmomentum=config.momentum).minimize(\n",
        "\t\t\t\t\tloss_with_reg, \n",
        "\t\t\t\t\tglobal_step=global_step)\n",
        "\telif config.optim == 'Adam':\n",
        "\t\toptimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate,\n",
        "\t\t\t\t\t\t\t\tbeta1=0.9,\n",
        "\t\t\t\t\t\t\t\tbeta2=0.999,\n",
        "\t\t\t\t\t\t\t\tepsilon=1e-08).minimize(\n",
        "\t\t\t\t\t\t\t\tloss_with_reg, \n",
        "\t\t\t\t\t\t\t\tglobal_step=global_step)\n",
        "\n",
        "\ttrain_inputs, train_labels = full_batch\n",
        "\tnum_data = train_labels.shape[0]\n",
        "\tnum_iters = math.ceil(num_data/config.bsize)\n",
        "\n",
        "\tprint(config.args)\n",
        "\tif not config.screen_log_only:\n",
        "\t\tlog_file = open(config.log_file, 'w')\n",
        "\t\tprint(config.args, file=log_file)\n",
        "\tsess.run(tf.compat.v1.global_variables_initializer())\n",
        "\t\n",
        "\n",
        "\tprint('-------------- initializing network by methods in He et al. (2015) --------------')\n",
        "\tparam = tf.compat.v1.trainable_variables()\n",
        "\tsess.run(init_model(param))\n",
        "\n",
        "\ttotal_running_time = 0.0\n",
        "\tbest_acc = 0.0\n",
        "\tlr = config.lr\n",
        "\n",
        "\tfor epoch in range(0, args.epoch):\n",
        "\t\t\n",
        "\t\tloss_avg = 0.0\n",
        "\t\tstart = time.time()\n",
        "\n",
        "\t\tfor i in range(num_iters):\n",
        "\t\t\t\n",
        "\t\t\tload_time = time.time()\n",
        "\t\t\t# randomly select the batch\n",
        "\t\t\tidx = np.random.choice(np.arange(0, num_data), \n",
        "\t\t\t\t\tsize=config.bsize, replace=False)\n",
        "\n",
        "\t\t\tbatch_input = train_inputs[idx]\n",
        "\t\t\tbatch_labels = train_labels[idx]\n",
        "\t\t\tbatch_input = np.ascontiguousarray(batch_input)\n",
        "\t\t\tbatch_labels = np.ascontiguousarray(batch_labels)\n",
        "\t\t\tconfig.elapsed_time += time.time() - load_time\n",
        "\n",
        "\t\t\tstep, _, batch_loss= sess.run(\n",
        "\t\t\t\t[global_step, optimizer, loss_with_reg],\n",
        "\t\t\t\tfeed_dict = {x: batch_input, y: batch_labels, learning_rate: lr}\n",
        "\t\t\t\t)\n",
        "\n",
        "\t\t\t# print initial loss\n",
        "\t\t\tif epoch == 0 and i == 0:\n",
        "\t\t\t\toutput_str = 'initial f (reg + avg. loss of 1st batch): {:.3f}'.format(batch_loss)\n",
        "\t\t\t\tprint(output_str)\n",
        "\t\t\t\tif not config.screen_log_only:\n",
        "\t\t\t\t\tprint(output_str, file=log_file)\n",
        "\n",
        "\t\t\tloss_avg = loss_avg + batch_loss\n",
        "\t\t\t# print log every 10% of the iterations\n",
        "\t\t\tif i % math.ceil(num_iters/10) == 0:\n",
        "\t\t\t\tend = time.time()\n",
        "\t\t\t\toutput_str = 'Epoch {}: {}/{} | loss {:.4f} | lr {:.6} | elapsed time {:.3f}'\\\n",
        "\t\t\t\t\t.format(epoch, i, num_iters, batch_loss , lr, end-start)\n",
        "\t\t\t\tprint(output_str)\n",
        "\t\t\t\tif not config.screen_log_only:\n",
        "\t\t\t\t\tprint(output_str, file=log_file)\n",
        "\t\t\t\n",
        "\t\t\t# adjust learning rate for SGD by inverse time decay\n",
        "\t\t\tif args.optim != 'Adam':\n",
        "\t\t\t\tlr = config.lr/(1 + args.lr_decay*step)\n",
        "\n",
        "\t\t# exclude data loading time for fair comparison\n",
        "\t\tepoch_end = time.time() - config.elapsed_time\n",
        "\t\ttotal_running_time += epoch_end - start\n",
        "\t\tconfig.elapsed_time = 0.0\n",
        "\t\t\n",
        "\t\tif val_batch is None:\n",
        "\t\t\toutput_str = 'In epoch {} train loss: {:.3f} | epoch time {:.3f}'\\\n",
        "\t\t\t\t.format(epoch, loss_avg/(i+1), epoch_end-start)\t\t\t\n",
        "\t\telse:\n",
        "\t\t\tif test_network == None:\n",
        "\t\t\t\tval_loss, val_acc, _ = predict(\n",
        "\t\t\t\t\tsess, \n",
        "\t\t\t\t\tnetwork=(x, y, loss, outputs),\n",
        "\t\t\t\t\ttest_batch=val_batch,\n",
        "\t\t\t\t\tbsize=config.bsize\n",
        "\t\t\t\t\t)\n",
        "\t\t\telse:\n",
        "\t\t\t\t# A separat test network part have been done...\n",
        "\t\t\t\tval_loss, val_acc, _ = predict(\n",
        "\t\t\t\t\tsess, \n",
        "\t\t\t\t\tnetwork=test_network,\n",
        "\t\t\t\t\ttest_batch=val_batch,\n",
        "\t\t\t\t\tbsize=config.bsize\n",
        "\t\t\t\t\t)\n",
        "\t\t\t\n",
        "\t\t\toutput_str = 'In epoch {} train loss: {:.3f} | val loss: {:.3f} | val accuracy: {:.3f}% | epoch time {:.3f}'\\\n",
        "\t\t\t\t.format(epoch, loss_avg/(i+1), val_loss, val_acc*100, epoch_end-start)\n",
        "\t\t\n",
        "\t\t\tif val_acc > best_acc:\n",
        "\t\t\t\tbest_acc = val_acc\n",
        "\t\t\t\tcheckpoint_path = config.model_file \n",
        "\t\t\t\tsave_path = saver.save(sess, checkpoint_path)\n",
        "\t\t\t\tprint('Saved best model in {}'.format(save_path))\n",
        "\n",
        "\t\tprint(output_str)\n",
        "\t\tif not config.screen_log_only:\n",
        "\t\t\tprint(output_str, file=log_file)\n",
        "\n",
        "\tif val_batch is None:\n",
        "\t\tcheckpoint_path = config.model_file \n",
        "\t\tsave_path = saver.save(sess, checkpoint_path)\n",
        "\t\tprint('Model at the last iteration saved in {}\\r\\n'.format(save_path))\n",
        "\t\toutput_str = 'total running time {:.3f}s'.format(total_running_time)\n",
        "\telse:\n",
        "\t\toutput_str = 'Final acc: {:.3f}% | best acc {:.3f}% | total running time {:.3f}s'\\\n",
        "\t\t\t.format(val_acc*100, best_acc*100, total_running_time)\n",
        "\t\n",
        "\tprint(output_str)\n",
        "\tif not config.screen_log_only:\n",
        "\t\tprint(output_str, file=log_file)\n",
        "\t\tlog_file.close()\n",
        "\n",
        "def newton_trainer(config, sess, network, full_batch, val_batch, saver, test_network):\n",
        "\n",
        "\t_, _, loss, outputs = network\n",
        "\tnewton_solver = newton_cg(config, sess, outputs, loss)\n",
        "\tsess.run(tf.compat.v1.global_variables_initializer())\n",
        "\n",
        "\tprint('-------------- initializing network by methods in He et al. (2015) --------------')\n",
        "\tparam = tf.compat.v1.trainable_variables()\n",
        "\tsess.run(init_model(param))\n",
        "\tnewton_solver.newton(full_batch, val_batch, saver, network, test_network)\n",
        "\n",
        "\n",
        "def train_model():\n",
        "\tfull_batch, num_cls, label_enum = read_data(filename=args.train_set, dim=args.dim)\n",
        "\tif args.val_set is None:\n",
        "\t\tprint('No validation set is provided. Will output model at the last iteration.')\n",
        "\t\tval_batch = None\n",
        "\telse:\n",
        "    #print(\"Train model inside else\")\n",
        "\t\tval_batch, _, _ = read_data(filename=args.val_set, dim=args.dim, label_enum=label_enum)\n",
        "\n",
        "\tnum_data = full_batch[0].shape[0]\n",
        "\t\n",
        "\tconfig = ConfigClass(args, num_data, num_cls)\n",
        "\n",
        "\tif isinstance(config.seed, int):\n",
        "\t\ttf.compat.v1.random.set_random_seed(config.seed)\n",
        "\t\tnp.random.seed(config.seed)\n",
        "\n",
        "\tif config.net in ('CNN_4layers', 'CNN_7layers', 'VGG11', 'VGG13', 'VGG16','VGG19'):\n",
        "\t\tx, y, outputs = CNN(config.net, num_cls, config.dim)\n",
        "\t\ttest_network = None\n",
        "\telse:\n",
        "\t\traise ValueError('Unrecognized training model')\n",
        "\n",
        "\tif config.loss == 'MSELoss':\n",
        "\t\tloss = tf.reduce_sum(input_tensor=tf.pow(outputs-y, 2))\n",
        "\telse:\n",
        "\t\tloss = tf.reduce_sum(input_tensor=tf.nn.softmax_cross_entropy_with_logits(logits=outputs, labels=y))\n",
        "\t\n",
        "\tnetwork = (x, y, loss, outputs)\n",
        "\n",
        "\tsess_config = tf.compat.v1.ConfigProto()\n",
        "\tsess_config.gpu_options.allow_growth = True\n",
        "\n",
        "\twith tf.compat.v1.Session(config=sess_config) as sess:\n",
        "\t\t\n",
        "\t\tfull_batch[0], mean_tr = normalize_and_reshape(full_batch[0], dim=config.dim, mean_tr=None)\n",
        "\t\tif val_batch is not None:\n",
        "\t\t\tval_batch[0], _ = normalize_and_reshape(val_batch[0], dim=config.dim, mean_tr=mean_tr)\n",
        "\n",
        "\t\tparam = tf.compat.v1.trainable_variables()\n",
        "\n",
        "\t\tmean_param = tf.compat.v1.get_variable(name='mean_tr', initializer=mean_tr, trainable=False, \n",
        "\t\t\t\t\tvalidate_shape=True, use_resource=False)\n",
        "\t\tlabel_enum_var=tf.compat.v1.get_variable(name='label_enum', initializer=label_enum, trainable=False,\n",
        "\t\t\t\t\tvalidate_shape=True, use_resource=False)\n",
        "\t\tsaver = tf.compat.v1.train.Saver(var_list=param+[mean_param])\n",
        "\t\t\n",
        "\t\tif config.optim in ('SGD', 'Adam'):\n",
        "\t\t\tgradient_trainer(\n",
        "\t\t\t\tconfig, sess, network, full_batch, val_batch, saver, test_network)\n",
        "\t\telif config.optim == 'NewtonCG':\n",
        "\t\t\tnewton_trainer(\n",
        "\t\t\t\tconfig, sess, network, full_batch, val_batch, saver, test_network=test_network)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wi6UglYiD7Zd"
      },
      "source": [
        "## Train ##"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cSgKfh4r5lu7",
        "outputId": "584d7216-f928-4e08-edd9-865c8b9ad2eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 619
        }
      },
      "source": [
        "train_model()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(709018,)\n",
            "(709018, 1024)\n",
            "[1538, 1024, 1]\n",
            "709018\n",
            "1 1538 1024\n",
            "1024\n",
            "(79976,)\n",
            "(79976, 1024)\n",
            "[1538, 1024, 1]\n",
            "79976\n",
            "1 1538 1024\n",
            "1024\n",
            "You choose not to specify a random seed.A different result is produced after each run.\n",
            "Saving log to: ./running_log/logger.log\n",
            "[709018, 1, 1538, 1024]\n",
            "No mean of data provided! Normalize images by their own mean.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-4dc2ba0c028a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-13-5340dcd3e468>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m    299\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msess_config\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 301\u001b[0;31m                 \u001b[0mfull_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean_tr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalize_and_reshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean_tr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    302\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mval_batch\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m                         \u001b[0mval_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalize_and_reshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean_tr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmean_tr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-a02bed765e10>\u001b[0m in \u001b[0;36mnormalize_and_reshape\u001b[0;34m(images, dim, mean_tr)\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mmean_tr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m     \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m     \u001b[0;31m# Tensorflow accepts data shape: B x H x W x C\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m     \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 726034432 into shape (709018,1,1538,1024)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UwH_nXJZDz2T"
      },
      "source": [
        "## Predict ##"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9zyzmNHFnOwD"
      },
      "source": [
        "# Arguments for prediction PSSP dataset\n",
        "pred_args = (\"--bsize 1024 --valid_set ./\" + VALID_FILE + \" --train_set ./\" + TRAIN_FILE + \n",
        "\t\t\t\t\t\t \" --model ./saved_model/model.ckpt --dim \" +\n",
        "             str(HEIGHT) + \" \" + str(WIDTH) + \" \" + str(DEPTH)).split()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s6MctxH5_nTR"
      },
      "source": [
        "valid_origin =  \"{0}_test_fold{1}.txt\".format(dataset.lower(),str(fold)) # train set  \n",
        "train_origin = \"{0}_train_fold{1}.txt\".format(dataset.lower(),str(fold)) # validation set\n",
        "test_origin = \"casp13_sorted.txt\" # test set CASP13\n",
        "print(valid_origin)\n",
        "print(train_origin)\n",
        "print(test_origin)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s0R1rY5rw030"
      },
      "source": [
        "import requests\n",
        "valid_f = requests.get(valid_origin)\n",
        "valid_f = valid_f.text.split('\\n')[0:-1]\n",
        "train_f = requests.get(train_origin)\n",
        "train_f = train_f.text.split('\\n')[0:-1]\n",
        "test_f = requests.get(test_origin)\n",
        "test_f = test_f.text.split('\\n')[0:-1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IeQqT96h_NHr"
      },
      "source": [
        "VALID_PRED_FILE=\"pred_test_fold{0}.txt\".format(fold)\n",
        "TRAIN_PRED_FILE=\"pred_train_fold{0}.txt\".format(fold)\n",
        "TEST_PRED_FILE=\"pred_casp13_fold{0}.txt\".format(fold)\n",
        "print(VALID_PRED_FILE)\n",
        "print(TRAIN_PRED_FILE)\n",
        "print(TEST_PRED_FILE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NNNnI_mU6LN5"
      },
      "source": [
        "##Declare Predict Methods##"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kYGPhdqk5wYp"
      },
      "source": [
        "def create_output_pred(pred, origin_f, outFileName):\n",
        "    pred = pred.astype(int)\n",
        "    labels = ['C', 'E', 'H']\n",
        "    counter = 0\n",
        "    with open(outFileName, 'w') as out_file:\n",
        "        for line in range(0, len(origin_f)//3):\n",
        "            protein_name = origin_f[line*3]\n",
        "            primary_structure = origin_f[line*3+1].replace('!', '')\n",
        "            secondary_structure = origin_f[line*3+2].replace('!', '')\n",
        "            prediction = \"\"\n",
        "            for c in secondary_structure:\n",
        "                if (c != '!'):\n",
        "                    prediction = prediction + labels[pred[counter]]\n",
        "                    counter += 1\n",
        "            out_file.write(protein_name + \"\\n\")\n",
        "            out_file.write(primary_structure + \"\\n\")\n",
        "            out_file.write(secondary_structure + \"\\n\")\n",
        "            out_file.write(prediction + \"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EA8Pq5a0C3m5"
      },
      "source": [
        "# import tensorflow as tf \n",
        "# tf.compat.v1.disable_eager_execution()\n",
        "# from utilities import predict, read_data, normalize_and_reshape\n",
        "# from net.net import CNN\n",
        "# import numpy as np \n",
        "# import argparse\n",
        "# import pdb\n",
        "\n",
        "def parse_args():\n",
        "\t\tparser = argparse.ArgumentParser(description='prediction')\n",
        "\t\tparser.add_argument('--test_set', dest='test_set',\n",
        "\t\t\t\t\t\t\thelp='provide the directory of .mat file for testing',\n",
        "\t\t\t\t\t\t\tdefault=None, type=str)\n",
        "\t\tparser.add_argument('--valid_set', dest='valid_set',\n",
        "\t\t\t\t\t\t\thelp='provide the directory of .mat file for validation',\n",
        "\t\t\t\t\t\t\tdefault=None, type=str)\n",
        "\t\tparser.add_argument('--train_set', dest='train_set',\n",
        "\t\t\t\t\t\t\thelp='provide the directory of .mat file for training',\n",
        "\t\t\t\t\t\t\tdefault=None, type=str)\n",
        "\t\tparser.add_argument('--model', dest='model_file',\n",
        "\t\t\t\t\t\t\thelp='provide file storing network parameters, i.e. ./dir/model.ckpt',\n",
        "\t\t\t\t\t\t\tdefault='./saved_model/model.ckpt', type=str)\n",
        "\t\tparser.add_argument('--bsize', dest='bsize',\n",
        "\t\t\t\t\t\t\thelp='batch size',\n",
        "\t\t\t\t\t\t\tdefault=1024, type=int)\n",
        "\t\tparser.add_argument('--loss', dest='loss', \n",
        "\t\t\t\t\t\t\thelp='which loss function to use: MSELoss or CrossEntropy',\n",
        "\t\t\t\t\t\t\tdefault='MSELoss', type=str)\n",
        "\t\tparser.add_argument('--dim', dest='dim', nargs='+', help='input dimension of data,'+\\\n",
        "\t\t\t\t\t\t\t'shape must be:  height width num_channels',\n",
        "\t\t\t\t\t\t\tdefault=[32, 32, 3], type=int)\n",
        "\n",
        "\t\targs = parser.parse_args(args=pred_args)\n",
        "\t\treturn args\n",
        "\n",
        "def predict_model():\n",
        "\t\targs = parse_args()\n",
        "\n",
        "\t\tsess_config = tf.compat.v1.ConfigProto()\n",
        "\t\tsess_config.gpu_options.allow_growth = True\n",
        "\n",
        "\t\twith tf.compat.v1.Session(config=sess_config) as sess:\n",
        "\t\t\t\tgraph_address = args.model_file + '.meta'\n",
        "\t\t\t\timported_graph = tf.compat.v1.train.import_meta_graph(graph_address)\n",
        "\t\t\t\timported_graph.restore(sess, args.model_file)\n",
        "\t\t\t\tmean_param = [v for v in tf.compat.v1.global_variables() if 'mean_tr:0' in v.name][0]\n",
        "\t\t\t\tlabel_enum_var = [v for v in tf.compat.v1.global_variables() if 'label_enum:0' in v.name][0]\n",
        "\n",
        "\t\t\t\tsess.run(tf.compat.v1.variables_initializer([mean_param, label_enum_var]))\n",
        "\t\t\t\tmean_tr = sess.run(mean_param)\n",
        "\t\t\t\tlabel_enum = sess.run(label_enum_var)\n",
        "\n",
        "\t\t\t\tx = tf.compat.v1.get_default_graph().get_tensor_by_name('main_params/input_of_net:0')\n",
        "\t\t\t\ty = tf.compat.v1.get_default_graph().get_tensor_by_name('main_params/labels:0')\n",
        "\t\t\t\toutputs = tf.compat.v1.get_default_graph().get_tensor_by_name('output_of_net:0')\n",
        "\n",
        "\t\t\t\tif args.loss == 'MSELoss':\n",
        "\t\t\t\t\t\tloss = tf.reduce_sum(input_tensor=tf.pow(outputs-y, 2))\n",
        "\t\t\t\telse:\n",
        "\t\t\t\t\t\tloss = tf.reduce_sum(input_tensor=\n",
        "\t\t\t\t\t\t    tf.nn.softmax_cross_entropy_with_logits(logits=outputs, labels=tf.stop_gradient(y)))\n",
        "\t\t\t\t\n",
        "\t\t\t\tnetwork = (x, y, loss, outputs)\n",
        "\n",
        "\t\t\t\tif args.valid_set is not None:\n",
        "\t\t\t\t\t\tvalid_batch, num_cls, _ = read_data(args.valid_set, dim=args.dim, label_enum=label_enum)\n",
        "\t\t\t\t\t\tvalid_batch[0], _ = normalize_and_reshape(valid_batch[0], dim=args.dim, mean_tr=mean_tr)\n",
        "\t\t\t\n",
        "\t\t\t\t\t\tavg_loss_valid, avg_acc_valid, results_valid = predict(sess, network, valid_batch, args.bsize)\n",
        "\n",
        "\t\t\t\t\t\t# convert results back to the original labels\n",
        "\t\t\t\t\t\tinverse_map = dict(zip(np.arange(num_cls), label_enum))\n",
        "\t\t\t\t\t\tresults_valid = np.expand_dims(results_valid, axis=1)\n",
        "\t\t\t\t\t\tresults_valid = np.apply_along_axis(lambda x: inverse_map[x[0]], axis=1, arr=results_valid)\n",
        "\n",
        "\t\t\t\t\t\tcreate_output_pred(results_valid, valid_f, VALID_PRED_FILE)\n",
        "\t\t\t\t\t\tprint('In valid phase, average loss: {:.3f} | average accuracy: {:.3f}%'.\\\n",
        "\t\t\t\t\t\t\tformat(avg_loss_valid, avg_acc_valid*100))\n",
        "\t\t\t\t\n",
        "\t\t\t\tif args.train_set is not None:\n",
        "\t\t\t\t\t\ttrain_batch, num_cls, _ = read_data(args.train_set, dim=args.dim, label_enum=label_enum)\n",
        "\t\t\t\t\t\ttrain_batch[0], _ = normalize_and_reshape(train_batch[0], dim=args.dim, mean_tr=mean_tr)\n",
        "\n",
        "\t\t\t\t\t\tavg_loss_train, avg_acc_train, results_train = predict(sess, network, train_batch, args.bsize)\n",
        "\t\t\t\t\t\t# convert results back to the original labels\n",
        "\t\t\t\t\t\tinverse_map = dict(zip(np.arange(num_cls), label_enum))\n",
        "\t\t\t\t\t\tresults_train = np.expand_dims(results_train, axis=1)\n",
        "\t\t\t\t\t\tresults_train = np.apply_along_axis(lambda x: inverse_map[x[0]], axis=1, arr=results_train)\n",
        "\n",
        "\t\t\t\t\t\t# create_output_pred(results, results_train)\n",
        "\n",
        "\t\t\t\t\t\tcreate_output_pred(results_train, train_f, TRAIN_PRED_FILE)\n",
        "\t\t\t\t\t\tprint('In train phase, average loss: {:.3f} | average accuracy: {:.3f}%'.\\\n",
        "\t\t\t\t\t\t\tformat(avg_loss_train, avg_acc_train*100))\n",
        "\n",
        "\t\t\t\tif args.test_set is not None:\n",
        "\t\t\t\t\t\ttest_batch, num_cls, _ = read_data(args.test_set, dim=args.dim, label_enum=label_enum)\n",
        "\t\t\t\t\t\ttest_batch[0], _ = normalize_and_reshape(test_batch[0], dim=args.dim, mean_tr=mean_tr)\n",
        "\n",
        "\t\t\t\t\t\tavg_loss_test, avg_acc_test, results_test = predict(sess, network, test_batch, args.bsize)\n",
        "\t\t\t\t\t\t# convert results back to the original labels\n",
        "\t\t\t\t\t\tinverse_map = dict(zip(np.arange(num_cls), label_enum))\n",
        "\t\t\t\t\t\tresults_test = np.expand_dims(results_test, axis=1)\n",
        "\t\t\t\t\t\tresults_test = np.apply_along_axis(lambda x: inverse_map[x[0]], axis=1, arr=results_test)\n",
        "\n",
        "\t\t\t\t\t\t# create_output_pred(results, results_train)\n",
        "\n",
        "\t\t\t\t\t\tcreate_output_pred(results_test, test_f, TEST_PRED_FILE)\n",
        "\t\t\t\t\t\tprint('In test phase, average loss: {:.3f} | average accuracy: {:.3f}%'.\\\n",
        "\t\t\t\t\t\t\tformat(avg_loss_test, avg_acc_test*100))\n",
        "\t\t\t"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aN8BTASh6eSL"
      },
      "source": [
        "##Run Predict and Display output##"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gCInY5uB6Y3G"
      },
      "source": [
        "predict_model()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "loR25EiOE4cQ"
      },
      "source": [
        "# !head \"$VALID_PRED_FILE\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NOnpS-N3DwhK"
      },
      "source": [
        "# !head \"$TRAIN_PRED_FILE\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exgdCTXb68Vl"
      },
      "source": [
        "## Check Test score on CASP13 ##"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lda3hrM4dmWk"
      },
      "source": [
        "# Arguments for prediction PSSP dataset\n",
        "pred_args = (\"--bsize 1024 --test_set ./\" + TEST_FILE + \n",
        "\t\t\t\t\t\t \" --model ./saved_model/model.ckpt --dim \" +\n",
        "             str(HEIGHT) + \" \" + str(WIDTH) + \" \" + str(DEPTH)).split()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3JCC5-2mk0rs"
      },
      "source": [
        "predict_model()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PMY6ihMjG1iO"
      },
      "source": [
        "# !head \"$TEST_PRED_FILE\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H93cFHmTH1su"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}