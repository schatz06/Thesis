{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Notebooks_CNN_HFO_v2.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "PYXlwVnXKhOJ",
        "CVtiFO19ymk_",
        "fzh5PNccJ28-",
        "gcdrh-r9JRdS",
        "2pQEA9LADkBP",
        "J5uoUeQBDq5Q",
        "vMCxn-8P5tsH",
        "UwH_nXJZDz2T",
        "NNNnI_mU6LN5"
      ],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/schatz06/Thesis/blob/main/Notebooks_CNN_HFO_v3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aa_PJwlt7zI2"
      },
      "source": [
        "##Mount drive## "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9DjpeG7t71KC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9c6bbe9-d406-479f-98c6-ca0d30e29fa9"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IMo4HooFl0dv"
      },
      "source": [
        "##General Variables to use to load data##"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sJHXZcSlNqL9"
      },
      "source": [
        "fold = 0 # which fold to take \n",
        "embedding = \"protbert\"\n",
        "# embedding = \"seqvec\"\n",
        "# dataset=\"PISCES\" \n",
        "dataset=\"CB513\"\n",
        "USE_HFO=True"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PYXlwVnXKhOJ"
      },
      "source": [
        "## Imports ##"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WCuZCtMgg5ns"
      },
      "source": [
        "# Reload all modules (except those excluded by %aimport) every time before executing the Python code typed.\n",
        "%load_ext autoreload \n",
        "%autoreload 2\n",
        "# matplotlib graphs will be included in your notebook, next to the code. \n",
        "%matplotlib inline"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HKDaAU3F5Bgs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb76547b-b126-4948-ea3c-57ec78a42f4a"
      },
      "source": [
        "# install hdf5storage package \r\n",
        "!pip install hdf5storage"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: hdf5storage in /usr/local/lib/python3.6/dist-packages (0.1.15)\n",
            "Requirement already satisfied: h5py>=2.1; python_version >= \"3.3\" in /usr/local/lib/python3.6/dist-packages (from hdf5storage) (2.10.0)\n",
            "Requirement already satisfied: numpy; python_version >= \"3.4\" in /usr/local/lib/python3.6/dist-packages (from hdf5storage) (1.19.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py>=2.1; python_version >= \"3.3\"->hdf5storage) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cjrANRzOFauh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e45dcbd7-d0dc-4eb7-dd91-a87578a963c9"
      },
      "source": [
        "import pdb # python debugger\n",
        "import numpy as np # import numpy \n",
        "import tensorflow as tf # import tensorflow  \n",
        "tf.compat.v1.disable_eager_execution() # disable eager execution\n",
        "import time # import time\n",
        "import math # import math\n",
        "import argparse # import argparse\n",
        "import os # import os\n",
        "import scipy.io as sio # import scipy.io \n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior() # makes different behaviors betweem tf_v1 & tf_v2 behave the same \n",
        "from tensorflow.python.client import device_lib # package to find available gpus\n",
        "import pandas as pd # import padas\n",
        "import hdf5storage # import hdf5storage"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CVtiFO19ymk_"
      },
      "source": [
        "## Get data ##"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yfOssHbYEh-W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef74bfe8-3f4c-43ae-c1e7-79eade0b8a53"
      },
      "source": [
        "VALID_FILE = \"/content/drive/MyDrive/Datasets/{0}_{1}_testSet{2}.mat\".format(dataset.lower(),embedding,str(fold)) # validation set\n",
        "TRAIN_FILE = \"/content/drive/MyDrive/Datasets/{0}_{1}_trainSet{2}.mat\".format(dataset.lower(),embedding,str(fold)) # train set  \n",
        "TEST_FILE = \"/content/drive/MyDrive/Datasets/casp13_{0}.mat\".format(embedding) # test set CASP13\n",
        "print(VALID_FILE)\n",
        "print(TRAIN_FILE)\n",
        "print(TEST_FILE)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/Datasets/cb513_protbert_testSet0.mat\n",
            "/content/drive/MyDrive/Datasets/cb513_protbert_trainSet0.mat\n",
            "/content/drive/MyDrive/Datasets/casp13_protbert.mat\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QKVIpZww1-dw"
      },
      "source": [
        "HEIGHT = 1538\n",
        "WIDTH = 1024\n",
        "DEPTH = 1\n",
        "CATEGORIES = 3 # number of different classification categories"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fzh5PNccJ28-"
      },
      "source": [
        "## VGG ##"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NwaNt8clJ5Vu"
      },
      "source": [
        "\"\"\"\n",
        "Codes are modifeid from PyTorch and Tensorflow Versions of VGG: \n",
        "https://github.com/pytorch/vision/blob/master/torchvision/models/vgg.py, and\n",
        "https://github.com/keras-team/keras-applications/blob/master/keras_applications/vgg16.py\n",
        "\"\"\"\n",
        "\n",
        "# import tensorflow.compat.v1 as tf\n",
        "# tf.disable_v2_behavior()\n",
        "# import numpy as np \n",
        "# import pdb\n",
        "from tensorflow.keras.applications.vgg16 import VGG16 as vgg16 # import vgg16 convolutional network\n",
        "from tensorflow.keras.applications.vgg19 import VGG19 as vgg19 # import vgg19 convolutional network\n",
        "\n",
        "__all__ = ['VGG11', 'VGG13', 'VGG16','VGG19'] # array holds all vgg CNN's names\n",
        " \n",
        "def VGG(feature, num_cls): # define VGG \n",
        "\n",
        "\twith tf.variable_scope('fully_connected') as scope:\n",
        "\t\tdim =np.prod(feature.shape[1:]) # returns the product of the given array\n",
        "\t\tx = tf.reshape(feature, [-1, dim]) # reshape tensor\n",
        "\n",
        "\t\tx = tf.keras.layers.Dense(units=4096, activation='relu', name=scope.name)(x) # define layers \n",
        "\t\tx = tf.keras.layers.Dense(units=4096, activation='relu', name=scope.name)(x)\n",
        "\t\tx = tf.keras.layers.Dense(units=num_cls, name=scope.name)(x)\n",
        "\n",
        "\treturn x\n",
        "# make the layers of CNN \n",
        "def make_layers(x, cfg):\n",
        "\tfor v in cfg:\n",
        "\t\tif v == 'M':\n",
        "\t\t\tx = tf.keras.layers.MaxPool2D(pool_size=[2, 2], strides=2, padding='valid')(x)\n",
        "\t\telse:\n",
        "\t\t\tx = tf.keras.layers.Conv2D(\n",
        "\t\t\tfilters=v,\n",
        "\t\t\tkernel_size=[3, 3],\n",
        "\t\t\tpadding='SAME',\n",
        "\t\t\tactivation=tf.nn.relu\n",
        "\t\t\t)(x)\n",
        "\treturn x\n",
        "\n",
        "cfg = {\n",
        "\t'A': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
        "\t'B': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
        "\t'D': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
        "\t'E': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', \n",
        "\t\t  512, 512, 512, 512, 'M'],\n",
        "}\n",
        "\n",
        "def VGG11(x_images, num_cls):\n",
        "\tfeature = make_layers(x_images, cfg['A'])\n",
        "\treturn VGG(feature, num_cls)\n",
        "\n",
        "def VGG13(x_images, num_cls):\n",
        "\tfeature = make_layers(x_images, cfg['B'])\n",
        "\treturn VGG(feature, num_cls)\n",
        "\n",
        "def VGG16(x_images, num_cls):\n",
        "\tfeature = make_layers(x_images, cfg['D'])\n",
        "\treturn VGG(feature, num_cls)\n",
        "\n",
        "def VGG19(x_images, num_cls):\n",
        "\tfeature = make_layers(x_images, cfg['E'])\n",
        "\treturn VGG(feature, num_cls)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gcdrh-r9JRdS"
      },
      "source": [
        "## Net ##"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kHA5zxK1JRtg"
      },
      "source": [
        "# import tensorflow.compat.v1 as tf\n",
        "# tf.disable_v2_behavior()\n",
        "# import math\n",
        "# import pdb\n",
        "# from tensorflow.python.client import device_lib\n",
        "# import numpy as np\n",
        "# from net.vgg import *\n",
        "\n",
        "def CNN_4layers(x_image, num_cls, reuse=False):\n",
        "\t_NUM_CLASSES = num_cls\n",
        "\twith tf.variable_scope('conv1', reuse=reuse) as scope:\n",
        "\t\tconv = tf.keras.layers.Conv2D(\n",
        "\t\t\tfilters=64,\n",
        "\t\t\tkernel_size=[3, 3],\n",
        "\t\t\tpadding='SAME',\n",
        "\t\t\tactivation=tf.nn.relu\n",
        "\t\t)(x_image)\n",
        "\n",
        "\twith tf.variable_scope('conv2', reuse=reuse) as scope:\n",
        "\t\tconv = tf.keras.layers.Conv2D(\n",
        "\t\t\tfilters=64,\n",
        "\t\t\tkernel_size=[3, 3],\n",
        "\t\t\tpadding='SAME',\n",
        "\t\t\tactivation=tf.nn.relu\n",
        "\t\t)(conv)\n",
        "\t\t\n",
        "\twith tf.variable_scope('conv3', reuse=reuse) as scope:\n",
        "\t\tconv = tf.keras.layers.Conv2D(\n",
        "\t\t\tfilters=64,\n",
        "\t\t\tkernel_size=[3, 3],\n",
        "\t\t\tpadding='SAME',\n",
        "\t\t\tactivation=tf.nn.relu\n",
        "\t\t)(conv)\n",
        "\n",
        "\twith tf.variable_scope('fully_connected', reuse=reuse) as scope:\n",
        "\t\tdim =np.prod(conv.shape[1:])\n",
        "\t\tflat = tf.reshape(conv, [-1, dim])\n",
        "\t\toutputs = tf.keras.layers.Dense(units=_NUM_CLASSES, name=scope.name)(flat)\n",
        "\n",
        "\treturn outputs\n",
        "\n",
        "\n",
        "\t# with tf.variable_scope('conv1', reuse=reuse) as scope:\n",
        "\t# \tconv = tf.keras.layers.Conv2D(\n",
        "\t# \t\tfilters=32,\n",
        "\t# \t\tkernel_size=[5, 5],\n",
        "\t# \t\tpadding='SAME',\n",
        "\t# \t\tactivation=tf.nn.relu\n",
        "\t# \t)(x_image)\n",
        "\t# \tpool = tf.keras.layers.MaxPool2D(pool_size=[2, 2], strides=2, padding='valid')(conv)\n",
        "\t# \t# N x 16 x 16 x 32\n",
        "\n",
        "\t# with tf.variable_scope('conv2', reuse=reuse) as scope:\n",
        "\t# \tconv = tf.keras.layers.Conv2D(\n",
        "\t# \t\tfilters=64,\n",
        "\t# \t\tkernel_size=[3, 3],\n",
        "\t# \t\tpadding='SAME',\n",
        "\t# \t\tactivation=tf.nn.relu\n",
        "\t# \t)(pool)\n",
        "\t# \tpool = tf.keras.layers.MaxPool2D(pool_size=[2, 2], strides=2, padding='valid')(conv)\n",
        "\t# \t# N x 8 x 8 x 64\n",
        "\t\t\n",
        "\t# with tf.variable_scope('conv3', reuse=reuse) as scope:\n",
        "\t# \tconv = tf.keras.layers.Conv2D(\n",
        "\t# \t\tfilters=64,\n",
        "\t# \t\tkernel_size=[3, 3],\n",
        "\t# \t\tpadding='SAME',\n",
        "\t# \t\tactivation=tf.nn.relu\n",
        "\t# \t)(pool)\n",
        "\t# \tpool = tf.keras.layers.MaxPool2D(pool_size=[2, 2], strides=2, padding='valid')(conv)\n",
        "\t# \t# N x 4 x 4 x 64\n",
        "\n",
        "\t# with tf.variable_scope('fully_connected', reuse=reuse) as scope:\n",
        "\t# \tdim =np.prod(pool.shape[1:])\n",
        "\t# \tflat = tf.reshape(pool, [-1, dim])\n",
        "\t# \toutputs = tf.keras.layers.Dense(units=_NUM_CLASSES, name=scope.name)(flat)\n",
        "\n",
        "\t# return outputs\n",
        "\n",
        "def CNN_7layers(x_image, num_cls, reuse=False):\n",
        "\t_NUM_CLASSES = num_cls\n",
        "\twith tf.variable_scope('conv1', reuse=reuse) as scope:\n",
        "\t\tconv = tf.keras.layers.Conv2D(\n",
        "\t\t\tfilters=64,\n",
        "\t\t\tkernel_size=[3, 3],\n",
        "\t\t\tpadding='SAME',\n",
        "\t\t\tactivation=tf.nn.relu\n",
        "\t\t)(x_image)\n",
        "\t\tconv = tf.keras.layers.Conv2D(\n",
        "\t\t\tfilters=64,\n",
        "\t\t\tkernel_size=[3, 3],\n",
        "\t\t\tpadding='SAME',\n",
        "\t\t\tactivation=tf.nn.relu\n",
        "\t\t)(conv)\n",
        "\n",
        "\twith tf.variable_scope('conv2', reuse=reuse) as scope:\n",
        "\t\tconv = tf.keras.layers.Conv2D(\n",
        "\t\t\tfilters=64,\n",
        "\t\t\tkernel_size=[3, 3],\n",
        "\t\t\tpadding='SAME',\n",
        "\t\t\tactivation=tf.nn.relu\n",
        "\t\t)(conv)\n",
        "\t\tconv = tf.keras.layers.Conv2D(\n",
        "\t\t\tfilters=64,\n",
        "\t\t\tkernel_size=[3, 3],\n",
        "\t\t\tpadding='SAME',\n",
        "\t\t\tactivation=tf.nn.relu\n",
        "\t\t)(conv)\n",
        "\n",
        "\twith tf.variable_scope('conv3', reuse=reuse) as scope:\n",
        "\t\tconv = tf.keras.layers.Conv2D(\n",
        "\t\t\tfilters=64,\n",
        "\t\t\tkernel_size=[3, 3],\n",
        "\t\t\tpadding='SAME',\n",
        "\t\t\tactivation=tf.nn.relu\n",
        "\t\t)(conv)\n",
        "\t\tconv = tf.keras.layers.Conv2D(\n",
        "\t\t\tfilters=64,\n",
        "\t\t\tkernel_size=[3, 3],\n",
        "\t\t\tpadding='SAME',\n",
        "\t\t\tactivation=tf.nn.relu\n",
        "\t\t)(conv)\n",
        "\n",
        "\twith tf.variable_scope('fully_connected', reuse=reuse) as scope:\n",
        "\t\tdim = np.prod(conv.shape[1:])\n",
        "\t\tflat = tf.reshape(conv, [-1, dim])\n",
        "\t\toutputs = tf.keras.layers.Dense(units=_NUM_CLASSES, name=scope.name)(flat)\n",
        "\n",
        "\treturn outputs\n",
        "\n",
        "\t# with tf.variable_scope('conv1', reuse=reuse) as scope:\n",
        "\t# \tconv = tf.keras.layers.Conv2D(\n",
        "\t# \t\tfilters=32,\n",
        "\t# \t\tkernel_size=[5, 5],\n",
        "\t# \t\tpadding='SAME',\n",
        "\t# \t\tactivation=tf.nn.relu\n",
        "\t# \t)(x_image)\n",
        "\t# \tconv = tf.keras.layers.Conv2D(\n",
        "\t# \t\tfilters=32,\n",
        "\t# \t\tkernel_size=[3, 3],\n",
        "\t# \t\tpadding='SAME',\n",
        "\t# \t\tactivation=tf.nn.relu\n",
        "\t# \t)(conv)\n",
        "\t# \tpool = tf.keras.layers.MaxPool2D(pool_size=[2, 2], strides=2, padding='valid')(conv)\n",
        "\t# \t# N x 16 x 16 x 32\n",
        "\n",
        "\t# with tf.variable_scope('conv2', reuse=reuse) as scope:\n",
        "\t# \tconv = tf.keras.layers.Conv2D(\n",
        "\t# \t\tfilters=64,\n",
        "\t# \t\tkernel_size=[3, 3],\n",
        "\t# \t\tpadding='SAME',\n",
        "\t# \t\tactivation=tf.nn.relu\n",
        "\t# \t)(pool)\n",
        "\t# \tconv = tf.keras.layers.Conv2D(\n",
        "\t# \t\tfilters=64,\n",
        "\t# \t\tkernel_size=[3, 3],\n",
        "\t# \t\tpadding='SAME',\n",
        "\t# \t\tactivation=tf.nn.relu\n",
        "\t# \t)(conv)\n",
        "\t# \tpool = tf.keras.layers.MaxPool2D(pool_size=[2, 2], strides=2, padding='valid')(conv)\n",
        "\t# \t# N x 8 x 8 x 64\n",
        "\n",
        "\t# with tf.variable_scope('conv3', reuse=reuse) as scope:\n",
        "\t# \tconv = tf.keras.layers.Conv2D(\n",
        "\t# \t\tfilters=64,\n",
        "\t# \t\tkernel_size=[3, 3],\n",
        "\t# \t\tpadding='SAME',\n",
        "\t# \t\tactivation=tf.nn.relu\n",
        "\t# \t)(pool)\n",
        "\t# \tconv = tf.keras.layers.Conv2D(\n",
        "\t# \t\tfilters=128,\n",
        "\t# \t\tkernel_size=[3, 3],\n",
        "\t# \t\tpadding='SAME',\n",
        "\t# \t\tactivation=tf.nn.relu\n",
        "\t# \t)(conv)\n",
        "\t# \tpool = tf.keras.layers.MaxPool2D(pool_size=[2, 2], strides=2, padding='valid')(conv)\n",
        "\t# \t# pool = tf.layers.dropout(pool, rate=0.25, name=scope.name)\n",
        "\t# \t# N x 4 x 4 x 128\n",
        "\n",
        "\t# with tf.variable_scope('fully_connected', reuse=reuse) as scope:\n",
        "\t# \tdim = np.prod(pool.shape[1:])\n",
        "\t# \tflat = tf.reshape(pool, [-1, dim])\n",
        "\t# \toutputs = tf.keras.layers.Dense(units=_NUM_CLASSES, name=scope.name)(flat)\n",
        "\n",
        "\t# return outputs\n",
        "\n",
        "def CNN(net, num_cls, dim):\n",
        "\n",
        "\t_NUM_CLASSES = num_cls\n",
        "\t_IMAGE_HEIGHT, _IMAGE_WIDTH, _IMAGE_CHANNELS = dim\n",
        "\n",
        "\twith tf.name_scope('main_params'):\n",
        "\t\tx = tf.placeholder(tf.float32, shape=[None, _IMAGE_HEIGHT, _IMAGE_WIDTH, _IMAGE_CHANNELS], name='input_of_net')\n",
        "\t\ty = tf.placeholder(tf.float32, shape=[None, _NUM_CLASSES], name='labels')\n",
        "\n",
        "\t# call CNN structure according to string net\n",
        "\toutputs = globals()[net](x, _NUM_CLASSES)\n",
        "\toutputs = tf.identity(outputs, name='output_of_net')\n",
        "\n",
        "\treturn (x, y, outputs)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2pQEA9LADkBP"
      },
      "source": [
        "## Utilities ##"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JxQzW-93DIdN"
      },
      "source": [
        "# import numpy as np\n",
        "# import math\n",
        "# import scipy.io as sio\n",
        "# import os\n",
        "# import math\n",
        "# import pdb\n",
        "\n",
        "class ConfigClass(object):\n",
        "    def __init__(self, args, num_data, num_cls):\n",
        "        super(ConfigClass, self).__init__()\n",
        "        self.args = args\n",
        "        self.iter_max = args.iter_max\n",
        "        \n",
        "        # Different notations of regularization term:\n",
        "        # In SGD, weight decay:\n",
        "        #     weight_decay <- lr/(C*num_of_training_samples)\n",
        "        # In Newton method:\n",
        "        #     C <- C * num_of_training_samples\n",
        "    \n",
        "\n",
        "        self.seed = args.seed\n",
        "\n",
        "        if self.seed is None:\n",
        "            print('You choose not to specify a random seed.'+\\\n",
        "                'A different result is produced after each run.')\n",
        "        elif isinstance(self.seed, int) and self.seed >= 0:\n",
        "            print('You specify random seed {}.'.format(self.seed))\n",
        "        else:\n",
        "            raise ValueError('Only accept None type or nonnegative integers for'+\\\n",
        "                    ' random seed argument!')\n",
        "\n",
        "        self.train_set = args.train_set\n",
        "        self.val_set = args.val_set\n",
        "        self.num_cls = num_cls\n",
        "        self.dim = args.dim\n",
        "\n",
        "        self.num_data = num_data\n",
        "        self.GNsize = min(args.GNsize, self.num_data)\n",
        "        self.C = args.C * self.num_data\n",
        "        self.net = args.net\n",
        "\n",
        "        self.xi = 0.1\n",
        "        self.CGmax = args.CGmax\n",
        "        self._lambda = args._lambda\n",
        "        self.drop = args.drop\n",
        "        self.boost = args.boost\n",
        "        self.eta = args.eta\n",
        "        self.lr = args.lr\n",
        "        self.lr_decay = args.lr_decay\n",
        "\n",
        "        self.bsize = args.bsize\n",
        "        if args.momentum < 0:\n",
        "            raise ValueError('Momentum needs to be larger than 0!')\n",
        "        self.momentum = args.momentum\n",
        "\n",
        "        self.loss = args.loss\n",
        "        if self.loss not in ('MSELoss', 'CrossEntropy'):\n",
        "            raise ValueError('Unrecognized loss type!')\n",
        "        self.optim = args.optim\n",
        "        if self.optim not in ('SGD', 'NewtonCG', 'Adam'):\n",
        "            raise ValueError('Only support SGD, Adam & NewtonCG optimizer!')\n",
        "        \n",
        "        self.log_file = args.log_file\n",
        "        self.model_file = args.model_file\n",
        "        self.screen_log_only = args.screen_log_only\n",
        "\n",
        "        if self.screen_log_only:\n",
        "            print('You choose not to store running log. Only store model to {}'.format(self.log_file))\n",
        "        else:\n",
        "            print('Saving log to: {}'.format(self.log_file))\n",
        "            dir_name, _ = os.path.split(self.log_file)\n",
        "            if not os.path.isdir(dir_name):\n",
        "                os.makedirs(dir_name, exist_ok=True)\n",
        "\n",
        "        dir_name, _ = os.path.split(self.model_file)\n",
        "        if not os.path.isdir(dir_name):\n",
        "            os.makedirs(dir_name, exist_ok=True)\n",
        "        \n",
        "        self.elapsed_time = 0.0\n",
        "\n",
        "def read_data(filename, dim, label_enum=None):\n",
        "    \"\"\"\n",
        "    args:\n",
        "    filename: the path where .mat files are stored\n",
        "    label_enum (default None): the list that stores the original labels. \n",
        "    If label_enum is None, the function will generate a new list which stores the \n",
        "    original labels in a sequence, and map original labels to [0, 1, ... number_of_classes-1]. \n",
        "    If label_enum is a list, the function will use it to convert \n",
        "    original labels to [0, 1,..., number_of_classes-1].\n",
        "    \"\"\"\n",
        "    mat_contents = sio.loadmat(filename)\n",
        "    #mat_contents = hdf5storage.loadmat(filename)\n",
        "    images, labels = mat_contents['x'], mat_contents['y']\n",
        "    labels = labels.reshape(-1)\n",
        "    images = images.reshape(images.shape[0], -1)\n",
        "    print(\"Images size after reshape = \",images.shape)\n",
        "    _IMAGE_HEIGHT, _IMAGE_WIDTH, _IMAGE_CHANNELS = dim\n",
        "    print(\"Single image dimension =\",dim)\n",
        "    zero_to_append = np.zeros((images.shape[0],_IMAGE_CHANNELS*_IMAGE_HEIGHT*_IMAGE_WIDTH-np.prod(images.shape[1:])))\n",
        "    print(zero_to_append.shape)\n",
        "    images = np.append(images, zero_to_append, axis=1)\n",
        "    #print(images_shape,labels.shape)\n",
        "    # check data validity\n",
        "    if label_enum is None:\n",
        "        label_enum, labels = np.unique(labels, return_inverse=True)\n",
        "        num_cls = labels.max() + 1\n",
        "        #print(num_cls)\n",
        "\n",
        "        if len(label_enum) != num_cls:\n",
        "            raise ValueError('The number of classes is not equal to the number of\\\n",
        "            labels in dataset. Please verify them.')\n",
        "    else:\n",
        "        num_cls = len(label_enum)\n",
        "        forward_map = dict(zip(label_enum, np.arange(num_cls)))\n",
        "        labels = np.expand_dims(labels, axis=1)\n",
        "        labels = np.apply_along_axis(lambda x:forward_map[x[0]], axis=1, arr=labels)\n",
        "      \n",
        "    # convert groundtruth to one-hot encoding\n",
        "    labels = np.eye(num_cls)[labels]\n",
        "    labels = labels.astype('float32')\n",
        "    print(\"Shape 0 \",images.shape[0])\n",
        "    labels = np.reshape(labels,(images.shape[0],-1))\n",
        "    print(\"Labels shape \", labels.shape)\n",
        "    print(labels)\n",
        "    return [images, labels], num_cls, label_enum\n",
        "\n",
        "def normalize_and_reshape(images, dim, mean_tr=None):\n",
        "    _IMAGE_HEIGHT, _IMAGE_WIDTH, _IMAGE_CHANNELS = dim\n",
        "    images_shape = [images.shape[0], _IMAGE_CHANNELS, _IMAGE_HEIGHT, _IMAGE_WIDTH]\n",
        "    # images normalization and zero centering\n",
        "    images = images.reshape(images_shape[0], -1)\n",
        "\n",
        "    images = images/255.0\n",
        "    if mean_tr is None:\n",
        "        print('No mean of data provided! Normalize images by their own mean.')\n",
        "        # if no mean_tr is provided, we calculate it according to the current data\n",
        "        mean_tr = images.mean(axis=0) \n",
        "    else:\n",
        "        print('Normalize images according to the provided mean.')\n",
        "        if np.prod(mean_tr.shape) != np.prod(dim):\n",
        "            raise ValueError('Dimension of provided mean does not agree with the data! Please verify them!')\n",
        "    images = images - mean_tr\n",
        "\n",
        "    images = images.reshape(images_shape)\n",
        "    # Tensorflow accepts data shape: B x H x W x C\n",
        "    images = np.transpose(images, (0, 2, 3, 1))\n",
        "    return images, mean_tr\n",
        "\n",
        "\n",
        "def predict(sess, network, test_batch, bsize):\n",
        "    x, y, loss, outputs = network\n",
        "\n",
        "    test_inputs, test_labels = test_batch\n",
        "    batch_size = bsize\n",
        "\n",
        "    num_data = test_labels.shape[0]\n",
        "    num_batches = math.ceil(num_data/batch_size)\n",
        "\n",
        "    results = np.zeros(shape=num_data, dtype=np.int)\n",
        "    infer_loss = 0.0\n",
        "\n",
        "    for i in range(num_batches):\n",
        "        batch_idx = np.arange(i*batch_size, min((i+1)*batch_size, num_data))\n",
        "\n",
        "        batch_input = test_inputs[batch_idx]\n",
        "        batch_labels = test_labels[batch_idx]\n",
        "\n",
        "        net_outputs, _loss = sess.run(\n",
        "            [outputs, loss], feed_dict={x: batch_input, y: batch_labels}\n",
        "            )\n",
        "        \n",
        "        results[batch_idx] = np.argmax(net_outputs, axis=1)\n",
        "        # note that _loss was summed over batches\n",
        "        infer_loss = infer_loss + _loss\n",
        "\n",
        "    avg_acc = (np.argmax(test_labels, axis=1) == results).mean()\n",
        "    avg_loss = infer_loss/num_data\n",
        "    \n",
        "    return avg_loss, avg_acc, results"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J5uoUeQBDq5Q"
      },
      "source": [
        "## Newton - CG ##"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IDug0aiKCqeG"
      },
      "source": [
        "# import pdb\n",
        "# import tensorflow as tf\n",
        "# import time\n",
        "# import numpy as np\n",
        "# import os\n",
        "# import math\n",
        "# from utilities import predict\n",
        "\n",
        "def Rop(f, weights, v):\n",
        "    \"\"\"Implementation of R operator\n",
        "    Args:\n",
        "        f: any function of weights\n",
        "        weights: list of tensors.\n",
        "        v: vector for right multiplication\n",
        "    Returns:\n",
        "        Jv: Jaccobian vector product, length same as\n",
        "            the number of output of f\n",
        "    \"\"\"\n",
        "    if type(f) == list:\n",
        "        u = [tf.zeros_like(ff) for ff in f]\n",
        "    else:\n",
        "        u = tf.zeros_like(f)  # dummy variable\n",
        "    g = tf.gradients(ys=f, xs=weights, grad_ys=u)\n",
        "    return tf.gradients(ys=g, xs=u, grad_ys=v)\n",
        "\n",
        "def Gauss_Newton_vec(outputs, loss, weights, v):\n",
        "    \"\"\"Implements Gauss-Newton vector product.\n",
        "    Args:\n",
        "        loss: Loss function.\n",
        "        outputs: outputs of the last layer (pre-softmax).\n",
        "        weights: Weights, list of tensors.\n",
        "        v: vector to be multiplied with Gauss Newton matrix\n",
        "    Returns:\n",
        "        J'BJv: Guass-Newton vector product.\n",
        "    \"\"\"\n",
        "    # Validate the input\n",
        "    if type(weights) == list:\n",
        "        if len(v) != len(weights):\n",
        "            raise ValueError(\"weights and v must have the same length.\")\n",
        "\n",
        "    grads_outputs = tf.gradients(ys=loss, xs=outputs)\n",
        "    BJv = Rop(grads_outputs, weights, v)\n",
        "    JBJv = tf.gradients(ys=outputs, xs=weights, grad_ys=BJv)\n",
        "    return JBJv\n",
        "    \n",
        "\n",
        "class newton_cg(object):\n",
        "    def __init__(self, config, sess, outputs, loss):\n",
        "        \"\"\"\n",
        "        initialize operations and vairables that will be used in newton\n",
        "        args:\n",
        "            sess: tensorflow session\n",
        "            outputs: output of the neural network (pre-softmax layer)\n",
        "            loss: function to calculate loss\n",
        "        \"\"\"\n",
        "        super(newton_cg, self).__init__()\n",
        "        self.sess = sess\n",
        "        self.config = config\n",
        "        self.outputs = outputs\n",
        "        self.loss = loss\n",
        "        self.param = tf.compat.v1.trainable_variables()\n",
        "\n",
        "        self.CGiter = 0\n",
        "        FLOAT = tf.float32\n",
        "        model_weight = self.vectorize(self.param)\n",
        "        \n",
        "        # initial variable used in CG\n",
        "        zeros = tf.zeros(model_weight.get_shape(), dtype=FLOAT)\n",
        "        self.r = tf.Variable(zeros, dtype=FLOAT, trainable=False)\n",
        "        self.v = tf.Variable(zeros, dtype=FLOAT, trainable=False)\n",
        "        self.s = tf.Variable(zeros, dtype=FLOAT, trainable=False)\n",
        "        self.g = tf.Variable(zeros, dtype=FLOAT, trainable=False)\n",
        "        # initial Gv, f for method minibatch\n",
        "        self.Gv = tf.Variable(zeros, dtype=FLOAT, trainable=False)\n",
        "        self.f = tf.Variable(0., dtype=FLOAT, trainable=False)\n",
        "\n",
        "        # rTr, cgtol and beta to be used in CG\n",
        "        self.rTr = tf.Variable(0., dtype=FLOAT, trainable=False)\n",
        "        self.cgtol = tf.Variable(0., dtype=FLOAT, trainable=False)\n",
        "        self.beta = tf.Variable(0., dtype=FLOAT, trainable=False)\n",
        "\n",
        "        # placeholder alpha, old_alpha and lambda\n",
        "        self.alpha = tf.compat.v1.placeholder(FLOAT, shape=[])\n",
        "        self.old_alpha = tf.compat.v1.placeholder(FLOAT, shape=[])\n",
        "        self._lambda = tf.compat.v1.placeholder(FLOAT, shape=[])\n",
        "\n",
        "        self.num_grad_segment = math.ceil(self.config.num_data/self.config.bsize)\n",
        "        self.num_Gv_segment = math.ceil(self.config.GNsize/self.config.bsize)\n",
        "\n",
        "        cal_loss, cal_lossgrad, cal_lossGv, \\\n",
        "        add_reg_avg_loss, add_reg_avg_grad, add_reg_avg_Gv, \\\n",
        "        zero_loss, zero_grad, zero_Gv = self._ops_in_minibatch()\n",
        "\n",
        "        # initial operations that will be used in minibatch and newton\n",
        "        self.cal_loss = cal_loss\n",
        "        self.cal_lossgrad = cal_lossgrad\n",
        "        self.cal_lossGv = cal_lossGv\n",
        "        self.add_reg_avg_loss = add_reg_avg_loss\n",
        "        self.add_reg_avg_grad = add_reg_avg_grad\n",
        "        self.add_reg_avg_Gv = add_reg_avg_Gv\n",
        "        self.zero_loss = zero_loss\n",
        "        self.zero_grad = zero_grad\n",
        "        self.zero_Gv = zero_Gv\n",
        "\n",
        "        self.CG, self.update_v = self._CG()\n",
        "        self.init_cg_vars = self._init_cg_vars()\n",
        "        self.update_gs = tf.tensordot(self.s, self.g, axes=1)\n",
        "        self.update_sGs = 0.5*tf.tensordot(self.s, -self.g-self.r-self._lambda*self.s, axes=1)\n",
        "        self.update_model = self._update_model()\n",
        "        self.gnorm = self.calc_norm(self.g)\n",
        "\n",
        "\n",
        "    def vectorize(self, tensors):\n",
        "        if isinstance(tensors, list) or isinstance(tensors, tuple):\n",
        "            vector = [tf.reshape(tensor, [-1]) for tensor in tensors]\n",
        "            return tf.concat(vector, 0) \n",
        "        else:\n",
        "            return tensors \n",
        "    \n",
        "    def inverse_vectorize(self, vector, param):\n",
        "        if isinstance(vector, list):\n",
        "            return vector\n",
        "        else:\n",
        "            tensors = []\n",
        "            offset = 0\n",
        "            num_total_param = np.sum([np.prod(p.shape.as_list()) for p in param])\n",
        "            for p in param:\n",
        "                numel = np.prod(p.shape.as_list())\n",
        "                tensors.append(tf.reshape(vector[offset: offset+numel], p.shape))\n",
        "                offset += numel\n",
        "\n",
        "            assert offset == num_total_param\n",
        "            return tensors\n",
        "\n",
        "    def calc_norm(self, v):\n",
        "        # default: frobenius norm\n",
        "        if isinstance(v, list):\n",
        "            norm = 0.\n",
        "            for p in v:\n",
        "                norm = norm + tf.norm(tensor=p)**2\n",
        "            return norm**0.5\n",
        "        else:\n",
        "            return tf.norm(tensor=v)\n",
        "\n",
        "    def _ops_in_minibatch(self):\n",
        "        \"\"\"\n",
        "        Define operations that will be used in method minibatch\n",
        "        Vectorization is already a deep copy operation.\n",
        "        Before using newton method, loss needs to be summed over training samples\n",
        "        to make results consistent.\n",
        "        \"\"\"\n",
        "\n",
        "        def cal_loss():\n",
        "            return tf.compat.v1.assign(self.f, self.f + self.loss)\n",
        "\n",
        "        def cal_lossgrad():\n",
        "            update_f = tf.compat.v1.assign(self.f, self.f + self.loss)\n",
        "\n",
        "            grad = tf.gradients(ys=self.loss, xs=self.param)\n",
        "            grad = self.vectorize(grad)\n",
        "            update_grad = tf.compat.v1.assign(self.g, self.g + grad)\n",
        "\n",
        "            return tf.group(*[update_f, update_grad])\n",
        "\n",
        "        def cal_lossGv():\n",
        "            v = self.inverse_vectorize(self.v, self.param)\n",
        "            Gv = Gauss_Newton_vec(self.outputs, self.loss, self.param, v)\n",
        "            Gv = self.vectorize(Gv)\n",
        "            return tf.compat.v1.assign(self.Gv, self.Gv + Gv) \n",
        "\n",
        "        # add regularization term to loss, gradient and Gv and further average over batches \n",
        "        def add_reg_avg_loss():\n",
        "            model_weight = self.vectorize(self.param)\n",
        "            reg = (self.calc_norm(model_weight))**2\n",
        "            reg = 1.0/(2*self.config.C) * reg\n",
        "            return tf.compat.v1.assign(self.f, reg + self.f/self.config.num_data)\n",
        "\n",
        "        def add_reg_avg_lossgrad():\n",
        "            model_weight = self.vectorize(self.param)\n",
        "            reg_grad = model_weight/self.config.C\n",
        "            return tf.compat.v1.assign(self.g, reg_grad + self.g/self.config.num_data)\n",
        "\n",
        "        def add_reg_avg_lossGv():\n",
        "            return tf.compat.v1.assign(self.Gv, (self._lambda + 1/self.config.C)*self.v\n",
        "             + self.Gv/self.config.GNsize) \n",
        "\n",
        "        # zero out loss, grad and Gv \n",
        "        def zero_loss():\n",
        "            return tf.compat.v1.assign(self.f, tf.zeros_like(self.f))\n",
        "        def zero_grad():\n",
        "            return tf.compat.v1.assign(self.g, tf.zeros_like(self.g))\n",
        "        def zero_Gv():\n",
        "            return tf.compat.v1.assign(self.Gv, tf.zeros_like(self.Gv))\n",
        "\n",
        "        return (cal_loss(), cal_lossgrad(), cal_lossGv(),\n",
        "                add_reg_avg_loss(), add_reg_avg_lossgrad(), add_reg_avg_lossGv(),\n",
        "                zero_loss(), zero_grad(), zero_Gv())\n",
        "\n",
        "    def minibatch(self, data_batch, place_holder_x, place_holder_y, mode):\n",
        "        \"\"\"\n",
        "        A function to evaluate either function value, global gradient or sub-sampled Gv\n",
        "        \"\"\"\n",
        "        if mode not in ('funonly', 'fungrad', 'Gv'):\n",
        "            raise ValueError('Unknown mode other than funonly & fungrad & Gv!')\n",
        "\n",
        "        inputs, labels = data_batch\n",
        "        num_data = labels.shape[0]\n",
        "        num_segment = math.ceil(num_data/self.config.bsize)\n",
        "        x, y = place_holder_x, place_holder_y\n",
        "\n",
        "        # before estimation starts, need to zero out f, grad and Gv according to the mode\n",
        "\n",
        "        if mode == 'funonly':\n",
        "            assert num_data == self.config.num_data\n",
        "            assert num_segment == self.num_grad_segment\n",
        "            self.sess.run(self.zero_loss)\n",
        "        elif mode == 'fungrad':\n",
        "            print(\"NUM data =:\",num_data)\n",
        "            print(\"COnfig num data \",self.config.num_data)\n",
        "            assert num_data == self.config.num_data\n",
        "            print(\"NUm segment \",num_segment)\n",
        "            print(\"Num grad segment \",self.num_grad_segment)\n",
        "            assert num_segment == self.num_grad_segment\n",
        "            self.sess.run([self.zero_loss, self.zero_grad])\n",
        "        else:\n",
        "            assert num_data == self.config.GNsize\n",
        "            assert num_segment == self.num_Gv_segment\n",
        "            self.sess.run(self.zero_Gv)\n",
        "\n",
        "        for i in range(num_segment):\n",
        "            \n",
        "            load_time = time.time()\n",
        "            idx = np.arange(i * self.config.bsize, min((i+1) * self.config.bsize, num_data))\n",
        "            batch_input = inputs[idx]\n",
        "            batch_labels = labels[idx]\n",
        "            batch_input = np.ascontiguousarray(batch_input)\n",
        "            batch_labels = np.ascontiguousarray(batch_labels)\n",
        "            self.config.elapsed_time += time.time() - load_time\n",
        "\n",
        "            if mode == 'funonly':\n",
        "\n",
        "                self.sess.run(self.cal_loss, feed_dict={\n",
        "                            x: batch_input, \n",
        "                            y: batch_labels,})\n",
        "\n",
        "            elif mode == 'fungrad':\n",
        "                \n",
        "                self.sess.run(self.cal_lossgrad, feed_dict={\n",
        "                            x: batch_input, \n",
        "                            y: batch_labels,})\n",
        "                \n",
        "            else:\n",
        "                \n",
        "                self.sess.run(self.cal_lossGv, feed_dict={\n",
        "                            x: batch_input, \n",
        "                            y: batch_labels})\n",
        "\n",
        "        # average over batches\n",
        "        if mode == 'funonly':\n",
        "            self.sess.run(self.add_reg_avg_loss)\n",
        "        elif mode == 'fungrad':\n",
        "            self.sess.run([self.add_reg_avg_loss, self.add_reg_avg_grad])\n",
        "        else:\n",
        "            self.sess.run(self.add_reg_avg_Gv, \n",
        "                feed_dict={self._lambda: self.config._lambda})\n",
        "\n",
        "\n",
        "    def _update_model(self):\n",
        "        update_model_ops = []\n",
        "        x = self.inverse_vectorize(self.s, self.param)\n",
        "        for i, p in enumerate(self.param):\n",
        "            op = tf.compat.v1.assign(p, p + (self.alpha-self.old_alpha) * x[i])\n",
        "            update_model_ops.append(op)\n",
        "        return tf.group(*update_model_ops)\n",
        "\n",
        "    def _init_cg_vars(self):\n",
        "        init_ops = []\n",
        "\n",
        "        init_r = tf.compat.v1.assign(self.r, -self.g)\n",
        "        init_v = tf.compat.v1.assign(self.v, -self.g)\n",
        "        init_s = tf.compat.v1.assign(self.s, tf.zeros_like(self.g))\n",
        "        gnorm = self.calc_norm(self.g)\n",
        "        init_rTr = tf.compat.v1.assign(self.rTr, gnorm**2)\n",
        "        init_cgtol = tf.compat.v1.assign(self.cgtol, self.config.xi*gnorm)\n",
        "\n",
        "        init_ops = [init_r, init_v, init_s, init_rTr, init_cgtol]\n",
        "\n",
        "        return tf.group(*init_ops)\n",
        "\n",
        "    def _CG(self):\n",
        "        \"\"\"\n",
        "        CG:\n",
        "            define operations that will be used in method newton\n",
        "        Same as the previous loss calculation,\n",
        "        Gv has been summed over batches when samples were fed into Neural Network.\n",
        "        \"\"\"\n",
        "\n",
        "        def CG_ops():\n",
        "            \n",
        "            vGv = tf.tensordot(self.v, self.Gv, axes=1)\n",
        "\n",
        "            alpha = self.rTr / vGv\n",
        "            with tf.control_dependencies([alpha]):\n",
        "                update_s = tf.compat.v1.assign(self.s, self.s + alpha * self.v, name='update_s_ops')\n",
        "                update_r = tf.compat.v1.assign(self.r, self.r - alpha * self.Gv, name='update_r_ops')\n",
        "\n",
        "                with tf.control_dependencies([update_s, update_r]):\n",
        "                    rnewTrnew = self.calc_norm(update_r)**2\n",
        "                    update_beta = tf.compat.v1.assign(self.beta, rnewTrnew / self.rTr)\n",
        "                    with tf.control_dependencies([update_beta]):\n",
        "                        update_rTr = tf.compat.v1.assign(self.rTr, rnewTrnew, name='update_rTr_ops')\n",
        "\n",
        "            return tf.group(*[update_s, update_beta, update_rTr])\n",
        "\n",
        "        def update_v():\n",
        "            return tf.compat.v1.assign(self.v, self.r + self.beta*self.v, name='update_v')\n",
        "\n",
        "        return (CG_ops(), update_v())\n",
        "\n",
        "\n",
        "    def newton(self, full_batch, val_batch, saver, network, test_network=None):\n",
        "        \"\"\"\n",
        "        Conduct newton steps for training\n",
        "        args:\n",
        "            full_batch & val_batch: provide training set and validation set. The function will\n",
        "                save the best model evaluted on validation set for future prediction.\n",
        "            network: a tuple contains (x, y, loss, outputs).\n",
        "            test_network: a tuple similar to argument network. If you use layers which behave differently\n",
        "                in test phase such as batchnorm, a separate test_network is needed.\n",
        "        return:\n",
        "            None\n",
        "        \"\"\"\n",
        "        # check whether data is valid\n",
        "        full_inputs, full_labels = full_batch\n",
        "        print(full_inputs.shape[0],full_labels.shape[0])\n",
        "        assert full_inputs.shape[0] == full_labels.shape[0]\n",
        "        if full_inputs.shape[0] != self.config.num_data:\n",
        "            raise ValueError('The number of full batch inputs does not agree with the config argument.\\\n",
        "                            This is important because global loss is averaged over those inputs')\n",
        "\n",
        "        x, y, _, outputs = network\n",
        "\n",
        "        tf.compat.v1.summary.scalar('loss', self.f)\n",
        "        merged = tf.compat.v1.summary.merge_all()\n",
        "        train_writer = tf.compat.v1.summary.FileWriter('./summary/train', self.sess.graph)\n",
        "\n",
        "        print(self.config.args)\n",
        "        if not self.config.screen_log_only:\n",
        "            log_file = open(self.config.log_file, 'w')\n",
        "            print(self.config.args, file=log_file)\n",
        "        \n",
        "        self.minibatch(full_batch, x, y, mode='fungrad')\n",
        "        f = self.sess.run(self.f)\n",
        "        output_str = 'initial f: {:.3f}'.format(f)\n",
        "        print(output_str)\n",
        "        if not self.config.screen_log_only:\n",
        "            print(output_str, file=log_file)\n",
        "        \n",
        "        best_acc = 0.0\n",
        "\n",
        "        total_running_time = 0.0\n",
        "        self.config.elapsed_time = 0.0\n",
        "        total_CG = 0\n",
        "        \n",
        "        for k in range(self.config.iter_max):\n",
        "\n",
        "            # randomly select the batch for Gv estimation\n",
        "            idx = np.random.choice(np.arange(0, full_labels.shape[0]),\n",
        "                    size=self.config.GNsize, replace=False)\n",
        "\n",
        "            mini_inputs = full_inputs[idx]\n",
        "            mini_labels = full_labels[idx]\n",
        "\n",
        "            start = time.time()\n",
        "\n",
        "            self.sess.run(self.init_cg_vars)\n",
        "            cgtol = self.sess.run(self.cgtol)\n",
        "\n",
        "            avg_cg_time = 0.0\n",
        "            for CGiter in range(1, self.config.CGmax+1):\n",
        "                \n",
        "                cg_time = time.time()\n",
        "                self.minibatch((mini_inputs, mini_labels), x, y, mode='Gv')\n",
        "                avg_cg_time += time.time() - cg_time\n",
        "                \n",
        "                self.sess.run(self.CG)\n",
        "\n",
        "                rnewTrnew = self.sess.run(self.rTr)\n",
        "                \n",
        "                if rnewTrnew**0.5 <= cgtol or CGiter == self.config.CGmax:\n",
        "                    break\n",
        "\n",
        "                self.sess.run(self.update_v)\n",
        "\n",
        "            print('Avg time per Gv iteration: {:.5f} s\\r\\n'.format(avg_cg_time/CGiter))\n",
        "\n",
        "            gs, sGs = self.sess.run([self.update_gs, self.update_sGs], feed_dict={\n",
        "                    self._lambda: self.config._lambda\n",
        "                })\n",
        "            \n",
        "            # line_search\n",
        "            f_old = f\n",
        "            alpha = 1\n",
        "            while True:\n",
        "\n",
        "                old_alpha = 0 if alpha == 1 else alpha/0.5\n",
        "                \n",
        "                self.sess.run(self.update_model, feed_dict={\n",
        "                    self.alpha:alpha, self.old_alpha:old_alpha\n",
        "                    })\n",
        "\n",
        "                prered = alpha*gs + (alpha**2)*sGs\n",
        "\n",
        "                self.minibatch(full_batch, x, y, mode='funonly')\n",
        "                f = self.sess.run(self.f)\n",
        "\n",
        "                actred = f - f_old\n",
        "\n",
        "                if actred <= self.config.eta*alpha*gs:\n",
        "                    break\n",
        "\n",
        "                alpha *= 0.5\n",
        "\n",
        "            # update lambda\n",
        "            ratio = actred / prered\n",
        "            if ratio < 0.25:\n",
        "                self.config._lambda *= self.config.boost\n",
        "            elif ratio >= 0.75:\n",
        "                self.config._lambda *= self.config.drop\n",
        "\n",
        "            self.minibatch(full_batch, x, y, mode='fungrad')\n",
        "            f = self.sess.run(self.f)\n",
        "\n",
        "            gnorm = self.sess.run(self.gnorm)\n",
        "\n",
        "            summary = self.sess.run(merged)\n",
        "            train_writer.add_summary(summary, k)\n",
        "\n",
        "            # exclude data loading time for fair comparison\n",
        "            end = time.time() \n",
        "            \n",
        "            end = end - self.config.elapsed_time\n",
        "            total_running_time += end-start\n",
        "\n",
        "            self.config.elapsed_time = 0.0\n",
        "            \n",
        "            total_CG += CGiter\n",
        "\n",
        "            output_str = '{}-iter f: {:.3f} |g|: {:.5f} alpha: {:.3e} ratio: {:.3f} lambda: {:.5f} #CG: {} actred: {:.5f} prered: {:.5f} time: {:.3f}'.\\\n",
        "                            format(k, f, gnorm, alpha, actred/prered, self.config._lambda, CGiter, actred, prered, end-start)\n",
        "            print(output_str)\n",
        "            if not self.config.screen_log_only:\n",
        "                print(output_str, file=log_file)\n",
        "\n",
        "            if val_batch is not None:\n",
        "                # Evaluate the performance after every Newton Step\n",
        "                if test_network == None:\n",
        "                    val_loss, val_acc, _ = predict(\n",
        "                        self.sess, \n",
        "                        network=(x, y, self.loss, outputs),\n",
        "                        test_batch=val_batch,\n",
        "                        bsize=self.config.bsize,\n",
        "                        )\n",
        "                else:\n",
        "                    # A separat test network part has not been done...\n",
        "                    val_loss, val_acc, _ = predict(\n",
        "                        self.sess, \n",
        "                        network=test_network,\n",
        "                        test_batch=val_batch,\n",
        "                        bsize=self.config.bsize\n",
        "                        )\n",
        "\n",
        "                output_str = '\\r\\n {}-iter val_acc: {:.3f}% val_loss {:.3f}\\r\\n'.\\\n",
        "                    format(k, val_acc*100, val_loss)\n",
        "                print(output_str)\n",
        "                if not self.config.screen_log_only:\n",
        "                    print(output_str, file=log_file)\n",
        "\n",
        "                if val_acc > best_acc:\n",
        "                    best_acc = val_acc\n",
        "                    checkpoint_path = self.config.model_file\n",
        "                    save_path = saver.save(self.sess, checkpoint_path)\n",
        "                    print('Best model saved in {}\\r\\n'.format(save_path))\n",
        "\n",
        "        if val_batch is None:\n",
        "            checkpoint_path = self.config.model_file\n",
        "            save_path = saver.save(self.sess, checkpoint_path)\n",
        "            print('Model at the last iteration saved in {}\\r\\n'.format(save_path))\n",
        "            output_str = 'total_#CG {} | total running time {:.3f}s'.format(total_CG, total_running_time)\n",
        "        else:\n",
        "            output_str = 'Final acc: {:.3f}% | best acc {:.3f}% | total_#CG {} | total running time {:.3f}s'.\\\n",
        "                format(val_acc*100, best_acc*100, total_CG, total_running_time)\n",
        "        print(output_str)\n",
        "        if not self.config.screen_log_only:\n",
        "            print(output_str, file=log_file)\n",
        "            log_file.close()"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xatdN2Zm7BOQ"
      },
      "source": [
        "##Set Train Arguments##"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v_DZPFd4m7Og"
      },
      "source": [
        "if USE_HFO:\n",
        "    # Arguments for HFO - PSSP dataset\n",
        "    train_args = (\"--optim NewtonCG --GNsize 512 --C 0.01 --net CNN_4layers --bsize 128 --iter_max 100 \" +\n",
        "              \"--train_set \" + TRAIN_FILE + \" --val_set \" + VALID_FILE + \" --dim \" + \n",
        "              str(HEIGHT) + \" \" + str(WIDTH) + \" \" + str(DEPTH)).split()\n",
        "else:\n",
        "    # Arguments for SGD - PSSP dataset\n",
        "    train_args = (\"--optim SGD --lr 0.05 --momentum 0.01 --C 0.01 --net CNN_4layers --bsize 128 --epoch_max 1000 \" +\n",
        "              \"--train_set \" + TRAIN_FILE + \" --val_set \" + VALID_FILE + \" --dim \" +\n",
        "              str(HEIGHT) + \" \" + str(WIDTH) + \" \" + str(DEPTH)).split()"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vMCxn-8P5tsH"
      },
      "source": [
        "##Declare Train Function##"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cr528VD1EDj9"
      },
      "source": [
        "# import pdb\n",
        "# import numpy as np\n",
        "# import tensorflow as tf\n",
        "# tf.compat.v1.disable_eager_execution()\n",
        "# import time\n",
        "# import math\n",
        "# import argparse\n",
        "\n",
        "# from net.net import CNN\n",
        "# from newton_cg import newton_cg\n",
        "# from utilities import read_data, predict, ConfigClass, normalize_and_reshape\n",
        "\n",
        "def parse_args():\n",
        "\tparser = argparse.ArgumentParser(description='Newton method on DNN')\n",
        "\tparser.add_argument('--C', dest='C',\n",
        "\t\t\t\t\t  help='regularization term, or so-called weight decay where'+\\\n",
        "\t\t\t\t\t  \t\t'weight_decay = lr/(C*num_of_samples) in this implementation' ,\n",
        "\t\t\t\t\t  default=0.01, type=float)\n",
        "\n",
        "\t# Newton method arguments\n",
        "\tparser.add_argument('--GNsize', dest='GNsize',\n",
        "\t\t\t\t\t  help='number of samples for estimating Gauss-Newton matrix',\n",
        "\t\t\t\t\t  default=4096, type=int)\n",
        "\tparser.add_argument('--iter_max', dest='iter_max',\n",
        "\t\t\t\t\t  help='the maximal number of Newton iterations',\n",
        "\t\t\t\t\t  default=100, type=int)\n",
        "\tparser.add_argument('--xi', dest='xi',\n",
        "\t\t\t\t\t  help='the tolerance in the relative stopping condition for CG',\n",
        "\t\t\t\t\t  default=0.1, type=float)\n",
        "\tparser.add_argument('--drop', dest='drop',\n",
        "\t\t\t\t\t  help='the drop constants for the LM method',\n",
        "\t\t\t\t\t  default=2/3, type=float)\n",
        "\tparser.add_argument('--boost', dest='boost',\n",
        "\t\t\t\t\t  help='the boost constants for the LM method',\n",
        "\t\t\t\t\t  default=3/2, type=float)\n",
        "\tparser.add_argument('--eta', dest='eta',\n",
        "\t\t\t\t\t  help='the parameter for the line search stopping condition',\n",
        "\t\t\t\t\t  default=0.0001, type=float)\n",
        "\tparser.add_argument('--CGmax', dest='CGmax',\n",
        "\t\t\t\t\t  help='the maximal number of CG iterations',\n",
        "\t\t\t\t\t  default=250, type=int)\n",
        "\tparser.add_argument('--lambda', dest='_lambda',\n",
        "\t\t\t\t\t  help='the initial lambda for the LM method',\n",
        "\t\t\t\t\t  default=1, type=float)\n",
        "\n",
        "\t# SGD arguments\n",
        "\tparser.add_argument('--epoch_max', dest='epoch',\n",
        "\t\t\t\t\t  help='number of training epoch',\n",
        "\t\t\t\t\t  default=500, type=int)\n",
        "\tparser.add_argument('--lr', dest='lr',\n",
        "\t\t\t\t\t  help='learning rate',\n",
        "\t\t\t\t\t  default=0.01, type=float)\n",
        "\tparser.add_argument('--decay', dest='lr_decay',\n",
        "\t\t\t\t\t  help='learning rate decay over each mini-batch update',\n",
        "\t\t\t\t\t  default=0, type=float)\n",
        "\tparser.add_argument('--momentum', dest='momentum',\n",
        "\t\t\t\t\t  help='momentum of learning',\n",
        "\t\t\t\t\t  default=0, type=float)\n",
        "\n",
        "\t# Model training arguments\n",
        "\tparser.add_argument('--bsize', dest='bsize',\n",
        "\t\t\t\t\t  help='batch size to evaluate stochastic gradient, Gv, etc. Since the sampled data \\\n",
        "\t\t\t\t\t  for computing Gauss-Newton matrix and etc. might not fit into memeory \\\n",
        "\t\t\t\t\t  for one time, we will split the data into several segements and average\\\n",
        "\t\t\t\t\t  over them.',\n",
        "\t\t\t\t\t  default=1024, type=int)\n",
        "\tparser.add_argument('--net', dest='net',\n",
        "\t\t\t\t\t  help='classifier type',\n",
        "\t\t\t\t\t  default='CNN_4layers', type=str)\n",
        "\tparser.add_argument('--train_set', dest='train_set',\n",
        "\t\t\t\t\t  help='provide the directory of .mat file for training',\n",
        "\t\t\t\t\t  default=None, type=str)\n",
        "\tparser.add_argument('--val_set', dest='val_set',\n",
        "\t\t\t\t\t  help='provide the directory of .mat file for validation',\n",
        "\t\t\t\t\t  default=None, type=str)\n",
        "\tparser.add_argument('--model', dest='model_file',\n",
        "\t\t\t\t\t  help='model saving address',\n",
        "\t\t\t\t\t  default='./saved_model/model.ckpt', type=str)\n",
        "\tparser.add_argument('--log', dest='log_file',\n",
        "\t\t\t\t\t  help='log saving directory',\n",
        "\t\t\t\t\t  default='./running_log/logger.log', type=str)\n",
        "\tparser.add_argument('--screen_log_only', dest='screen_log_only',\n",
        "\t\t\t\t\t  help='screen printing running log instead of storing it',\n",
        "\t\t\t\t\t  action='store_true')\n",
        "\tparser.add_argument('--optim', '-optim', \n",
        "\t\t\t\t\t  help='which optimizer to use: SGD, Adam or NewtonCG',\n",
        "\t\t\t\t\t  default='NewtonCG', type=str)\n",
        "\tparser.add_argument('--loss', dest='loss', \n",
        "\t\t\t\t\t  help='which loss function to use: MSELoss or CrossEntropy',\n",
        "\t\t\t\t\t  default='MSELoss', type=str)\n",
        "\tparser.add_argument('--dim', dest='dim', nargs='+', help='input dimension of data,'+\\\n",
        "\t\t\t\t\t\t'shape must be:  height width num_channels',\n",
        "\t\t\t\t\t  default=[32, 32, 3], type=int)\n",
        "\tparser.add_argument('--seed', dest='seed', help='a nonnegative integer for \\\n",
        "\t\t\t\t\t\treproducibility', type=int)\t \n",
        "\t\n",
        "\targs = parser.parse_args(args=train_args)\n",
        "\treturn args\n",
        "\n",
        "\n",
        "args = parse_args()\n",
        "\n",
        "def init_model(param):\n",
        "\tinit_ops = []\n",
        "\tfor p in param:\n",
        "\t\tif 'kernel' in p.name:\n",
        "\t\t\tweight = np.random.standard_normal(p.shape)* np.sqrt(2.0 / ((np.prod(p.get_shape().as_list()[:-1]))))\n",
        "\t\t\topt = tf.compat.v1.assign(p, weight)\n",
        "\t\telif 'bias' in p.name:\n",
        "\t\t\tzeros = np.zeros(p.shape)\n",
        "\t\t\topt = tf.compat.v1.assign(p, zeros)\n",
        "\t\tinit_ops.append(opt)\n",
        "\treturn tf.group(*init_ops)\n",
        "\n",
        "def gradient_trainer(config, sess, network, full_batch, val_batch, saver, test_network):\n",
        "\tx, y, loss, outputs,  = network\n",
        "\t\n",
        "\tglobal_step = tf.Variable(initial_value=0, trainable=False, name='global_step')\n",
        "\tlearning_rate = tf.compat.v1.placeholder(tf.float32, shape=[], name='learning_rate')\n",
        "\n",
        "\t# Probably not a good way to add regularization.\n",
        "\t# Just to confirm the implementation is the same as MATLAB.\n",
        "\treg = 0.0\n",
        "\tparam = tf.compat.v1.trainable_variables()\n",
        "\tfor p in param:\n",
        "\t\treg = reg + tf.reduce_sum(input_tensor=tf.pow(p,2))\n",
        "\treg_const = 1/(2*config.C)\n",
        "\tbatch_size = tf.compat.v1.cast(tf.shape(x)[0], tf.float32)\n",
        "\tloss_with_reg = reg_const*reg + loss/batch_size\n",
        "\n",
        "\tif config.optim == 'SGD':\n",
        "\t\toptimizer = tf.compat.v1.train.MomentumOptimizer(\n",
        "\t\t\t\t\tlearning_rate=learning_rate, \n",
        "\t\t\t\t\tmomentum=config.momentum).minimize(\n",
        "\t\t\t\t\tloss_with_reg, \n",
        "\t\t\t\t\tglobal_step=global_step)\n",
        "\telif config.optim == 'Adam':\n",
        "\t\toptimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate,\n",
        "\t\t\t\t\t\t\t\tbeta1=0.9,\n",
        "\t\t\t\t\t\t\t\tbeta2=0.999,\n",
        "\t\t\t\t\t\t\t\tepsilon=1e-08).minimize(\n",
        "\t\t\t\t\t\t\t\tloss_with_reg, \n",
        "\t\t\t\t\t\t\t\tglobal_step=global_step)\n",
        "\n",
        "\ttrain_inputs, train_labels = full_batch\n",
        "\tnum_data = train_labels.shape[0]\n",
        "\tnum_iters = math.ceil(num_data/config.bsize)\n",
        "\n",
        "\tprint(config.args)\n",
        "\tif not config.screen_log_only:\n",
        "\t\tlog_file = open(config.log_file, 'w')\n",
        "\t\tprint(config.args, file=log_file)\n",
        "\tsess.run(tf.compat.v1.global_variables_initializer())\n",
        "\t\n",
        "\n",
        "\tprint('-------------- initializing network by methods in He et al. (2015) --------------')\n",
        "\tparam = tf.compat.v1.trainable_variables()\n",
        "\tsess.run(init_model(param))\n",
        "\n",
        "\ttotal_running_time = 0.0\n",
        "\tbest_acc = 0.0\n",
        "\tlr = config.lr\n",
        "\n",
        "\tfor epoch in range(0, args.epoch):\n",
        "\t\t\n",
        "\t\tloss_avg = 0.0\n",
        "\t\tstart = time.time()\n",
        "\n",
        "\t\tfor i in range(num_iters):\n",
        "\t\t\t\n",
        "\t\t\tload_time = time.time()\n",
        "\t\t\t# randomly select the batch\n",
        "\t\t\tidx = np.random.choice(np.arange(0, num_data), \n",
        "\t\t\t\t\tsize=config.bsize, replace=False)\n",
        "\n",
        "\t\t\tbatch_input = train_inputs[idx]\n",
        "\t\t\tbatch_labels = train_labels[idx]\n",
        "\t\t\tbatch_input = np.ascontiguousarray(batch_input)\n",
        "\t\t\tbatch_labels = np.ascontiguousarray(batch_labels)\n",
        "\t\t\tconfig.elapsed_time += time.time() - load_time\n",
        "\n",
        "\t\t\tstep, _, batch_loss= sess.run(\n",
        "\t\t\t\t[global_step, optimizer, loss_with_reg],\n",
        "\t\t\t\tfeed_dict = {x: batch_input, y: batch_labels, learning_rate: lr}\n",
        "\t\t\t\t)\n",
        "\n",
        "\t\t\t# print initial loss\n",
        "\t\t\tif epoch == 0 and i == 0:\n",
        "\t\t\t\toutput_str = 'initial f (reg + avg. loss of 1st batch): {:.3f}'.format(batch_loss)\n",
        "\t\t\t\tprint(output_str)\n",
        "\t\t\t\tif not config.screen_log_only:\n",
        "\t\t\t\t\tprint(output_str, file=log_file)\n",
        "\n",
        "\t\t\tloss_avg = loss_avg + batch_loss\n",
        "\t\t\t# print log every 10% of the iterations\n",
        "\t\t\tif i % math.ceil(num_iters/10) == 0:\n",
        "\t\t\t\tend = time.time()\n",
        "\t\t\t\toutput_str = 'Epoch {}: {}/{} | loss {:.4f} | lr {:.6} | elapsed time {:.3f}'\\\n",
        "\t\t\t\t\t.format(epoch, i, num_iters, batch_loss , lr, end-start)\n",
        "\t\t\t\tprint(output_str)\n",
        "\t\t\t\tif not config.screen_log_only:\n",
        "\t\t\t\t\tprint(output_str, file=log_file)\n",
        "\t\t\t\n",
        "\t\t\t# adjust learning rate for SGD by inverse time decay\n",
        "\t\t\tif args.optim != 'Adam':\n",
        "\t\t\t\tlr = config.lr/(1 + args.lr_decay*step)\n",
        "\n",
        "\t\t# exclude data loading time for fair comparison\n",
        "\t\tepoch_end = time.time() - config.elapsed_time\n",
        "\t\ttotal_running_time += epoch_end - start\n",
        "\t\tconfig.elapsed_time = 0.0\n",
        "\t\t\n",
        "\t\tif val_batch is None:\n",
        "\t\t\toutput_str = 'In epoch {} train loss: {:.3f} | epoch time {:.3f}'\\\n",
        "\t\t\t\t.format(epoch, loss_avg/(i+1), epoch_end-start)\t\t\t\n",
        "\t\telse:\n",
        "\t\t\tif test_network == None:\n",
        "\t\t\t\tval_loss, val_acc, _ = predict(\n",
        "\t\t\t\t\tsess, \n",
        "\t\t\t\t\tnetwork=(x, y, loss, outputs),\n",
        "\t\t\t\t\ttest_batch=val_batch,\n",
        "\t\t\t\t\tbsize=config.bsize\n",
        "\t\t\t\t\t)\n",
        "\t\t\telse:\n",
        "\t\t\t\t# A separat test network part have been done...\n",
        "\t\t\t\tval_loss, val_acc, _ = predict(\n",
        "\t\t\t\t\tsess, \n",
        "\t\t\t\t\tnetwork=test_network,\n",
        "\t\t\t\t\ttest_batch=val_batch,\n",
        "\t\t\t\t\tbsize=config.bsize\n",
        "\t\t\t\t\t)\n",
        "\t\t\t\n",
        "\t\t\toutput_str = 'In epoch {} train loss: {:.3f} | val loss: {:.3f} | val accuracy: {:.3f}% | epoch time {:.3f}'\\\n",
        "\t\t\t\t.format(epoch, loss_avg/(i+1), val_loss, val_acc*100, epoch_end-start)\n",
        "\t\t\n",
        "\t\t\tif val_acc > best_acc:\n",
        "\t\t\t\tbest_acc = val_acc\n",
        "\t\t\t\tcheckpoint_path = config.model_file \n",
        "\t\t\t\tsave_path = saver.save(sess, checkpoint_path)\n",
        "\t\t\t\tprint('Saved best model in {}'.format(save_path))\n",
        "\n",
        "\t\tprint(output_str)\n",
        "\t\tif not config.screen_log_only:\n",
        "\t\t\tprint(output_str, file=log_file)\n",
        "\n",
        "\tif val_batch is None:\n",
        "\t\tcheckpoint_path = config.model_file \n",
        "\t\tsave_path = saver.save(sess, checkpoint_path)\n",
        "\t\tprint('Model at the last iteration saved in {}\\r\\n'.format(save_path))\n",
        "\t\toutput_str = 'total running time {:.3f}s'.format(total_running_time)\n",
        "\telse:\n",
        "\t\toutput_str = 'Final acc: {:.3f}% | best acc {:.3f}% | total running time {:.3f}s'\\\n",
        "\t\t\t.format(val_acc*100, best_acc*100, total_running_time)\n",
        "\t\n",
        "\tprint(output_str)\n",
        "\tif not config.screen_log_only:\n",
        "\t\tprint(output_str, file=log_file)\n",
        "\t\tlog_file.close()\n",
        "\n",
        "def newton_trainer(config, sess, network, full_batch, val_batch, saver, test_network):\n",
        "\n",
        "\t_, _, loss, outputs = network\n",
        "\tnewton_solver = newton_cg(config, sess, outputs, loss)\n",
        "\tsess.run(tf.compat.v1.global_variables_initializer())\n",
        "\n",
        "\tprint('-------------- initializing network by methods in He et al. (2015) --------------')\n",
        "\tparam = tf.compat.v1.trainable_variables()\n",
        "\tsess.run(init_model(param))\n",
        "\tnewton_solver.newton(full_batch, val_batch, saver, network, test_network)\n",
        "\n",
        "\n",
        "def train_model():\n",
        "\tfull_batch, num_cls, label_enum = read_data(filename=args.train_set, dim=args.dim)\n",
        "\tif args.val_set is None:\n",
        "\t\tprint('No validation set is provided. Will output model at the last iteration.')\n",
        "\t\tval_batch = None\n",
        "\telse:\n",
        "    #print(\"Train model inside else\")\n",
        "\t\tval_batch, _, _ = read_data(filename=args.val_set, dim=args.dim, label_enum=label_enum)\n",
        "\n",
        "\tnum_data = full_batch[0].shape[0]\n",
        "\t\n",
        "\tconfig = ConfigClass(args, num_data, num_cls)\n",
        "\n",
        "\tif isinstance(config.seed, int):\n",
        "\t\ttf.compat.v1.random.set_random_seed(config.seed)\n",
        "\t\tnp.random.seed(config.seed)\n",
        "\n",
        "\tif config.net in ('CNN_4layers', 'CNN_7layers', 'VGG11', 'VGG13', 'VGG16','VGG19'):\n",
        "\t\tx, y, outputs = CNN(config.net, num_cls, config.dim)\n",
        "\t\ttest_network = None\n",
        "\telse:\n",
        "\t\traise ValueError('Unrecognized training model')\n",
        "\n",
        "\tif config.loss == 'MSELoss':\n",
        "\t\tloss = tf.reduce_sum(input_tensor=tf.pow(outputs-y, 2))\n",
        "\telse:\n",
        "\t\tloss = tf.reduce_sum(input_tensor=tf.nn.softmax_cross_entropy_with_logits(logits=outputs, labels=y))\n",
        "\t\n",
        "\tnetwork = (x, y, loss, outputs)\n",
        "\n",
        "\tsess_config = tf.compat.v1.ConfigProto()\n",
        "\tsess_config.gpu_options.allow_growth = True\n",
        "\n",
        "\twith tf.compat.v1.Session(config=sess_config) as sess:\n",
        "\t\t\n",
        "\t\tfull_batch[0], mean_tr = normalize_and_reshape(full_batch[0], dim=config.dim, mean_tr=None)\n",
        "\t\tif val_batch is not None:\n",
        "\t\t\tval_batch[0], _ = normalize_and_reshape(val_batch[0], dim=config.dim, mean_tr=mean_tr)\n",
        "\n",
        "\t\tparam = tf.compat.v1.trainable_variables()\n",
        "\n",
        "\t\tmean_param = tf.compat.v1.get_variable(name='mean_tr', initializer=mean_tr, trainable=False, \n",
        "\t\t\t\t\tvalidate_shape=True, use_resource=False)\n",
        "\t\tlabel_enum_var=tf.compat.v1.get_variable(name='label_enum', initializer=label_enum, trainable=False,\n",
        "\t\t\t\t\tvalidate_shape=True, use_resource=False)\n",
        "\t\tsaver = tf.compat.v1.train.Saver(var_list=param+[mean_param])\n",
        "\t\t\n",
        "\t\tif config.optim in ('SGD', 'Adam'):\n",
        "\t\t\tgradient_trainer(\n",
        "\t\t\t\tconfig, sess, network, full_batch, val_batch, saver, test_network)\n",
        "\t\telif config.optim == 'NewtonCG':\n",
        "\t\t\tnewton_trainer(\n",
        "\t\t\t\tconfig, sess, network, full_batch, val_batch, saver, test_network=test_network)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wi6UglYiD7Zd"
      },
      "source": [
        "## Train ##"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cSgKfh4r5lu7",
        "outputId": "ef94acbf-207d-41ed-a7d0-8adee38af586",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 980
        }
      },
      "source": [
        "train_model()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Images size after reshape =  (461, 1574912)\n",
            "Single image dimension = [1538, 1024, 1]\n",
            "(461, 0)\n",
            "Shape 0  461\n",
            "Labels shape  (461, 6152)\n",
            "[[0. 1. 0. ... 0. 0. 0.]\n",
            " [0. 1. 0. ... 0. 0. 0.]\n",
            " [0. 1. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 1. 0. ... 0. 0. 0.]\n",
            " [0. 1. 0. ... 0. 0. 0.]\n",
            " [0. 1. 0. ... 0. 0. 0.]]\n",
            "Images size after reshape =  (52, 1574912)\n",
            "Single image dimension = [1538, 1024, 1]\n",
            "(52, 0)\n",
            "Shape 0  52\n",
            "Labels shape  (52, 6152)\n",
            "[[0. 1. 0. ... 0. 0. 0.]\n",
            " [0. 1. 0. ... 0. 0. 0.]\n",
            " [0. 1. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 1. 0. ... 0. 0. 0.]\n",
            " [0. 1. 0. ... 0. 0. 0.]\n",
            " [0. 1. 0. ... 0. 0. 0.]]\n",
            "You choose not to specify a random seed.A different result is produced after each run.\n",
            "Saving log to: ./running_log/logger.log\n",
            "No mean of data provided! Normalize images by their own mean.\n",
            "Normalize images according to the provided mean.\n",
            "-------------- initializing network by methods in He et al. (2015) --------------\n",
            "461 461\n",
            "Namespace(C=0.01, CGmax=250, GNsize=512, _lambda=1, boost=1.5, bsize=128, dim=[1538, 1024, 1], drop=0.6666666666666666, epoch=500, eta=0.0001, iter_max=100, log_file='./running_log/logger.log', loss='MSELoss', lr=0.01, lr_decay=0, model_file='./saved_model/model.ckpt', momentum=0, net='CNN_4layers', optim='NewtonCG', screen_log_only=False, seed=None, train_set='/content/drive/MyDrive/Datasets/cb513_protbert_trainSet0.mat', val_set='/content/drive/MyDrive/Datasets/cb513_protbert_testSet0.mat', xi=0.1)\n",
            "NUM data =: 461\n",
            "COnfig num data  461\n",
            "NUm segment  4\n",
            "Num grad segment  4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-4dc2ba0c028a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-13-5340dcd3e468>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m    316\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'NewtonCG'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m \t\t\tnewton_trainer(\n\u001b[0;32m--> 318\u001b[0;31m \t\t\t\tconfig, sess, network, full_batch, val_batch, saver, test_network=test_network)\n\u001b[0m",
            "\u001b[0;32m<ipython-input-13-5340dcd3e468>\u001b[0m in \u001b[0;36mnewton_trainer\u001b[0;34m(config, sess, network, full_batch, val_batch, saver, test_network)\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0mparam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m         \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m         \u001b[0mnewton_solver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_network\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-ce3f9e5ebbd7>\u001b[0m in \u001b[0;36mnewton\u001b[0;34m(self, full_batch, val_batch, saver, network, test_network)\u001b[0m\n\u001b[1;32m    347\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlog_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 349\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminibatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'fungrad'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    350\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m         \u001b[0moutput_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'initial f: {:.3f}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-ce3f9e5ebbd7>\u001b[0m in \u001b[0;36mminibatch\u001b[0;34m(self, data_batch, place_holder_x, place_holder_y, mode)\u001b[0m\n\u001b[1;32m    246\u001b[0m                 self.sess.run(self.cal_lossgrad, feed_dict={\n\u001b[1;32m    247\u001b[0m                             \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 248\u001b[0;31m                             y: batch_labels,})\n\u001b[0m\u001b[1;32m    249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    966\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 968\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    969\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    970\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1165\u001b[0m                 \u001b[0;34m'Cannot feed value of shape %r for Tensor %r, '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1166\u001b[0m                 \u001b[0;34m'which has shape %r'\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1167\u001b[0;31m                 (np_val.shape, subfeed_t.name, str(subfeed_t.get_shape())))\n\u001b[0m\u001b[1;32m   1168\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_feedable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1169\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Tensor %s may not be fed.'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0msubfeed_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Cannot feed value of shape (128, 6152) for Tensor 'main_params/labels:0', which has shape '(?, 4)'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UwH_nXJZDz2T"
      },
      "source": [
        "## Predict ##"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9zyzmNHFnOwD"
      },
      "source": [
        "# Arguments for prediction PSSP dataset\n",
        "pred_args = (\"--bsize 1024 --valid_set ./\" + VALID_FILE + \" --train_set ./\" + TRAIN_FILE + \n",
        "\t\t\t\t\t\t \" --model ./saved_model/model.ckpt --dim \" +\n",
        "             str(HEIGHT) + \" \" + str(WIDTH) + \" \" + str(DEPTH)).split()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s6MctxH5_nTR"
      },
      "source": [
        "valid_origin =  \"{0}_test_fold{1}.txt\".format(dataset.lower(),str(fold)) # train set  \n",
        "train_origin = \"{0}_train_fold{1}.txt\".format(dataset.lower(),str(fold)) # validation set\n",
        "test_origin = \"casp13_sorted.txt\" # test set CASP13\n",
        "print(valid_origin)\n",
        "print(train_origin)\n",
        "print(test_origin)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s0R1rY5rw030"
      },
      "source": [
        "import requests\n",
        "valid_f = requests.get(valid_origin)\n",
        "valid_f = valid_f.text.split('\\n')[0:-1]\n",
        "train_f = requests.get(train_origin)\n",
        "train_f = train_f.text.split('\\n')[0:-1]\n",
        "test_f = requests.get(test_origin)\n",
        "test_f = test_f.text.split('\\n')[0:-1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IeQqT96h_NHr"
      },
      "source": [
        "VALID_PRED_FILE=\"pred_test_fold{0}.txt\".format(fold)\n",
        "TRAIN_PRED_FILE=\"pred_train_fold{0}.txt\".format(fold)\n",
        "TEST_PRED_FILE=\"pred_casp13_fold{0}.txt\".format(fold)\n",
        "print(VALID_PRED_FILE)\n",
        "print(TRAIN_PRED_FILE)\n",
        "print(TEST_PRED_FILE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NNNnI_mU6LN5"
      },
      "source": [
        "##Declare Predict Methods##"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kYGPhdqk5wYp"
      },
      "source": [
        "def create_output_pred(pred, origin_f, outFileName):\n",
        "    pred = pred.astype(int)\n",
        "    labels = ['C', 'E', 'H']\n",
        "    counter = 0\n",
        "    with open(outFileName, 'w') as out_file:\n",
        "        for line in range(0, len(origin_f)//3):\n",
        "            protein_name = origin_f[line*3]\n",
        "            primary_structure = origin_f[line*3+1].replace('!', '')\n",
        "            secondary_structure = origin_f[line*3+2].replace('!', '')\n",
        "            prediction = \"\"\n",
        "            for c in secondary_structure:\n",
        "                if (c != '!'):\n",
        "                    prediction = prediction + labels[pred[counter]]\n",
        "                    counter += 1\n",
        "            out_file.write(protein_name + \"\\n\")\n",
        "            out_file.write(primary_structure + \"\\n\")\n",
        "            out_file.write(secondary_structure + \"\\n\")\n",
        "            out_file.write(prediction + \"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EA8Pq5a0C3m5"
      },
      "source": [
        "# import tensorflow as tf \n",
        "# tf.compat.v1.disable_eager_execution()\n",
        "# from utilities import predict, read_data, normalize_and_reshape\n",
        "# from net.net import CNN\n",
        "# import numpy as np \n",
        "# import argparse\n",
        "# import pdb\n",
        "\n",
        "def parse_args():\n",
        "\t\tparser = argparse.ArgumentParser(description='prediction')\n",
        "\t\tparser.add_argument('--test_set', dest='test_set',\n",
        "\t\t\t\t\t\t\thelp='provide the directory of .mat file for testing',\n",
        "\t\t\t\t\t\t\tdefault=None, type=str)\n",
        "\t\tparser.add_argument('--valid_set', dest='valid_set',\n",
        "\t\t\t\t\t\t\thelp='provide the directory of .mat file for validation',\n",
        "\t\t\t\t\t\t\tdefault=None, type=str)\n",
        "\t\tparser.add_argument('--train_set', dest='train_set',\n",
        "\t\t\t\t\t\t\thelp='provide the directory of .mat file for training',\n",
        "\t\t\t\t\t\t\tdefault=None, type=str)\n",
        "\t\tparser.add_argument('--model', dest='model_file',\n",
        "\t\t\t\t\t\t\thelp='provide file storing network parameters, i.e. ./dir/model.ckpt',\n",
        "\t\t\t\t\t\t\tdefault='./saved_model/model.ckpt', type=str)\n",
        "\t\tparser.add_argument('--bsize', dest='bsize',\n",
        "\t\t\t\t\t\t\thelp='batch size',\n",
        "\t\t\t\t\t\t\tdefault=1024, type=int)\n",
        "\t\tparser.add_argument('--loss', dest='loss', \n",
        "\t\t\t\t\t\t\thelp='which loss function to use: MSELoss or CrossEntropy',\n",
        "\t\t\t\t\t\t\tdefault='MSELoss', type=str)\n",
        "\t\tparser.add_argument('--dim', dest='dim', nargs='+', help='input dimension of data,'+\\\n",
        "\t\t\t\t\t\t\t'shape must be:  height width num_channels',\n",
        "\t\t\t\t\t\t\tdefault=[32, 32, 3], type=int)\n",
        "\n",
        "\t\targs = parser.parse_args(args=pred_args)\n",
        "\t\treturn args\n",
        "\n",
        "def predict_model():\n",
        "\t\targs = parse_args()\n",
        "\n",
        "\t\tsess_config = tf.compat.v1.ConfigProto()\n",
        "\t\tsess_config.gpu_options.allow_growth = True\n",
        "\n",
        "\t\twith tf.compat.v1.Session(config=sess_config) as sess:\n",
        "\t\t\t\tgraph_address = args.model_file + '.meta'\n",
        "\t\t\t\timported_graph = tf.compat.v1.train.import_meta_graph(graph_address)\n",
        "\t\t\t\timported_graph.restore(sess, args.model_file)\n",
        "\t\t\t\tmean_param = [v for v in tf.compat.v1.global_variables() if 'mean_tr:0' in v.name][0]\n",
        "\t\t\t\tlabel_enum_var = [v for v in tf.compat.v1.global_variables() if 'label_enum:0' in v.name][0]\n",
        "\n",
        "\t\t\t\tsess.run(tf.compat.v1.variables_initializer([mean_param, label_enum_var]))\n",
        "\t\t\t\tmean_tr = sess.run(mean_param)\n",
        "\t\t\t\tlabel_enum = sess.run(label_enum_var)\n",
        "\n",
        "\t\t\t\tx = tf.compat.v1.get_default_graph().get_tensor_by_name('main_params/input_of_net:0')\n",
        "\t\t\t\ty = tf.compat.v1.get_default_graph().get_tensor_by_name('main_params/labels:0')\n",
        "\t\t\t\toutputs = tf.compat.v1.get_default_graph().get_tensor_by_name('output_of_net:0')\n",
        "\n",
        "\t\t\t\tif args.loss == 'MSELoss':\n",
        "\t\t\t\t\t\tloss = tf.reduce_sum(input_tensor=tf.pow(outputs-y, 2))\n",
        "\t\t\t\telse:\n",
        "\t\t\t\t\t\tloss = tf.reduce_sum(input_tensor=\n",
        "\t\t\t\t\t\t    tf.nn.softmax_cross_entropy_with_logits(logits=outputs, labels=tf.stop_gradient(y)))\n",
        "\t\t\t\t\n",
        "\t\t\t\tnetwork = (x, y, loss, outputs)\n",
        "\n",
        "\t\t\t\tif args.valid_set is not None:\n",
        "\t\t\t\t\t\tvalid_batch, num_cls, _ = read_data(args.valid_set, dim=args.dim, label_enum=label_enum)\n",
        "\t\t\t\t\t\tvalid_batch[0], _ = normalize_and_reshape(valid_batch[0], dim=args.dim, mean_tr=mean_tr)\n",
        "\t\t\t\n",
        "\t\t\t\t\t\tavg_loss_valid, avg_acc_valid, results_valid = predict(sess, network, valid_batch, args.bsize)\n",
        "\n",
        "\t\t\t\t\t\t# convert results back to the original labels\n",
        "\t\t\t\t\t\tinverse_map = dict(zip(np.arange(num_cls), label_enum))\n",
        "\t\t\t\t\t\tresults_valid = np.expand_dims(results_valid, axis=1)\n",
        "\t\t\t\t\t\tresults_valid = np.apply_along_axis(lambda x: inverse_map[x[0]], axis=1, arr=results_valid)\n",
        "\n",
        "\t\t\t\t\t\tcreate_output_pred(results_valid, valid_f, VALID_PRED_FILE)\n",
        "\t\t\t\t\t\tprint('In valid phase, average loss: {:.3f} | average accuracy: {:.3f}%'.\\\n",
        "\t\t\t\t\t\t\tformat(avg_loss_valid, avg_acc_valid*100))\n",
        "\t\t\t\t\n",
        "\t\t\t\tif args.train_set is not None:\n",
        "\t\t\t\t\t\ttrain_batch, num_cls, _ = read_data(args.train_set, dim=args.dim, label_enum=label_enum)\n",
        "\t\t\t\t\t\ttrain_batch[0], _ = normalize_and_reshape(train_batch[0], dim=args.dim, mean_tr=mean_tr)\n",
        "\n",
        "\t\t\t\t\t\tavg_loss_train, avg_acc_train, results_train = predict(sess, network, train_batch, args.bsize)\n",
        "\t\t\t\t\t\t# convert results back to the original labels\n",
        "\t\t\t\t\t\tinverse_map = dict(zip(np.arange(num_cls), label_enum))\n",
        "\t\t\t\t\t\tresults_train = np.expand_dims(results_train, axis=1)\n",
        "\t\t\t\t\t\tresults_train = np.apply_along_axis(lambda x: inverse_map[x[0]], axis=1, arr=results_train)\n",
        "\n",
        "\t\t\t\t\t\t# create_output_pred(results, results_train)\n",
        "\n",
        "\t\t\t\t\t\tcreate_output_pred(results_train, train_f, TRAIN_PRED_FILE)\n",
        "\t\t\t\t\t\tprint('In train phase, average loss: {:.3f} | average accuracy: {:.3f}%'.\\\n",
        "\t\t\t\t\t\t\tformat(avg_loss_train, avg_acc_train*100))\n",
        "\n",
        "\t\t\t\tif args.test_set is not None:\n",
        "\t\t\t\t\t\ttest_batch, num_cls, _ = read_data(args.test_set, dim=args.dim, label_enum=label_enum)\n",
        "\t\t\t\t\t\ttest_batch[0], _ = normalize_and_reshape(test_batch[0], dim=args.dim, mean_tr=mean_tr)\n",
        "\n",
        "\t\t\t\t\t\tavg_loss_test, avg_acc_test, results_test = predict(sess, network, test_batch, args.bsize)\n",
        "\t\t\t\t\t\t# convert results back to the original labels\n",
        "\t\t\t\t\t\tinverse_map = dict(zip(np.arange(num_cls), label_enum))\n",
        "\t\t\t\t\t\tresults_test = np.expand_dims(results_test, axis=1)\n",
        "\t\t\t\t\t\tresults_test = np.apply_along_axis(lambda x: inverse_map[x[0]], axis=1, arr=results_test)\n",
        "\n",
        "\t\t\t\t\t\t# create_output_pred(results, results_train)\n",
        "\n",
        "\t\t\t\t\t\tcreate_output_pred(results_test, test_f, TEST_PRED_FILE)\n",
        "\t\t\t\t\t\tprint('In test phase, average loss: {:.3f} | average accuracy: {:.3f}%'.\\\n",
        "\t\t\t\t\t\t\tformat(avg_loss_test, avg_acc_test*100))\n",
        "\t\t\t"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aN8BTASh6eSL"
      },
      "source": [
        "##Run Predict and Display output##"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gCInY5uB6Y3G"
      },
      "source": [
        "predict_model()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "loR25EiOE4cQ"
      },
      "source": [
        "# !head \"$VALID_PRED_FILE\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NOnpS-N3DwhK"
      },
      "source": [
        "# !head \"$TRAIN_PRED_FILE\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exgdCTXb68Vl"
      },
      "source": [
        "## Check Test score on CASP13 ##"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lda3hrM4dmWk"
      },
      "source": [
        "# Arguments for prediction PSSP dataset\n",
        "pred_args = (\"--bsize 1024 --test_set ./\" + TEST_FILE + \n",
        "\t\t\t\t\t\t \" --model ./saved_model/model.ckpt --dim \" +\n",
        "             str(HEIGHT) + \" \" + str(WIDTH) + \" \" + str(DEPTH)).split()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3JCC5-2mk0rs"
      },
      "source": [
        "predict_model()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PMY6ihMjG1iO"
      },
      "source": [
        "# !head \"$TEST_PRED_FILE\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H93cFHmTH1su"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}