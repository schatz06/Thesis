{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Notebooks_CNN_HFO_v2.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "PYXlwVnXKhOJ",
        "CVtiFO19ymk_",
        "fzh5PNccJ28-",
        "gcdrh-r9JRdS",
        "2pQEA9LADkBP",
        "J5uoUeQBDq5Q",
        "vMCxn-8P5tsH",
        "UwH_nXJZDz2T",
        "NNNnI_mU6LN5"
      ],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/schatz06/Thesis/blob/main/Notebooks_CNN_HFO_currently_working.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aa_PJwlt7zI2"
      },
      "source": [
        "##Mount drive## "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9DjpeG7t71KC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59db31f5-30f1-4942-c68f-73d4ea992a6c"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IMo4HooFl0dv"
      },
      "source": [
        "##General Variables to use to load data##"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sJHXZcSlNqL9"
      },
      "source": [
        "fold = 3 # which fold to take \n",
        "embedding = \"protbert\"\n",
        "#embedding = \"seqvec\"\n",
        "#dataset=\"PISCES\" \n",
        "dataset=\"CB513\"\n",
        "USE_HFO=True "
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PYXlwVnXKhOJ"
      },
      "source": [
        "## Imports ##"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WCuZCtMgg5ns"
      },
      "source": [
        "# Reload all modules (except those excluded by %aimport) every time before executing the Python code typed.\n",
        "%load_ext autoreload \n",
        "%autoreload 2\n",
        "# matplotlib graphs will be included in your notebook, next to the code. \n",
        "%matplotlib inline"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HKDaAU3F5Bgs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e13434f7-4eb2-4c9f-824e-232fb7b9c482"
      },
      "source": [
        "# install hdf5storage package \r\n",
        "!pip install hdf5storage"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: hdf5storage in /usr/local/lib/python3.6/dist-packages (0.1.15)\n",
            "Requirement already satisfied: numpy; python_version >= \"3.4\" in /usr/local/lib/python3.6/dist-packages (from hdf5storage) (1.19.5)\n",
            "Requirement already satisfied: h5py>=2.1; python_version >= \"3.3\" in /usr/local/lib/python3.6/dist-packages (from hdf5storage) (2.10.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py>=2.1; python_version >= \"3.3\"->hdf5storage) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cjrANRzOFauh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2a209a9-ae58-46f7-b48c-254d3a006450"
      },
      "source": [
        "import pdb # python debugger\n",
        "import numpy as np # import numpy \n",
        "import tensorflow as tf # import tensorflow  \n",
        "tf.compat.v1.disable_eager_execution() # disable eager execution\n",
        "import time # import time\n",
        "import math # import math\n",
        "import argparse # import argparse\n",
        "import os # import os\n",
        "import scipy.io as sio # import scipy.io \n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior() # makes different behaviors betweem tf_v1 & tf_v2 behave the same \n",
        "from tensorflow.python.client import device_lib # package to find available gpus\n",
        "import pandas as pd # import padas\n",
        "import hdf5storage # import hdf5storage"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CVtiFO19ymk_"
      },
      "source": [
        "## Get data ##"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yfOssHbYEh-W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c59ec350-fb07-47da-b183-f693d96cf4e6"
      },
      "source": [
        "VALID_FILE = \"/content/drive/MyDrive/Datasets/{0}_{1}_testSet{2}.mat\".format(dataset.lower(),embedding,str(fold)) # validation set\n",
        "TRAIN_FILE = \"/content/drive/MyDrive/Datasets/{0}_{1}_trainSet{2}.mat\".format(dataset.lower(),embedding,str(fold)) # train set  \n",
        "TEST_FILE = \"/content/drive/MyDrive/Datasets/casp13_{0}.mat\".format(embedding) # test set CASP13\n",
        "print(VALID_FILE)\n",
        "print(TRAIN_FILE)\n",
        "print(TEST_FILE)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/Datasets/cb513_protbert_testSet3.mat\n",
            "/content/drive/MyDrive/Datasets/cb513_protbert_trainSet3.mat\n",
            "/content/drive/MyDrive/Datasets/casp13_protbert.mat\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QKVIpZww1-dw"
      },
      "source": [
        "HEIGHT = 32\n",
        "WIDTH = 32\n",
        "DEPTH = 1\n",
        "CATEGORIES = 3 # number of different classification categories"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fzh5PNccJ28-"
      },
      "source": [
        "## VGG ##"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NwaNt8clJ5Vu"
      },
      "source": [
        "\"\"\"\n",
        "Codes are modifeid from PyTorch and Tensorflow Versions of VGG: \n",
        "https://github.com/pytorch/vision/blob/master/torchvision/models/vgg.py, and\n",
        "https://github.com/keras-team/keras-applications/blob/master/keras_applications/vgg16.py\n",
        "\"\"\"\n",
        "\n",
        "# import tensorflow.compat.v1 as tf\n",
        "# tf.disable_v2_behavior()\n",
        "# import numpy as np \n",
        "# import pdb\n",
        "from tensorflow.keras.applications.vgg16 import VGG16 as vgg16 # import vgg16 convolutional network\n",
        "from tensorflow.keras.applications.vgg19 import VGG19 as vgg19 # import vgg19 convolutional network\n",
        "\n",
        "__all__ = ['VGG11', 'VGG13', 'VGG16','VGG19'] # array holds all vgg CNN's names\n",
        " \n",
        "def VGG(feature, num_cls): # define VGG \n",
        "\n",
        "\twith tf.variable_scope('fully_connected') as scope:\n",
        "\t\tdim =np.prod(feature.shape[1:]) # returns the product of the given array\n",
        "\t\tx = tf.reshape(feature, [-1, dim]) # reshape tensor\n",
        "\n",
        "\t\tx = tf.keras.layers.Dense(units=4096, activation='relu', name=scope.name)(x) # define layers \n",
        "\t\tx = tf.keras.layers.Dense(units=4096, activation='relu', name=scope.name)(x)\n",
        "\t\tx = tf.keras.layers.Dense(units=num_cls, name=scope.name)(x)\n",
        "\n",
        "\treturn x\n",
        "# make the layers of CNN \n",
        "def make_layers(x, cfg):\n",
        "\tfor v in cfg:\n",
        "\t\tif v == 'M':\n",
        "\t\t\tx = tf.keras.layers.MaxPool2D(pool_size=[2, 2], strides=2, padding='valid')(x)\n",
        "\t\telse:\n",
        "\t\t\tx = tf.keras.layers.Conv2D(\n",
        "\t\t\tfilters=v,\n",
        "\t\t\tkernel_size=[3, 3],\n",
        "\t\t\tpadding='SAME',\n",
        "\t\t\tactivation=tf.nn.relu\n",
        "\t\t\t)(x)\n",
        "\treturn x\n",
        "\n",
        "cfg = {\n",
        "\t'A': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
        "\t'B': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
        "\t'D': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
        "\t'E': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', \n",
        "\t\t  512, 512, 512, 512, 'M'],\n",
        "}\n",
        "\n",
        "def VGG11(x_images, num_cls):\n",
        "\tfeature = make_layers(x_images, cfg['A'])\n",
        "\treturn VGG(feature, num_cls)\n",
        "\n",
        "def VGG13(x_images, num_cls):\n",
        "\tfeature = make_layers(x_images, cfg['B'])\n",
        "\treturn VGG(feature, num_cls)\n",
        "\n",
        "def VGG16(x_images, num_cls):\n",
        "\tfeature = make_layers(x_images, cfg['D'])\n",
        "\treturn VGG(feature, num_cls)\n",
        "\n",
        "def VGG19(x_images, num_cls):\n",
        "\tfeature = make_layers(x_images, cfg['E'])\n",
        "\treturn VGG(feature, num_cls)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gcdrh-r9JRdS"
      },
      "source": [
        "## Net ##"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kHA5zxK1JRtg"
      },
      "source": [
        "# import tensorflow.compat.v1 as tf\n",
        "# tf.disable_v2_behavior()\n",
        "# import math\n",
        "# import pdb\n",
        "# from tensorflow.python.client import device_lib\n",
        "# import numpy as np\n",
        "# from net.vgg import *\n",
        "\n",
        "def CNN_4layers(x_image, num_cls, reuse=False):\n",
        "\t_NUM_CLASSES = num_cls\n",
        "\twith tf.variable_scope('conv1', reuse=reuse) as scope:\n",
        "\t\tconv = tf.keras.layers.Conv2D(\n",
        "\t\t\tfilters=64,\n",
        "\t\t\tkernel_size=[5,5],\n",
        "\t\t\tpadding='SAME',\n",
        "\t\t\tactivation=tf.nn.relu\n",
        "\t\t)(x_image)\n",
        "\n",
        "\twith tf.variable_scope('conv2', reuse=reuse) as scope:\n",
        "\t\tconv = tf.keras.layers.Conv2D(\n",
        "\t\t\tfilters=64,\n",
        "\t\t\tkernel_size=[3, 3],\n",
        "\t\t\tpadding='SAME',\n",
        "\t\t\tactivation=tf.nn.relu\n",
        "\t\t)(conv)\n",
        "\t\t\n",
        "\twith tf.variable_scope('conv3', reuse=reuse) as scope:\n",
        "\t\tconv = tf.keras.layers.Conv2D(\n",
        "\t\t\tfilters=64,\n",
        "\t\t\tkernel_size=[3, 3],\n",
        "\t\t\tpadding='SAME',\n",
        "\t\t\tactivation=tf.nn.relu\n",
        "\t\t)(conv)\n",
        "\n",
        "\twith tf.variable_scope('fully_connected', reuse=reuse) as scope:\n",
        "\t\tdim =np.prod(conv.shape[1:])\n",
        "\t\tflat = tf.reshape(conv, [-1, dim])\n",
        "\t\toutputs = tf.keras.layers.Dense(units=_NUM_CLASSES, name=scope.name)(flat)\n",
        "\n",
        "\treturn outputs\n",
        "\n",
        "\n",
        "\t# with tf.variable_scope('conv1', reuse=reuse) as scope:\n",
        "\t# \tconv = tf.keras.layers.Conv2D(\n",
        "\t# \t\tfilters=32,\n",
        "\t# \t\tkernel_size=[5, 5],\n",
        "\t# \t\tpadding='SAME',\n",
        "\t# \t\tactivation=tf.nn.relu\n",
        "\t# \t)(x_image)\n",
        "\t# \tpool = tf.keras.layers.MaxPool2D(pool_size=[2, 2], strides=2, padding='valid')(conv)\n",
        "\t# \t# N x 16 x 16 x 32\n",
        "\n",
        "\t# with tf.variable_scope('conv2', reuse=reuse) as scope:\n",
        "\t# \tconv = tf.keras.layers.Conv2D(\n",
        "\t# \t\tfilters=64,\n",
        "\t# \t\tkernel_size=[3, 3],\n",
        "\t# \t\tpadding='SAME',\n",
        "\t# \t\tactivation=tf.nn.relu\n",
        "\t# \t)(pool)\n",
        "\t# \tpool = tf.keras.layers.MaxPool2D(pool_size=[2, 2], strides=2, padding='valid')(conv)\n",
        "\t# \t# N x 8 x 8 x 64\n",
        "\t\t\n",
        "\t# with tf.variable_scope('conv3', reuse=reuse) as scope:\n",
        "\t# \tconv = tf.keras.layers.Conv2D(\n",
        "\t# \t\tfilters=64,\n",
        "\t# \t\tkernel_size=[3, 3],\n",
        "\t# \t\tpadding='SAME',\n",
        "\t# \t\tactivation=tf.nn.relu\n",
        "\t# \t)(pool)\n",
        "\t# \tpool = tf.keras.layers.MaxPool2D(pool_size=[2, 2], strides=2, padding='valid')(conv)\n",
        "\t# \t# N x 4 x 4 x 64\n",
        "\n",
        "\t# with tf.variable_scope('fully_connected', reuse=reuse) as scope:\n",
        "\t# \tdim =np.prod(pool.shape[1:])\n",
        "\t# \tflat = tf.reshape(pool, [-1, dim])\n",
        "\t# \toutputs = tf.keras.layers.Dense(units=_NUM_CLASSES, name=scope.name)(flat)\n",
        "\n",
        "\t# return outputs\n",
        "\n",
        "def CNN_7layers(x_image, num_cls, reuse=False):\n",
        "\t_NUM_CLASSES = num_cls\n",
        "\twith tf.variable_scope('conv1', reuse=reuse) as scope:\n",
        "\t\tconv = tf.keras.layers.Conv2D(\n",
        "\t\t\tfilters=64,\n",
        "\t\t\tkernel_size=[3, 3],\n",
        "\t\t\tpadding='SAME',\n",
        "\t\t\tactivation=tf.nn.relu\n",
        "\t\t)(x_image)\n",
        "\t\tconv = tf.keras.layers.Conv2D(\n",
        "\t\t\tfilters=64,\n",
        "\t\t\tkernel_size=[3, 3],\n",
        "\t\t\tpadding='SAME',\n",
        "\t\t\tactivation=tf.nn.relu\n",
        "\t\t)(conv)\n",
        "\n",
        "\twith tf.variable_scope('conv2', reuse=reuse) as scope:\n",
        "\t\tconv = tf.keras.layers.Conv2D(\n",
        "\t\t\tfilters=64,\n",
        "\t\t\tkernel_size=[3, 3],\n",
        "\t\t\tpadding='SAME',\n",
        "\t\t\tactivation=tf.nn.relu\n",
        "\t\t)(conv)\n",
        "\t\tconv = tf.keras.layers.Conv2D(\n",
        "\t\t\tfilters=64,\n",
        "\t\t\tkernel_size=[3, 3],\n",
        "\t\t\tpadding='SAME',\n",
        "\t\t\tactivation=tf.nn.relu\n",
        "\t\t)(conv)\n",
        "\n",
        "\twith tf.variable_scope('conv3', reuse=reuse) as scope:\n",
        "\t\tconv = tf.keras.layers.Conv2D(\n",
        "\t\t\tfilters=64,\n",
        "\t\t\tkernel_size=[3, 3],\n",
        "\t\t\tpadding='SAME',\n",
        "\t\t\tactivation=tf.nn.relu\n",
        "\t\t)(conv)\n",
        "\t\tconv = tf.keras.layers.Conv2D(\n",
        "\t\t\tfilters=64,\n",
        "\t\t\tkernel_size=[3, 3],\n",
        "\t\t\tpadding='SAME',\n",
        "\t\t\tactivation=tf.nn.relu\n",
        "\t\t)(conv)\n",
        "\n",
        "\twith tf.variable_scope('fully_connected', reuse=reuse) as scope:\n",
        "\t\tdim = np.prod(conv.shape[1:])\n",
        "\t\tflat = tf.reshape(conv, [-1, dim])\n",
        "\t\toutputs = tf.keras.layers.Dense(units=_NUM_CLASSES, name=scope.name)(flat)\n",
        "\n",
        "\treturn outputs\n",
        "\n",
        "\t# with tf.variable_scope('conv1', reuse=reuse) as scope:\n",
        "\t# \tconv = tf.keras.layers.Conv2D(\n",
        "\t# \t\tfilters=32,\n",
        "\t# \t\tkernel_size=[5, 5],\n",
        "\t# \t\tpadding='SAME',\n",
        "\t# \t\tactivation=tf.nn.relu\n",
        "\t# \t)(x_image)\n",
        "\t# \tconv = tf.keras.layers.Conv2D(\n",
        "\t# \t\tfilters=32,\n",
        "\t# \t\tkernel_size=[3, 3],\n",
        "\t# \t\tpadding='SAME',\n",
        "\t# \t\tactivation=tf.nn.relu\n",
        "\t# \t)(conv)\n",
        "\t# \tpool = tf.keras.layers.MaxPool2D(pool_size=[2, 2], strides=2, padding='valid')(conv)\n",
        "\t# \t# N x 16 x 16 x 32\n",
        "\n",
        "\t# with tf.variable_scope('conv2', reuse=reuse) as scope:\n",
        "\t# \tconv = tf.keras.layers.Conv2D(\n",
        "\t# \t\tfilters=64,\n",
        "\t# \t\tkernel_size=[3, 3],\n",
        "\t# \t\tpadding='SAME',\n",
        "\t# \t\tactivation=tf.nn.relu\n",
        "\t# \t)(pool)\n",
        "\t# \tconv = tf.keras.layers.Conv2D(\n",
        "\t# \t\tfilters=64,\n",
        "\t# \t\tkernel_size=[3, 3],\n",
        "\t# \t\tpadding='SAME',\n",
        "\t# \t\tactivation=tf.nn.relu\n",
        "\t# \t)(conv)\n",
        "\t# \tpool = tf.keras.layers.MaxPool2D(pool_size=[2, 2], strides=2, padding='valid')(conv)\n",
        "\t# \t# N x 8 x 8 x 64\n",
        "\n",
        "\t# with tf.variable_scope('conv3', reuse=reuse) as scope:\n",
        "\t# \tconv = tf.keras.layers.Conv2D(\n",
        "\t# \t\tfilters=64,\n",
        "\t# \t\tkernel_size=[3, 3],\n",
        "\t# \t\tpadding='SAME',\n",
        "\t# \t\tactivation=tf.nn.relu\n",
        "\t# \t)(pool)\n",
        "\t# \tconv = tf.keras.layers.Conv2D(\n",
        "\t# \t\tfilters=128,\n",
        "\t# \t\tkernel_size=[3, 3],\n",
        "\t# \t\tpadding='SAME',\n",
        "\t# \t\tactivation=tf.nn.relu\n",
        "\t# \t)(conv)\n",
        "\t# \tpool = tf.keras.layers.MaxPool2D(pool_size=[2, 2], strides=2, padding='valid')(conv)\n",
        "\t# \t# pool = tf.layers.dropout(pool, rate=0.25, name=scope.name)\n",
        "\t# \t# N x 4 x 4 x 128\n",
        "\n",
        "\t# with tf.variable_scope('fully_connected', reuse=reuse) as scope:\n",
        "\t# \tdim = np.prod(pool.shape[1:])\n",
        "\t# \tflat = tf.reshape(pool, [-1, dim])\n",
        "\t# \toutputs = tf.keras.layers.Dense(units=_NUM_CLASSES, name=scope.name)(flat)\n",
        "\n",
        "\t# return outputs\n",
        "\n",
        "def CNN(net, num_cls, dim):\n",
        "\n",
        "\t_NUM_CLASSES = num_cls\n",
        "\t_IMAGE_HEIGHT, _IMAGE_WIDTH, _IMAGE_CHANNELS = dim\n",
        "\n",
        "\twith tf.name_scope('main_params'):\n",
        "\t\tx = tf.placeholder(tf.float32, shape=[None, _IMAGE_HEIGHT, _IMAGE_WIDTH, _IMAGE_CHANNELS], name='input_of_net')\n",
        "\t\ty = tf.placeholder(tf.float32, shape=[None, _NUM_CLASSES], name='labels')\n",
        "\n",
        "\t# call CNN structure according to string net\n",
        "\toutputs = globals()[net](x, _NUM_CLASSES)\n",
        "\toutputs = tf.identity(outputs, name='output_of_net')\n",
        "\n",
        "\treturn (x, y, outputs)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2pQEA9LADkBP"
      },
      "source": [
        "## Utilities ##"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JxQzW-93DIdN"
      },
      "source": [
        "# import numpy as np\n",
        "# import math\n",
        "# import scipy.io as sio\n",
        "# import os\n",
        "# import math\n",
        "# import pdb\n",
        "\n",
        "class ConfigClass(object):\n",
        "    def __init__(self, args, num_data, num_cls):\n",
        "        super(ConfigClass, self).__init__()\n",
        "        self.args = args\n",
        "        self.iter_max = args.iter_max\n",
        "        \n",
        "        # Different notations of regularization term:\n",
        "        # In SGD, weight decay:\n",
        "        #     weight_decay <- lr/(C*num_of_training_samples)\n",
        "        # In Newton method:\n",
        "        #     C <- C * num_of_training_samples\n",
        "\n",
        "        self.seed = args.seed\n",
        "\n",
        "        if self.seed is None:\n",
        "            print('You choose not to specify a random seed.'+\\\n",
        "                'A different result is produced after each run.')\n",
        "        elif isinstance(self.seed, int) and self.seed >= 0:\n",
        "            print('You specify random seed {}.'.format(self.seed))\n",
        "        else:\n",
        "            raise ValueError('Only accept None type or nonnegative integers for'+\\\n",
        "                    ' random seed argument!')\n",
        "\n",
        "        self.train_set = args.train_set\n",
        "        self.val_set = args.val_set\n",
        "        self.num_cls = num_cls\n",
        "        self.dim = args.dim\n",
        "\n",
        "        self.num_data = num_data\n",
        "        self.GNsize = min(args.GNsize, self.num_data)\n",
        "        self.C = args.C * self.num_data\n",
        "        self.net = args.net\n",
        "\n",
        "        self.xi = 0.1\n",
        "        self.CGmax = args.CGmax\n",
        "        self._lambda = args._lambda\n",
        "        self.drop = args.drop\n",
        "        self.boost = args.boost\n",
        "        self.eta = args.eta\n",
        "        self.lr = args.lr\n",
        "        self.lr_decay = args.lr_decay\n",
        "\n",
        "        self.bsize = args.bsize\n",
        "        if args.momentum < 0:\n",
        "            raise ValueError('Momentum needs to be larger than 0!')\n",
        "        self.momentum = args.momentum\n",
        "\n",
        "        self.loss = args.loss\n",
        "        if self.loss not in ('MSELoss', 'CrossEntropy'):\n",
        "            raise ValueError('Unrecognized loss type!')\n",
        "        self.optim = args.optim\n",
        "        if self.optim not in ('SGD', 'NewtonCG', 'Adam'):\n",
        "            raise ValueError('Only support SGD, Adam & NewtonCG optimizer!')\n",
        "        \n",
        "        self.log_file = args.log_file\n",
        "        self.model_file = args.model_file\n",
        "        self.screen_log_only = args.screen_log_only\n",
        "\n",
        "        if self.screen_log_only:\n",
        "            print('You choose not to store running log. Only store model to {}'.format(self.log_file))\n",
        "        else:\n",
        "            print('Saving log to: {}'.format(self.log_file))\n",
        "            dir_name, _ = os.path.split(self.log_file)\n",
        "            if not os.path.isdir(dir_name):\n",
        "                os.makedirs(dir_name, exist_ok=True)\n",
        "\n",
        "        dir_name, _ = os.path.split(self.model_file)\n",
        "        if not os.path.isdir(dir_name):\n",
        "            os.makedirs(dir_name, exist_ok=True)\n",
        "        \n",
        "        self.elapsed_time = 0.0\n",
        "\n",
        "def read_data(filename, dim, label_enum=None):\n",
        "    \"\"\"\n",
        "    args:\n",
        "        filename: the path where .mat files are stored\n",
        "        label_enum (default None): the list that stores the original labels. \n",
        "            If label_enum is None, the function will generate a new list which stores the \n",
        "            original labels in a sequence, and map original labels to [0, 1, ... number_of_classes-1]. \n",
        "            If label_enum is a list, the function will use it to convert \n",
        "            original labels to [0, 1,..., number_of_classes-1].\n",
        "    \"\"\"\n",
        "\n",
        "    #mat_contents = sio.loadmat(filename)\n",
        "    mat_contents = hdf5storage.loadmat(filename)\n",
        "    images, labels = mat_contents['x'], mat_contents['y']\n",
        "    \n",
        "    labels = labels.reshape(-1)\n",
        "    images = images.reshape(images.shape[0], -1)\n",
        "\n",
        "    _IMAGE_HEIGHT, _IMAGE_WIDTH, _IMAGE_CHANNELS = dim\n",
        "    zero_to_append = np.zeros((images.shape[0],\n",
        "            _IMAGE_CHANNELS*_IMAGE_HEIGHT*_IMAGE_WIDTH-np.prod(images.shape[1:])))\n",
        "    images = np.append(images, zero_to_append, axis=1)\n",
        "\n",
        "    # check data validity\n",
        "    if label_enum is None:\n",
        "        label_enum, labels = np.unique(labels, return_inverse=True)\n",
        "        num_cls = labels.max() + 1\n",
        "\n",
        "        if len(label_enum) != num_cls:\n",
        "            raise ValueError('The number of classes is not equal to the number of\\\n",
        "                            labels in dataset. Please verify them.')\n",
        "    else:\n",
        "        num_cls = len(label_enum)\n",
        "        forward_map = dict(zip(label_enum, np.arange(num_cls)))\n",
        "        labels = np.expand_dims(labels, axis=1)\n",
        "        labels = np.apply_along_axis(lambda x:forward_map[x[0]], axis=1, arr=labels)\n",
        "        \n",
        "\n",
        "    # convert groundtruth to one-hot encoding\n",
        "    labels = np.eye(num_cls)[labels]\n",
        "    labels = labels.astype('float32')\n",
        "\n",
        "    return [images, labels], num_cls, label_enum\n",
        "\n",
        "def normalize_and_reshape(images, dim, mean_tr=None):\n",
        "    _IMAGE_HEIGHT, _IMAGE_WIDTH, _IMAGE_CHANNELS = dim\n",
        "    images_shape = [images.shape[0], _IMAGE_CHANNELS, _IMAGE_HEIGHT, _IMAGE_WIDTH]\n",
        "    # images normalization and zero centering\n",
        "    images = images.reshape(images_shape[0], -1)\n",
        "    #images = (images/255.0).astype(np.single)\n",
        "    if mean_tr is None:\n",
        "        print('No mean of data provided! Normalize images by their own mean.')\n",
        "        # if no mean_tr is provided, we calculate it according to the current data\n",
        "        mean_tr = images.mean(axis=0) \n",
        "    else:\n",
        "        print('Normalize images according to the provided mean.')\n",
        "        if np.prod(mean_tr.shape) != np.prod(dim):\n",
        "            raise ValueError('Dimension of provided mean does not agree with the data! Please verify them!')\n",
        "    #images = (images - mean_tr).astype(np.single)\n",
        "\n",
        "    images = images.reshape(images_shape)\n",
        "    # Tensorflow accepts data shape: B x H x W x C\n",
        "    images = np.transpose(images, (0, 2, 3, 1))\n",
        "    return images, mean_tr\n",
        "\n",
        "\n",
        "def predict(sess, network, test_batch, bsize):\n",
        "    x, y, loss, outputs = network\n",
        "\n",
        "    test_inputs, test_labels = test_batch\n",
        "    batch_size = bsize\n",
        "\n",
        "    num_data = test_labels.shape[0]\n",
        "    num_batches = math.ceil(num_data/batch_size)\n",
        "\n",
        "    results = np.zeros(shape=num_data, dtype=np.int)\n",
        "    infer_loss = 0.0\n",
        "\n",
        "    for i in range(num_batches):\n",
        "        batch_idx = np.arange(i*batch_size, min((i+1)*batch_size, num_data))\n",
        "\n",
        "        batch_input = test_inputs[batch_idx]\n",
        "        batch_labels = test_labels[batch_idx]\n",
        "\n",
        "        net_outputs, _loss = sess.run(\n",
        "            [outputs, loss], feed_dict={x: batch_input, y: batch_labels}\n",
        "            )\n",
        "        \n",
        "        results[batch_idx] = np.argmax(net_outputs, axis=1)\n",
        "        # note that _loss was summed over batches\n",
        "        infer_loss = infer_loss + _loss\n",
        "\n",
        "    avg_acc = (np.argmax(test_labels, axis=1) == results).mean()\n",
        "    avg_loss = infer_loss/num_data\n",
        "    \n",
        "    return avg_loss, avg_acc, results"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J5uoUeQBDq5Q"
      },
      "source": [
        "## Newton - CG ##"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IDug0aiKCqeG"
      },
      "source": [
        "# import pdb\n",
        "# import tensorflow as tf\n",
        "# import time\n",
        "# import numpy as np\n",
        "# import os\n",
        "# import math\n",
        "# from utilities import predict\n",
        "\n",
        "def Rop(f, weights, v):\n",
        "    \"\"\"Implementation of R operator\n",
        "    Args:\n",
        "        f: any function of weights\n",
        "        weights: list of tensors.\n",
        "        v: vector for right multiplication\n",
        "    Returns:\n",
        "        Jv: Jaccobian vector product, length same as\n",
        "            the number of output of f\n",
        "    \"\"\"\n",
        "    if type(f) == list:\n",
        "        u = [tf.zeros_like(ff) for ff in f]\n",
        "    else:\n",
        "        u = tf.zeros_like(f)  # dummy variable\n",
        "    g = tf.gradients(ys=f, xs=weights, grad_ys=u)\n",
        "    return tf.gradients(ys=g, xs=u, grad_ys=v)\n",
        "\n",
        "def Gauss_Newton_vec(outputs, loss, weights, v):\n",
        "    \"\"\"Implements Gauss-Newton vector product.\n",
        "    Args:\n",
        "        loss: Loss function.\n",
        "        outputs: outputs of the last layer (pre-softmax).\n",
        "        weights: Weights, list of tensors.\n",
        "        v: vector to be multiplied with Gauss Newton matrix\n",
        "    Returns:\n",
        "        J'BJv: Guass-Newton vector product.\n",
        "    \"\"\"\n",
        "    # Validate the input\n",
        "    if type(weights) == list:\n",
        "        if len(v) != len(weights):\n",
        "            raise ValueError(\"weights and v must have the same length.\")\n",
        "\n",
        "    grads_outputs = tf.gradients(ys=loss, xs=outputs)\n",
        "    BJv = Rop(grads_outputs, weights, v)\n",
        "    JBJv = tf.gradients(ys=outputs, xs=weights, grad_ys=BJv)\n",
        "    return JBJv\n",
        "    \n",
        "\n",
        "class newton_cg(object):\n",
        "    def __init__(self, config, sess, outputs, loss):\n",
        "        \"\"\"\n",
        "        initialize operations and vairables that will be used in newton\n",
        "        args:\n",
        "            sess: tensorflow session\n",
        "            outputs: output of the neural network (pre-softmax layer)\n",
        "            loss: function to calculate loss\n",
        "        \"\"\"\n",
        "        super(newton_cg, self).__init__()\n",
        "        self.sess = sess\n",
        "        self.config = config\n",
        "        self.outputs = outputs\n",
        "        self.loss = loss\n",
        "        self.param = tf.compat.v1.trainable_variables()\n",
        "\n",
        "        self.CGiter = 0\n",
        "        FLOAT = tf.float32\n",
        "        model_weight = self.vectorize(self.param)\n",
        "        \n",
        "        # initial variable used in CG\n",
        "        zeros = tf.zeros(model_weight.get_shape(), dtype=FLOAT)\n",
        "        self.r = tf.Variable(zeros, dtype=FLOAT, trainable=False)\n",
        "        self.v = tf.Variable(zeros, dtype=FLOAT, trainable=False)\n",
        "        self.s = tf.Variable(zeros, dtype=FLOAT, trainable=False)\n",
        "        self.g = tf.Variable(zeros, dtype=FLOAT, trainable=False)\n",
        "        # initial Gv, f for method minibatch\n",
        "        self.Gv = tf.Variable(zeros, dtype=FLOAT, trainable=False)\n",
        "        self.f = tf.Variable(0., dtype=FLOAT, trainable=False)\n",
        "\n",
        "        # rTr, cgtol and beta to be used in CG\n",
        "        self.rTr = tf.Variable(0., dtype=FLOAT, trainable=False)\n",
        "        self.cgtol = tf.Variable(0., dtype=FLOAT, trainable=False)\n",
        "        self.beta = tf.Variable(0., dtype=FLOAT, trainable=False)\n",
        "\n",
        "        # placeholder alpha, old_alpha and lambda\n",
        "        self.alpha = tf.compat.v1.placeholder(FLOAT, shape=[])\n",
        "        self.old_alpha = tf.compat.v1.placeholder(FLOAT, shape=[])\n",
        "        self._lambda = tf.compat.v1.placeholder(FLOAT, shape=[])\n",
        "\n",
        "        self.num_grad_segment = math.ceil(self.config.num_data/self.config.bsize)\n",
        "        self.num_Gv_segment = math.ceil(self.config.GNsize/self.config.bsize)\n",
        "\n",
        "        cal_loss, cal_lossgrad, cal_lossGv, \\\n",
        "        add_reg_avg_loss, add_reg_avg_grad, add_reg_avg_Gv, \\\n",
        "        zero_loss, zero_grad, zero_Gv = self._ops_in_minibatch()\n",
        "\n",
        "        # initial operations that will be used in minibatch and newton\n",
        "        self.cal_loss = cal_loss\n",
        "        self.cal_lossgrad = cal_lossgrad\n",
        "        self.cal_lossGv = cal_lossGv\n",
        "        self.add_reg_avg_loss = add_reg_avg_loss\n",
        "        self.add_reg_avg_grad = add_reg_avg_grad\n",
        "        self.add_reg_avg_Gv = add_reg_avg_Gv\n",
        "        self.zero_loss = zero_loss\n",
        "        self.zero_grad = zero_grad\n",
        "        self.zero_Gv = zero_Gv\n",
        "\n",
        "        self.CG, self.update_v = self._CG()\n",
        "        self.init_cg_vars = self._init_cg_vars()\n",
        "        self.update_gs = tf.tensordot(self.s, self.g, axes=1)\n",
        "        self.update_sGs = 0.5*tf.tensordot(self.s, -self.g-self.r-self._lambda*self.s, axes=1)\n",
        "        self.update_model = self._update_model()\n",
        "        self.gnorm = self.calc_norm(self.g)\n",
        "\n",
        "\n",
        "    def vectorize(self, tensors):\n",
        "        if isinstance(tensors, list) or isinstance(tensors, tuple):\n",
        "            vector = [tf.reshape(tensor, [-1]) for tensor in tensors]\n",
        "            return tf.concat(vector, 0) \n",
        "        else:\n",
        "            return tensors \n",
        "    \n",
        "    def inverse_vectorize(self, vector, param):\n",
        "        if isinstance(vector, list):\n",
        "            return vector\n",
        "        else:\n",
        "            tensors = []\n",
        "            offset = 0\n",
        "            num_total_param = np.sum([np.prod(p.shape.as_list()) for p in param])\n",
        "            for p in param:\n",
        "                numel = np.prod(p.shape.as_list())\n",
        "                tensors.append(tf.reshape(vector[offset: offset+numel], p.shape))\n",
        "                offset += numel\n",
        "\n",
        "            assert offset == num_total_param\n",
        "            return tensors\n",
        "\n",
        "    def calc_norm(self, v):\n",
        "        # default: frobenius norm\n",
        "        if isinstance(v, list):\n",
        "            norm = 0.\n",
        "            for p in v:\n",
        "                norm = norm + tf.norm(tensor=p)**2\n",
        "            return norm**0.5\n",
        "        else:\n",
        "            return tf.norm(tensor=v)\n",
        "\n",
        "    def _ops_in_minibatch(self):\n",
        "        \"\"\"\n",
        "        Define operations that will be used in method minibatch\n",
        "        Vectorization is already a deep copy operation.\n",
        "        Before using newton method, loss needs to be summed over training samples\n",
        "        to make results consistent.\n",
        "        \"\"\"\n",
        "\n",
        "        def cal_loss():\n",
        "            return tf.compat.v1.assign(self.f, self.f + self.loss)\n",
        "\n",
        "        def cal_lossgrad():\n",
        "            update_f = tf.compat.v1.assign(self.f, self.f + self.loss)\n",
        "\n",
        "            grad = tf.gradients(ys=self.loss, xs=self.param)\n",
        "            grad = self.vectorize(grad)\n",
        "            update_grad = tf.compat.v1.assign(self.g, self.g + grad)\n",
        "\n",
        "            return tf.group(*[update_f, update_grad])\n",
        "\n",
        "        def cal_lossGv():\n",
        "            v = self.inverse_vectorize(self.v, self.param)\n",
        "            Gv = Gauss_Newton_vec(self.outputs, self.loss, self.param, v)\n",
        "            Gv = self.vectorize(Gv)\n",
        "            return tf.compat.v1.assign(self.Gv, self.Gv + Gv) \n",
        "\n",
        "        # add regularization term to loss, gradient and Gv and further average over batches \n",
        "        def add_reg_avg_loss():\n",
        "            model_weight = self.vectorize(self.param)\n",
        "            reg = (self.calc_norm(model_weight))**2\n",
        "            reg = 1.0/(2*self.config.C) * reg\n",
        "            return tf.compat.v1.assign(self.f, reg + self.f/self.config.num_data)\n",
        "\n",
        "        def add_reg_avg_lossgrad():\n",
        "            model_weight = self.vectorize(self.param)\n",
        "            reg_grad = model_weight/self.config.C\n",
        "            return tf.compat.v1.assign(self.g, reg_grad + self.g/self.config.num_data)\n",
        "\n",
        "        def add_reg_avg_lossGv():\n",
        "            return tf.compat.v1.assign(self.Gv, (self._lambda + 1/self.config.C)*self.v\n",
        "             + self.Gv/self.config.GNsize) \n",
        "\n",
        "        # zero out loss, grad and Gv \n",
        "        def zero_loss():\n",
        "            return tf.compat.v1.assign(self.f, tf.zeros_like(self.f))\n",
        "        def zero_grad():\n",
        "            return tf.compat.v1.assign(self.g, tf.zeros_like(self.g))\n",
        "        def zero_Gv():\n",
        "            return tf.compat.v1.assign(self.Gv, tf.zeros_like(self.Gv))\n",
        "\n",
        "        return (cal_loss(), cal_lossgrad(), cal_lossGv(),\n",
        "                add_reg_avg_loss(), add_reg_avg_lossgrad(), add_reg_avg_lossGv(),\n",
        "                zero_loss(), zero_grad(), zero_Gv())\n",
        "\n",
        "    def minibatch(self, data_batch, place_holder_x, place_holder_y, mode):\n",
        "        \"\"\"\n",
        "        A function to evaluate either function value, global gradient or sub-sampled Gv\n",
        "        \"\"\"\n",
        "        if mode not in ('funonly', 'fungrad', 'Gv'):\n",
        "            raise ValueError('Unknown mode other than funonly & fungrad & Gv!')\n",
        "\n",
        "        inputs, labels = data_batch\n",
        "        num_data = labels.shape[0]\n",
        "        num_segment = math.ceil(num_data/self.config.bsize)\n",
        "        x, y = place_holder_x, place_holder_y\n",
        "\n",
        "        # before estimation starts, need to zero out f, grad and Gv according to the mode\n",
        "\n",
        "        if mode == 'funonly':\n",
        "            assert num_data == self.config.num_data\n",
        "            assert num_segment == self.num_grad_segment\n",
        "            self.sess.run(self.zero_loss)\n",
        "        elif mode == 'fungrad':\n",
        "            assert num_data == self.config.num_data\n",
        "            assert num_segment == self.num_grad_segment\n",
        "            self.sess.run([self.zero_loss, self.zero_grad])\n",
        "        else:\n",
        "            assert num_data == self.config.GNsize\n",
        "            assert num_segment == self.num_Gv_segment\n",
        "            self.sess.run(self.zero_Gv)\n",
        "\n",
        "        for i in range(num_segment):\n",
        "            \n",
        "            load_time = time.time()\n",
        "            idx = np.arange(i * self.config.bsize, min((i+1) * self.config.bsize, num_data))\n",
        "            batch_input = inputs[idx]\n",
        "            batch_labels = labels[idx]\n",
        "            batch_input = np.ascontiguousarray(batch_input)\n",
        "            batch_labels = np.ascontiguousarray(batch_labels)\n",
        "            self.config.elapsed_time += time.time() - load_time\n",
        "\n",
        "            if mode == 'funonly':\n",
        "\n",
        "                self.sess.run(self.cal_loss, feed_dict={\n",
        "                            x: batch_input, \n",
        "                            y: batch_labels,})\n",
        "\n",
        "            elif mode == 'fungrad':\n",
        "                \n",
        "                self.sess.run(self.cal_lossgrad, feed_dict={\n",
        "                            x: batch_input, \n",
        "                            y: batch_labels,})\n",
        "                \n",
        "            else:\n",
        "                \n",
        "                self.sess.run(self.cal_lossGv, feed_dict={\n",
        "                            x: batch_input, \n",
        "                            y: batch_labels})\n",
        "\n",
        "        # average over batches\n",
        "        if mode == 'funonly':\n",
        "            self.sess.run(self.add_reg_avg_loss)\n",
        "        elif mode == 'fungrad':\n",
        "            self.sess.run([self.add_reg_avg_loss, self.add_reg_avg_grad])\n",
        "        else:\n",
        "            self.sess.run(self.add_reg_avg_Gv, \n",
        "                feed_dict={self._lambda: self.config._lambda})\n",
        "\n",
        "\n",
        "    def _update_model(self):\n",
        "        update_model_ops = []\n",
        "        x = self.inverse_vectorize(self.s, self.param)\n",
        "        for i, p in enumerate(self.param):\n",
        "            op = tf.compat.v1.assign(p, p + (self.alpha-self.old_alpha) * x[i])\n",
        "            update_model_ops.append(op)\n",
        "        return tf.group(*update_model_ops)\n",
        "\n",
        "    def _init_cg_vars(self):\n",
        "        init_ops = []\n",
        "\n",
        "        init_r = tf.compat.v1.assign(self.r, -self.g)\n",
        "        init_v = tf.compat.v1.assign(self.v, -self.g)\n",
        "        init_s = tf.compat.v1.assign(self.s, tf.zeros_like(self.g))\n",
        "        gnorm = self.calc_norm(self.g)\n",
        "        init_rTr = tf.compat.v1.assign(self.rTr, gnorm**2)\n",
        "        init_cgtol = tf.compat.v1.assign(self.cgtol, self.config.xi*gnorm)\n",
        "\n",
        "        init_ops = [init_r, init_v, init_s, init_rTr, init_cgtol]\n",
        "\n",
        "        return tf.group(*init_ops)\n",
        "\n",
        "    def _CG(self):\n",
        "        \"\"\"\n",
        "        CG:\n",
        "            define operations that will be used in method newton\n",
        "        Same as the previous loss calculation,\n",
        "        Gv has been summed over batches when samples were fed into Neural Network.\n",
        "        \"\"\"\n",
        "\n",
        "        def CG_ops():\n",
        "            \n",
        "            vGv = tf.tensordot(self.v, self.Gv, axes=1)\n",
        "\n",
        "            alpha = self.rTr / vGv\n",
        "            with tf.control_dependencies([alpha]):\n",
        "                update_s = tf.compat.v1.assign(self.s, self.s + alpha * self.v, name='update_s_ops')\n",
        "                update_r = tf.compat.v1.assign(self.r, self.r - alpha * self.Gv, name='update_r_ops')\n",
        "\n",
        "                with tf.control_dependencies([update_s, update_r]):\n",
        "                    rnewTrnew = self.calc_norm(update_r)**2\n",
        "                    update_beta = tf.compat.v1.assign(self.beta, rnewTrnew / self.rTr)\n",
        "                    with tf.control_dependencies([update_beta]):\n",
        "                        update_rTr = tf.compat.v1.assign(self.rTr, rnewTrnew, name='update_rTr_ops')\n",
        "\n",
        "            return tf.group(*[update_s, update_beta, update_rTr])\n",
        "\n",
        "        def update_v():\n",
        "            return tf.compat.v1.assign(self.v, self.r + self.beta*self.v, name='update_v')\n",
        "\n",
        "        return (CG_ops(), update_v())\n",
        "\n",
        "\n",
        "    def newton(self, full_batch, val_batch, saver, network, test_network=None):\n",
        "        \"\"\"\n",
        "        Conduct newton steps for training\n",
        "        args:\n",
        "            full_batch & val_batch: provide training set and validation set. The function will\n",
        "                save the best model evaluted on validation set for future prediction.\n",
        "            network: a tuple contains (x, y, loss, outputs).\n",
        "            test_network: a tuple similar to argument network. If you use layers which behave differently\n",
        "                in test phase such as batchnorm, a separate test_network is needed.\n",
        "        return:\n",
        "            None\n",
        "        \"\"\"\n",
        "        # check whether data is valid\n",
        "        full_inputs, full_labels = full_batch\n",
        "        assert full_inputs.shape[0] == full_labels.shape[0]\n",
        "\n",
        "        if full_inputs.shape[0] != self.config.num_data:\n",
        "            raise ValueError('The number of full batch inputs does not agree with the config argument.\\\n",
        "                            This is important because global loss is averaged over those inputs')\n",
        "\n",
        "        x, y, _, outputs = network\n",
        "\n",
        "        tf.compat.v1.summary.scalar('loss', self.f)\n",
        "        merged = tf.compat.v1.summary.merge_all()\n",
        "        train_writer = tf.compat.v1.summary.FileWriter('./summary/train', self.sess.graph)\n",
        "\n",
        "        print(self.config.args)\n",
        "        if not self.config.screen_log_only:\n",
        "            log_file = open(self.config.log_file, 'w')\n",
        "            print(self.config.args, file=log_file)\n",
        "        \n",
        "        self.minibatch(full_batch, x, y, mode='fungrad')\n",
        "        f = self.sess.run(self.f)\n",
        "        output_str = 'initial f: {:.3f}'.format(f)\n",
        "        print(output_str)\n",
        "        if not self.config.screen_log_only:\n",
        "            print(output_str, file=log_file)\n",
        "        \n",
        "        best_acc = 0.0\n",
        "\n",
        "        total_running_time = 0.0\n",
        "        self.config.elapsed_time = 0.0\n",
        "        total_CG = 0\n",
        "        \n",
        "        for k in range(self.config.iter_max):\n",
        "\n",
        "            # randomly select the batch for Gv estimation\n",
        "            idx = np.random.choice(np.arange(0, full_labels.shape[0]),\n",
        "                    size=self.config.GNsize, replace=False)\n",
        "\n",
        "            mini_inputs = full_inputs[idx]\n",
        "            mini_labels = full_labels[idx]\n",
        "\n",
        "            start = time.time()\n",
        "\n",
        "            self.sess.run(self.init_cg_vars)\n",
        "            cgtol = self.sess.run(self.cgtol)\n",
        "\n",
        "            avg_cg_time = 0.0\n",
        "            for CGiter in range(1, self.config.CGmax+1):\n",
        "                \n",
        "                cg_time = time.time()\n",
        "                self.minibatch((mini_inputs, mini_labels), x, y, mode='Gv')\n",
        "                avg_cg_time += time.time() - cg_time\n",
        "                \n",
        "                self.sess.run(self.CG)\n",
        "\n",
        "                rnewTrnew = self.sess.run(self.rTr)\n",
        "                \n",
        "                if rnewTrnew**0.5 <= cgtol or CGiter == self.config.CGmax:\n",
        "                    break\n",
        "\n",
        "                self.sess.run(self.update_v)\n",
        "\n",
        "            print('Avg time per Gv iteration: {:.5f} s\\r\\n'.format(avg_cg_time/CGiter))\n",
        "\n",
        "            gs, sGs = self.sess.run([self.update_gs, self.update_sGs], feed_dict={\n",
        "                    self._lambda: self.config._lambda\n",
        "                })\n",
        "            \n",
        "            # line_search\n",
        "            f_old = f\n",
        "            alpha = 1\n",
        "            while True:\n",
        "\n",
        "                old_alpha = 0 if alpha == 1 else alpha/0.5\n",
        "                \n",
        "                self.sess.run(self.update_model, feed_dict={\n",
        "                    self.alpha:alpha, self.old_alpha:old_alpha\n",
        "                    })\n",
        "\n",
        "                prered = alpha*gs + (alpha**2)*sGs\n",
        "\n",
        "                self.minibatch(full_batch, x, y, mode='funonly')\n",
        "                f = self.sess.run(self.f)\n",
        "\n",
        "                actred = f - f_old\n",
        "\n",
        "                if actred <= self.config.eta*alpha*gs:\n",
        "                    break\n",
        "\n",
        "                alpha *= 0.5\n",
        "\n",
        "            # update lambda\n",
        "            ratio = actred / prered\n",
        "            if ratio < 0.25:\n",
        "                self.config._lambda *= self.config.boost\n",
        "            elif ratio >= 0.75:\n",
        "                self.config._lambda *= self.config.drop\n",
        "\n",
        "            self.minibatch(full_batch, x, y, mode='fungrad')\n",
        "            f = self.sess.run(self.f)\n",
        "\n",
        "            gnorm = self.sess.run(self.gnorm)\n",
        "\n",
        "            summary = self.sess.run(merged)\n",
        "            train_writer.add_summary(summary, k)\n",
        "\n",
        "            # exclude data loading time for fair comparison\n",
        "            end = time.time() \n",
        "            \n",
        "            end = end - self.config.elapsed_time\n",
        "            total_running_time += end-start\n",
        "\n",
        "            self.config.elapsed_time = 0.0\n",
        "            \n",
        "            total_CG += CGiter\n",
        "\n",
        "            output_str = '{}-iter f: {:.3f} |g|: {:.5f} alpha: {:.3e} ratio: {:.3f} lambda: {:.5f} #CG: {} actred: {:.5f} prered: {:.5f} time: {:.3f}'.\\\n",
        "                            format(k, f, gnorm, alpha, actred/prered, self.config._lambda, CGiter, actred, prered, end-start)\n",
        "            print(output_str)\n",
        "            if not self.config.screen_log_only:\n",
        "                print(output_str, file=log_file)\n",
        "\n",
        "            if val_batch is not None:\n",
        "                # Evaluate the performance after every Newton Step\n",
        "                if test_network == None:\n",
        "                    val_loss, val_acc, _ = predict(\n",
        "                        self.sess, \n",
        "                        network=(x, y, self.loss, outputs),\n",
        "                        test_batch=val_batch,\n",
        "                        bsize=self.config.bsize,\n",
        "                        )\n",
        "                else:\n",
        "                    # A separat test network part has not been done...\n",
        "                    val_loss, val_acc, _ = predict(\n",
        "                        self.sess, \n",
        "                        network=test_network,\n",
        "                        test_batch=val_batch,\n",
        "                        bsize=self.config.bsize\n",
        "                        )\n",
        "\n",
        "                output_str = '\\r\\n {}-iter val_acc: {:.3f}% val_loss {:.3f}\\r\\n'.\\\n",
        "                    format(k, val_acc*100, val_loss)\n",
        "                print(output_str)\n",
        "                if not self.config.screen_log_only:\n",
        "                    print(output_str, file=log_file)\n",
        "\n",
        "                if val_acc > best_acc:\n",
        "                    best_acc = val_acc\n",
        "                    checkpoint_path = self.config.model_file\n",
        "                    save_path = saver.save(self.sess, checkpoint_path)\n",
        "                    print('Best model saved in {}\\r\\n'.format(save_path))\n",
        "\n",
        "        if val_batch is None:\n",
        "            checkpoint_path = self.config.model_file\n",
        "            save_path = saver.save(self.sess, checkpoint_path)\n",
        "            print('Model at the last iteration saved in {}\\r\\n'.format(save_path))\n",
        "            output_str = 'total_#CG {} | total running time {:.3f}s'.format(total_CG, total_running_time)\n",
        "        else:\n",
        "            output_str = 'Final acc: {:.3f}% | best acc {:.3f}% | total_#CG {} | total running time {:.3f}s'.\\\n",
        "                format(val_acc*100, best_acc*100, total_CG, total_running_time)\n",
        "        print(output_str)\n",
        "        if not self.config.screen_log_only:\n",
        "            print(output_str, file=log_file)\n",
        "            log_file.close()"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xatdN2Zm7BOQ"
      },
      "source": [
        "##Set Train Arguments##"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v_DZPFd4m7Og"
      },
      "source": [
        "if USE_HFO:\n",
        "    # Arguments for HFO - PSSP dataset\n",
        "    train_args = (\"--optim NewtonCG --GNsize 1024 --C 0.01 --net CNN_4layers --bsize 1024 --iter_max 50 \" +\n",
        "              \"--train_set \" + TRAIN_FILE + \" --val_set \" + VALID_FILE + \" --dim \" + \n",
        "              str(HEIGHT) + \" \" + str(WIDTH) + \" \" + str(DEPTH)).split()\n",
        "else:\n",
        "    # Arguments for SGD - PSSP dataset\n",
        "    train_args = (\"--optim SGD --lr 0.05 --momentum 0.01 --C 0.01 --net CNN_4layers --bsize 1024 --epoch_max 1000 \" +\n",
        "              \"--train_set \" + TRAIN_FILE + \" --val_set \" + VALID_FILE + \" --dim \" +\n",
        "              str(HEIGHT) + \" \" + str(WIDTH) + \" \" + str(DEPTH)).split()"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vMCxn-8P5tsH"
      },
      "source": [
        "##Declare Train Function##"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cr528VD1EDj9"
      },
      "source": [
        "# import pdb\n",
        "# import numpy as np\n",
        "# import tensorflow as tf\n",
        "# tf.compat.v1.disable_eager_execution()\n",
        "# import time\n",
        "# import math\n",
        "# import argparse\n",
        "\n",
        "# from net.net import CNN\n",
        "# from newton_cg import newton_cg\n",
        "# from utilities import read_data, predict, ConfigClass, normalize_and_reshape\n",
        "\n",
        "def parse_args():\n",
        "    parser = argparse.ArgumentParser(description='Newton method on DNN')\n",
        "    parser.add_argument('--C', dest='C',\n",
        "                      help='regularization term, or so-called weight decay where'+\\\n",
        "                              'weight_decay = lr/(C*num_of_samples) in this implementation' ,\n",
        "                      default=0.01, type=float)\n",
        "\n",
        "    # Newton method arguments\n",
        "    parser.add_argument('--GNsize', dest='GNsize',\n",
        "                      help='number of samples for estimating Gauss-Newton matrix',\n",
        "                      default=4096, type=int)\n",
        "    parser.add_argument('--iter_max', dest='iter_max',\n",
        "                      help='the maximal number of Newton iterations',\n",
        "                      default=100, type=int)\n",
        "    parser.add_argument('--xi', dest='xi',\n",
        "                      help='the tolerance in the relative stopping condition for CG',\n",
        "                      default=0.1, type=float)\n",
        "    parser.add_argument('--drop', dest='drop',\n",
        "                      help='the drop constants for the LM method',\n",
        "                      default=2/3, type=float)\n",
        "    parser.add_argument('--boost', dest='boost',\n",
        "                      help='the boost constants for the LM method',\n",
        "                      default=3/2, type=float)\n",
        "    parser.add_argument('--eta', dest='eta',\n",
        "                      help='the parameter for the line search stopping condition',\n",
        "                      default=0.0001, type=float)\n",
        "    parser.add_argument('--CGmax', dest='CGmax',\n",
        "                      help='the maximal number of CG iterations',\n",
        "                      default=250, type=int)\n",
        "    parser.add_argument('--lambda', dest='_lambda',\n",
        "                      help='the initial lambda for the LM method',\n",
        "                      default=1, type=float)\n",
        "\n",
        "    # SGD arguments\n",
        "    parser.add_argument('--epoch_max', dest='epoch',\n",
        "                      help='number of training epoch',\n",
        "                      default=500, type=int)\n",
        "    parser.add_argument('--lr', dest='lr',\n",
        "                      help='learning rate',\n",
        "                      default=0.01, type=float)\n",
        "    parser.add_argument('--decay', dest='lr_decay',\n",
        "                      help='learning rate decay over each mini-batch update',\n",
        "                      default=0, type=float)\n",
        "    parser.add_argument('--momentum', dest='momentum',\n",
        "                      help='momentum of learning',\n",
        "                      default=0, type=float)\n",
        "\n",
        "    # Model training arguments\n",
        "    parser.add_argument('--bsize', dest='bsize',\n",
        "                      help='batch size to evaluate stochastic gradient, Gv, etc. Since the sampled data \\\n",
        "                      for computing Gauss-Newton matrix and etc. might not fit into memeory \\\n",
        "                      for one time, we will split the data into several segements and average\\\n",
        "                      over them.',\n",
        "                      default=1024, type=int)\n",
        "    parser.add_argument('--net', dest='net',\n",
        "                      help='classifier type',\n",
        "                      default='CNN_4layers', type=str)\n",
        "    parser.add_argument('--train_set', dest='train_set',\n",
        "                      help='provide the directory of .mat file for training',\n",
        "                      default=None, type=str)\n",
        "    parser.add_argument('--val_set', dest='val_set',\n",
        "                      help='provide the directory of .mat file for validation',\n",
        "                      default=None, type=str)\n",
        "    parser.add_argument('--model', dest='model_file',\n",
        "                      help='model saving address',\n",
        "                      default='./saved_model/model.ckpt', type=str)\n",
        "    parser.add_argument('--log', dest='log_file',\n",
        "                      help='log saving directory',\n",
        "                      default='./running_log/logger.log', type=str)\n",
        "    parser.add_argument('--screen_log_only', dest='screen_log_only',\n",
        "                      help='screen printing running log instead of storing it',\n",
        "                      action='store_true')\n",
        "    parser.add_argument('--optim', '-optim', \n",
        "                      help='which optimizer to use: SGD, Adam or NewtonCG',\n",
        "                      default='NewtonCG', type=str)\n",
        "    parser.add_argument('--loss', dest='loss', \n",
        "                      help='which loss function to use: MSELoss or CrossEntropy',\n",
        "                      default='MSELoss', type=str)\n",
        "    parser.add_argument('--dim', dest='dim', nargs='+', help='input dimension of data,'+\\\n",
        "                        'shape must be:  height width num_channels',\n",
        "                      default=[32, 32, 3], type=int)\n",
        "    parser.add_argument('--seed', dest='seed', help='a nonnegative integer for \\\n",
        "                        reproducibility', type=int)     \n",
        "    \n",
        "    args = parser.parse_args(args=train_args)\n",
        "    return args\n",
        "\n",
        "\n",
        "args = parse_args()\n",
        "\n",
        "def init_model(param):\n",
        "    init_ops = []\n",
        "    for p in param:\n",
        "        if 'kernel' in p.name:\n",
        "            weight = np.random.standard_normal(p.shape)* np.sqrt(2.0 / ((np.prod(p.get_shape().as_list()[:-1]))))\n",
        "            opt = tf.compat.v1.assign(p, weight)\n",
        "        elif 'bias' in p.name:\n",
        "            zeros = np.zeros(p.shape)\n",
        "            opt = tf.compat.v1.assign(p, zeros)\n",
        "        init_ops.append(opt)\n",
        "    return tf.group(*init_ops)\n",
        "\n",
        "def gradient_trainer(config, sess, network, full_batch, val_batch, saver, test_network):\n",
        "    x, y, loss, outputs,  = network\n",
        "    \n",
        "    global_step = tf.Variable(initial_value=0, trainable=False, name='global_step')\n",
        "    learning_rate = tf.compat.v1.placeholder(tf.float32, shape=[], name='learning_rate')\n",
        "\n",
        "    # Probably not a good way to add regularization.\n",
        "    # Just to confirm the implementation is the same as MATLAB.\n",
        "    reg = 0.0\n",
        "    param = tf.compat.v1.trainable_variables()\n",
        "    for p in param:\n",
        "        reg = reg + tf.reduce_sum(input_tensor=tf.pow(p,2))\n",
        "    reg_const = 1/(2*config.C)\n",
        "    batch_size = tf.compat.v1.cast(tf.shape(x)[0], tf.float32)\n",
        "    loss_with_reg = reg_const*reg + loss/batch_size\n",
        "\n",
        "    if config.optim == 'SGD':\n",
        "        optimizer = tf.compat.v1.train.MomentumOptimizer(\n",
        "                    learning_rate=learning_rate, \n",
        "                    momentum=config.momentum).minimize(\n",
        "                    loss_with_reg, \n",
        "                    global_step=global_step)\n",
        "    elif config.optim == 'Adam':\n",
        "        optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate,\n",
        "                                beta1=0.9,\n",
        "                                beta2=0.999,\n",
        "                                epsilon=1e-08).minimize(\n",
        "                                loss_with_reg, \n",
        "                                global_step=global_step)\n",
        "\n",
        "    train_inputs, train_labels = full_batch\n",
        "    num_data = train_labels.shape[0]\n",
        "    num_iters = math.ceil(num_data/config.bsize)\n",
        "\n",
        "    print(config.args)\n",
        "    if not config.screen_log_only:\n",
        "        log_file = open(config.log_file, 'w')\n",
        "        print(config.args, file=log_file)\n",
        "    sess.run(tf.compat.v1.global_variables_initializer())\n",
        "    \n",
        "\n",
        "    print('-------------- initializing network by methods in He et al. (2015) --------------')\n",
        "    param = tf.compat.v1.trainable_variables()\n",
        "    sess.run(init_model(param))\n",
        "\n",
        "    total_running_time = 0.0\n",
        "    best_acc = 0.0\n",
        "    lr = config.lr\n",
        "\n",
        "    for epoch in range(0, args.epoch):\n",
        "        \n",
        "        loss_avg = 0.0\n",
        "        start = time.time()\n",
        "\n",
        "        for i in range(num_iters):\n",
        "            \n",
        "            load_time = time.time()\n",
        "            # randomly select the batch\n",
        "            idx = np.random.choice(np.arange(0, num_data), \n",
        "                    size=config.bsize, replace=False)\n",
        "\n",
        "            batch_input = train_inputs[idx]\n",
        "            batch_labels = train_labels[idx]\n",
        "            batch_input = np.ascontiguousarray(batch_input)\n",
        "            batch_labels = np.ascontiguousarray(batch_labels)\n",
        "            config.elapsed_time += time.time() - load_time\n",
        "\n",
        "            step, _, batch_loss= sess.run(\n",
        "                [global_step, optimizer, loss_with_reg],\n",
        "                feed_dict = {x: batch_input, y: batch_labels, learning_rate: lr}\n",
        "                )\n",
        "\n",
        "            # print initial loss\n",
        "            if epoch == 0 and i == 0:\n",
        "                output_str = 'initial f (reg + avg. loss of 1st batch): {:.3f}'.format(batch_loss)\n",
        "                print(output_str)\n",
        "                if not config.screen_log_only:\n",
        "                    print(output_str, file=log_file)\n",
        "\n",
        "            loss_avg = loss_avg + batch_loss\n",
        "            # print log every 10% of the iterations\n",
        "            if i % math.ceil(num_iters/10) == 0:\n",
        "                end = time.time()\n",
        "                output_str = 'Epoch {}: {}/{} | loss {:.4f} | lr {:.6} | elapsed time {:.3f}'\\\n",
        "                    .format(epoch, i, num_iters, batch_loss , lr, end-start)\n",
        "                print(output_str)\n",
        "                if not config.screen_log_only:\n",
        "                    print(output_str, file=log_file)\n",
        "            \n",
        "            # adjust learning rate for SGD by inverse time decay\n",
        "            if args.optim != 'Adam':\n",
        "                lr = config.lr/(1 + args.lr_decay*step)\n",
        "\n",
        "        # exclude data loading time for fair comparison\n",
        "        epoch_end = time.time() - config.elapsed_time\n",
        "        total_running_time += epoch_end - start\n",
        "        config.elapsed_time = 0.0\n",
        "        \n",
        "        if val_batch is None:\n",
        "            output_str = 'In epoch {} train loss: {:.3f} | epoch time {:.3f}'\\\n",
        "                .format(epoch, loss_avg/(i+1), epoch_end-start)            \n",
        "        else:\n",
        "            if test_network == None:\n",
        "                val_loss, val_acc, _ = predict(\n",
        "                    sess, \n",
        "                    network=(x, y, loss, outputs),\n",
        "                    test_batch=val_batch,\n",
        "                    bsize=config.bsize\n",
        "                    )\n",
        "            else:\n",
        "                # A separat test network part have been done...\n",
        "                val_loss, val_acc, _ = predict(\n",
        "                    sess, \n",
        "                    network=test_network,\n",
        "                    test_batch=val_batch,\n",
        "                    bsize=config.bsize\n",
        "                    )\n",
        "            \n",
        "            output_str = 'In epoch {} train loss: {:.3f} | val loss: {:.3f} | val accuracy: {:.3f}% | epoch time {:.3f}'\\\n",
        "                .format(epoch, loss_avg/(i+1), val_loss, val_acc*100, epoch_end-start)\n",
        "        \n",
        "            if val_acc > best_acc:\n",
        "                best_acc = val_acc\n",
        "                checkpoint_path = config.model_file \n",
        "                save_path = saver.save(sess, checkpoint_path)\n",
        "                print('Saved best model in {}'.format(save_path))\n",
        "\n",
        "        print(output_str)\n",
        "        if not config.screen_log_only:\n",
        "            print(output_str, file=log_file)\n",
        "\n",
        "    if val_batch is None:\n",
        "        checkpoint_path = config.model_file \n",
        "        save_path = saver.save(sess, checkpoint_path)\n",
        "        print('Model at the last iteration saved in {}\\r\\n'.format(save_path))\n",
        "        output_str = 'total running time {:.3f}s'.format(total_running_time)\n",
        "    else:\n",
        "        output_str = 'Final acc: {:.3f}% | best acc {:.3f}% | total running time {:.3f}s'\\\n",
        "            .format(val_acc*100, best_acc*100, total_running_time)\n",
        "    \n",
        "    print(output_str)\n",
        "    if not config.screen_log_only:\n",
        "        print(output_str, file=log_file)\n",
        "        log_file.close()\n",
        "\n",
        "def newton_trainer(config, sess, network, full_batch, val_batch, saver, test_network):\n",
        "    _, _, loss, outputs = network\n",
        "    newton_solver = newton_cg(config, sess, outputs, loss)\n",
        "    sess.run(tf.compat.v1.global_variables_initializer())\n",
        "\n",
        "    print('-------------- initializing network by methods in He et al. (2015) --------------')\n",
        "    param = tf.compat.v1.trainable_variables()\n",
        "    sess.run(init_model(param))\n",
        "    newton_solver.newton(full_batch, val_batch, saver, network, test_network)\n",
        "\n",
        "\n",
        "def train_model():\n",
        "    full_batch, num_cls, label_enum = read_data(filename=args.train_set, dim=args.dim)\n",
        "    \n",
        "    if args.val_set is None:\n",
        "        print('No validation set is provided. Will output model at the last iteration.')\n",
        "        val_batch = None\n",
        "    else:\n",
        "        val_batch, _, _ = read_data(filename=args.val_set, dim=args.dim, label_enum=label_enum)\n",
        "\n",
        "    num_data = full_batch[0].shape[0]\n",
        "    \n",
        "    config = ConfigClass(args, num_data, num_cls)\n",
        "\n",
        "    if isinstance(config.seed, int):\n",
        "        tf.compat.v1.random.set_random_seed(config.seed)\n",
        "        np.random.seed(config.seed)\n",
        "\n",
        "    if config.net in ('CNN_4layers', 'CNN_7layers', 'VGG11', 'VGG13', 'VGG16','VGG19'):\n",
        "        x, y, outputs = CNN(config.net, num_cls, config.dim)\n",
        "        test_network = None\n",
        "    else:\n",
        "        raise ValueError('Unrecognized training model')\n",
        "\n",
        "    if config.loss == 'MSELoss':\n",
        "        loss = tf.reduce_sum(input_tensor=tf.pow(outputs-y, 2))\n",
        "    else:\n",
        "        loss = tf.reduce_sum(input_tensor=tf.nn.softmax_cross_entropy_with_logits(logits=outputs, labels=y))\n",
        "    \n",
        "    network = (x, y, loss, outputs)\n",
        "\n",
        "    sess_config = tf.compat.v1.ConfigProto()\n",
        "    sess_config.gpu_options.allow_growth = True\n",
        "\n",
        "    with tf.compat.v1.Session(config=sess_config) as sess:\n",
        "        \n",
        "        full_batch[0], mean_tr = normalize_and_reshape(full_batch[0], dim=config.dim, mean_tr=None)\n",
        "        if val_batch is not None:\n",
        "            val_batch[0], _ = normalize_and_reshape(val_batch[0], dim=config.dim, mean_tr=mean_tr)\n",
        "\n",
        "        param = tf.compat.v1.trainable_variables()\n",
        "\n",
        "        mean_param = tf.compat.v1.get_variable(name='mean_tr', initializer=mean_tr, trainable=False, \n",
        "                    validate_shape=True, use_resource=False)\n",
        "        label_enum_var=tf.compat.v1.get_variable(name='label_enum', initializer=label_enum, trainable=False,\n",
        "                    validate_shape=True, use_resource=False)\n",
        "        saver = tf.compat.v1.train.Saver(var_list=param+[mean_param])\n",
        "        \n",
        "        if config.optim in ('SGD', 'Adam'):\n",
        "            gradient_trainer(\n",
        "                config, sess, network, full_batch, val_batch, saver, test_network)\n",
        "        elif config.optim == 'NewtonCG':\n",
        "            newton_trainer(\n",
        "                config, sess, network, full_batch, val_batch, saver, test_network=test_network)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wi6UglYiD7Zd"
      },
      "source": [
        "## Train ##"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cSgKfh4r5lu7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a60aba2c-99b3-4fb2-9fec-f30675c47e93"
      },
      "source": [
        "train_model()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "You choose not to specify a random seed.A different result is produced after each run.\n",
            "Saving log to: ./running_log/logger.log\n",
            "No mean of data provided! Normalize images by their own mean.\n",
            "Normalize images according to the provided mean.\n",
            "-------------- initializing network by methods in He et al. (2015) --------------\n",
            "Namespace(C=0.01, CGmax=250, GNsize=1024, _lambda=1, boost=1.5, bsize=1024, dim=[32, 32, 1], drop=0.6666666666666666, epoch=500, eta=0.0001, iter_max=50, log_file='./running_log/logger.log', loss='MSELoss', lr=0.01, lr_decay=0, model_file='./saved_model/model.ckpt', momentum=0, net='CNN_4layers', optim='NewtonCG', screen_log_only=False, seed=None, train_set='/content/drive/MyDrive/Datasets/cb513_protbert_trainSet3.mat', val_set='/content/drive/MyDrive/Datasets/cb513_protbert_testSet3.mat', xi=0.1)\n",
            "initial f: 1.433\n",
            "Avg time per Gv iteration: 0.32683 s\n",
            "\n",
            "0-iter f: 0.944 |g|: 2.67765 alpha: 1.000e+00 ratio: 1.007 lambda: 0.66667 #CG: 1 actred: -0.48900 prered: -0.48555 time: 7.213\n",
            "\n",
            " 0-iter val_acc: 40.786% val_loss 0.683\n",
            "\n",
            "Best model saved in ./saved_model/model.ckpt\n",
            "\n",
            "Avg time per Gv iteration: 0.13891 s\n",
            "\n",
            "1-iter f: 0.638 |g|: 1.10117 alpha: 1.000e+00 ratio: 0.565 lambda: 0.66667 #CG: 17 actred: -0.30513 prered: -0.54005 time: 9.186\n",
            "\n",
            " 1-iter val_acc: 74.913% val_loss 0.394\n",
            "\n",
            "Best model saved in ./saved_model/model.ckpt\n",
            "\n",
            "Avg time per Gv iteration: 0.13905 s\n",
            "\n",
            "2-iter f: 0.592 |g|: 0.60963 alpha: 1.000e+00 ratio: 0.642 lambda: 0.66667 #CG: 15 actred: -0.04650 prered: -0.07241 time: 8.891\n",
            "\n",
            " 2-iter val_acc: 78.299% val_loss 0.354\n",
            "\n",
            "Best model saved in ./saved_model/model.ckpt\n",
            "\n",
            "Avg time per Gv iteration: 0.13901 s\n",
            "\n",
            "3-iter f: 0.574 |g|: 0.32247 alpha: 1.000e+00 ratio: 0.777 lambda: 0.44444 #CG: 14 actred: -0.01815 prered: -0.02335 time: 8.763\n",
            "\n",
            " 3-iter val_acc: 78.931% val_loss 0.339\n",
            "\n",
            "Best model saved in ./saved_model/model.ckpt\n",
            "\n",
            "Avg time per Gv iteration: 0.13917 s\n",
            "\n",
            "4-iter f: 0.560 |g|: 0.23353 alpha: 1.000e+00 ratio: 0.839 lambda: 0.29630 #CG: 17 actred: -0.01398 prered: -0.01666 time: 9.171\n",
            "\n",
            " 4-iter val_acc: 79.241% val_loss 0.333\n",
            "\n",
            "Best model saved in ./saved_model/model.ckpt\n",
            "\n",
            "Avg time per Gv iteration: 0.13871 s\n",
            "\n",
            "5-iter f: 0.547 |g|: 0.32468 alpha: 1.000e+00 ratio: 0.800 lambda: 0.19753 #CG: 20 actred: -0.01287 prered: -0.01609 time: 9.599\n",
            "\n",
            " 5-iter val_acc: 79.080% val_loss 0.328\n",
            "\n",
            "Avg time per Gv iteration: 0.13903 s\n",
            "\n",
            "6-iter f: 0.536 |g|: 0.78462 alpha: 1.000e+00 ratio: 0.549 lambda: 0.19753 #CG: 24 actred: -0.01076 prered: -0.01959 time: 10.144\n",
            "\n",
            " 6-iter val_acc: 79.278% val_loss 0.331\n",
            "\n",
            "Best model saved in ./saved_model/model.ckpt\n",
            "\n",
            "Avg time per Gv iteration: 0.13881 s\n",
            "\n",
            "7-iter f: 0.528 |g|: 0.48351 alpha: 5.000e-01 ratio: 0.553 lambda: 0.19753 #CG: 22 actred: -0.00793 prered: -0.01435 time: 11.474\n",
            "\n",
            " 7-iter val_acc: 79.563% val_loss 0.326\n",
            "\n",
            "Best model saved in ./saved_model/model.ckpt\n",
            "\n",
            "Avg time per Gv iteration: 0.13951 s\n",
            "\n",
            "8-iter f: 0.521 |g|: 1.08392 alpha: 1.000e+00 ratio: 0.462 lambda: 0.19753 #CG: 21 actred: -0.00717 prered: -0.01552 time: 9.733\n",
            "\n",
            " 8-iter val_acc: 79.253% val_loss 0.329\n",
            "\n",
            "Avg time per Gv iteration: 0.13909 s\n",
            "\n",
            "9-iter f: 0.515 |g|: 0.39400 alpha: 5.000e-01 ratio: 0.533 lambda: 0.19753 #CG: 17 actred: -0.00620 prered: -0.01163 time: 10.794\n",
            "\n",
            " 9-iter val_acc: 79.340% val_loss 0.326\n",
            "\n",
            "Avg time per Gv iteration: 0.13923 s\n",
            "\n",
            "10-iter f: 0.509 |g|: 0.94632 alpha: 1.000e+00 ratio: 0.405 lambda: 0.19753 #CG: 24 actred: -0.00547 prered: -0.01351 time: 10.152\n",
            "\n",
            " 10-iter val_acc: 79.539% val_loss 0.327\n",
            "\n",
            "Avg time per Gv iteration: 0.13924 s\n",
            "\n",
            "11-iter f: 0.504 |g|: 0.61458 alpha: 5.000e-01 ratio: 0.468 lambda: 0.19753 #CG: 17 actred: -0.00543 prered: -0.01161 time: 10.781\n",
            "\n",
            " 11-iter val_acc: 79.750% val_loss 0.324\n",
            "\n",
            "Best model saved in ./saved_model/model.ckpt\n",
            "\n",
            "Avg time per Gv iteration: 0.13946 s\n",
            "\n",
            "12-iter f: 0.500 |g|: 1.27816 alpha: 1.000e+00 ratio: 0.331 lambda: 0.19753 #CG: 19 actred: -0.00402 prered: -0.01214 time: 9.451\n",
            "\n",
            " 12-iter val_acc: 79.216% val_loss 0.330\n",
            "\n",
            "Avg time per Gv iteration: 0.13898 s\n",
            "\n",
            "13-iter f: 0.495 |g|: 0.75279 alpha: 5.000e-01 ratio: 0.463 lambda: 0.19753 #CG: 17 actred: -0.00516 prered: -0.01115 time: 10.777\n",
            "\n",
            " 13-iter val_acc: 79.464% val_loss 0.325\n",
            "\n",
            "Avg time per Gv iteration: 0.13919 s\n",
            "\n",
            "14-iter f: 0.493 |g|: 2.22894 alpha: 1.000e+00 ratio: 0.125 lambda: 0.29630 #CG: 19 actred: -0.00148 prered: -0.01188 time: 9.451\n",
            "\n",
            " 14-iter val_acc: 79.377% val_loss 0.331\n",
            "\n",
            "Avg time per Gv iteration: 0.13923 s\n",
            "\n",
            "15-iter f: 0.489 |g|: 0.44726 alpha: 1.000e+00 ratio: 0.803 lambda: 0.19753 #CG: 4 actred: -0.00402 prered: -0.00500 time: 7.376\n",
            "\n",
            " 15-iter val_acc: 79.625% val_loss 0.325\n",
            "\n",
            "Avg time per Gv iteration: 0.13920 s\n",
            "\n",
            "16-iter f: 0.488 |g|: 1.35448 alpha: 1.000e+00 ratio: 0.119 lambda: 0.29630 #CG: 24 actred: -0.00168 prered: -0.01412 time: 10.154\n",
            "\n",
            " 16-iter val_acc: 79.253% val_loss 0.331\n",
            "\n",
            "Avg time per Gv iteration: 0.13939 s\n",
            "\n",
            "17-iter f: 0.485 |g|: 1.32691 alpha: 1.000e+00 ratio: 0.114 lambda: 0.44444 #CG: 19 actred: -0.00220 prered: -0.01939 time: 9.471\n",
            "\n",
            " 17-iter val_acc: 79.601% val_loss 0.332\n",
            "\n",
            "Avg time per Gv iteration: 0.13922 s\n",
            "\n",
            "18-iter f: 0.481 |g|: 1.11388 alpha: 1.000e+00 ratio: 0.240 lambda: 0.66667 #CG: 16 actred: -0.00413 prered: -0.01721 time: 9.033\n",
            "\n",
            " 18-iter val_acc: 79.638% val_loss 0.330\n",
            "\n",
            "Avg time per Gv iteration: 0.13914 s\n",
            "\n",
            "19-iter f: 0.477 |g|: 0.89746 alpha: 1.000e+00 ratio: 0.445 lambda: 0.66667 #CG: 13 actred: -0.00432 prered: -0.00972 time: 8.617\n",
            "\n",
            " 19-iter val_acc: 79.365% val_loss 0.327\n",
            "\n",
            "Avg time per Gv iteration: 0.13927 s\n",
            "\n",
            "20-iter f: 0.474 |g|: 0.59627 alpha: 1.000e+00 ratio: 0.504 lambda: 0.66667 #CG: 13 actred: -0.00249 prered: -0.00493 time: 8.614\n",
            "\n",
            " 20-iter val_acc: 79.638% val_loss 0.326\n",
            "\n",
            "Avg time per Gv iteration: 0.13908 s\n",
            "\n",
            "21-iter f: 0.473 |g|: 0.44408 alpha: 1.000e+00 ratio: 0.565 lambda: 0.66667 #CG: 13 actred: -0.00183 prered: -0.00325 time: 8.617\n",
            "\n",
            " 21-iter val_acc: 79.526% val_loss 0.325\n",
            "\n",
            "Avg time per Gv iteration: 0.13936 s\n",
            "\n",
            "22-iter f: 0.471 |g|: 0.52340 alpha: 1.000e+00 ratio: 0.421 lambda: 0.66667 #CG: 15 actred: -0.00136 prered: -0.00323 time: 8.895\n",
            "\n",
            " 22-iter val_acc: 79.663% val_loss 0.325\n",
            "\n",
            "Avg time per Gv iteration: 0.13899 s\n",
            "\n",
            "23-iter f: 0.470 |g|: 0.52324 alpha: 1.000e+00 ratio: 0.471 lambda: 0.66667 #CG: 13 actred: -0.00151 prered: -0.00321 time: 8.629\n",
            "\n",
            " 23-iter val_acc: 79.464% val_loss 0.325\n",
            "\n",
            "Avg time per Gv iteration: 0.13924 s\n",
            "\n",
            "24-iter f: 0.468 |g|: 0.41073 alpha: 1.000e+00 ratio: 0.527 lambda: 0.66667 #CG: 13 actred: -0.00153 prered: -0.00290 time: 8.616\n",
            "\n",
            " 24-iter val_acc: 79.539% val_loss 0.325\n",
            "\n",
            "Avg time per Gv iteration: 0.13894 s\n",
            "\n",
            "25-iter f: 0.467 |g|: 0.50070 alpha: 1.000e+00 ratio: 0.528 lambda: 0.66667 #CG: 13 actred: -0.00140 prered: -0.00265 time: 8.617\n",
            "\n",
            " 25-iter val_acc: 79.427% val_loss 0.326\n",
            "\n",
            "Avg time per Gv iteration: 0.13916 s\n",
            "\n",
            "26-iter f: 0.466 |g|: 0.45282 alpha: 1.000e+00 ratio: 0.328 lambda: 0.66667 #CG: 13 actred: -0.00092 prered: -0.00281 time: 8.613\n",
            "\n",
            " 26-iter val_acc: 79.390% val_loss 0.327\n",
            "\n",
            "Avg time per Gv iteration: 0.13906 s\n",
            "\n",
            "27-iter f: 0.465 |g|: 0.42201 alpha: 1.000e+00 ratio: 0.368 lambda: 0.66667 #CG: 13 actred: -0.00130 prered: -0.00354 time: 8.618\n",
            "\n",
            " 27-iter val_acc: 79.452% val_loss 0.325\n",
            "\n",
            "Avg time per Gv iteration: 0.13917 s\n",
            "\n",
            "28-iter f: 0.463 |g|: 0.45632 alpha: 1.000e+00 ratio: 0.565 lambda: 0.66667 #CG: 16 actred: -0.00182 prered: -0.00323 time: 9.035\n",
            "\n",
            " 28-iter val_acc: 79.477% val_loss 0.327\n",
            "\n",
            "Avg time per Gv iteration: 0.13914 s\n",
            "\n",
            "29-iter f: 0.461 |g|: 0.31133 alpha: 1.000e+00 ratio: 0.722 lambda: 0.66667 #CG: 13 actred: -0.00149 prered: -0.00206 time: 8.623\n",
            "\n",
            " 29-iter val_acc: 79.526% val_loss 0.325\n",
            "\n",
            "Avg time per Gv iteration: 0.13938 s\n",
            "\n",
            "30-iter f: 0.460 |g|: 0.17782 alpha: 1.000e+00 ratio: 0.827 lambda: 0.44444 #CG: 16 actred: -0.00159 prered: -0.00192 time: 9.036\n",
            "\n",
            " 30-iter val_acc: 79.551% val_loss 0.326\n",
            "\n",
            "Avg time per Gv iteration: 0.13903 s\n",
            "\n",
            "31-iter f: 0.458 |g|: 0.26532 alpha: 1.000e+00 ratio: 0.836 lambda: 0.29630 #CG: 19 actred: -0.00215 prered: -0.00257 time: 9.450\n",
            "\n",
            " 31-iter val_acc: 79.588% val_loss 0.326\n",
            "\n",
            "Avg time per Gv iteration: 0.13919 s\n",
            "\n",
            "32-iter f: 0.455 |g|: 0.56807 alpha: 1.000e+00 ratio: 0.617 lambda: 0.29630 #CG: 19 actred: -0.00241 prered: -0.00390 time: 9.449\n",
            "\n",
            " 32-iter val_acc: 79.501% val_loss 0.328\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UwH_nXJZDz2T"
      },
      "source": [
        "## Predict ##"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9zyzmNHFnOwD"
      },
      "source": [
        "# Arguments for prediction PSSP dataset\n",
        "pred_args = (\"--bsize 1024 --valid_set \" + VALID_FILE + \" --train_set \" + TRAIN_FILE + \n",
        "\t\t\t\t\t\t \" --model ./saved_model/model.ckpt --dim \" +\n",
        "             str(HEIGHT) + \" \" + str(WIDTH) + \" \" + str(DEPTH)).split()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s6MctxH5_nTR"
      },
      "source": [
        "valid_f =  \"/content/drive/MyDrive/Datasets/{0}_test_fold{1}.txt\".format(dataset.lower(),str(fold)) # train set  \n",
        "train_f = \"/content/drive/MyDrive/Datasets/{0}_train_fold{1}.txt\".format(dataset.lower(),str(fold)) # validation set\n",
        "test_f = \"/content/drive/MyDrive/Datasets/CASP13_3class.txt\" # test set CASP13\n",
        "print(valid_f)\n",
        "print(train_f)\n",
        "print(test_f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IeQqT96h_NHr"
      },
      "source": [
        "VALID_PRED_FILE=\"pred_test_fold{0}.txt\".format(fold)\n",
        "TRAIN_PRED_FILE=\"pred_train_fold{0}.txt\".format(fold)\n",
        "TEST_PRED_FILE=\"pred_casp13_fold{0}.txt\".format(fold)\n",
        "print(VALID_PRED_FILE)\n",
        "print(TRAIN_PRED_FILE)\n",
        "print(TEST_PRED_FILE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NNNnI_mU6LN5"
      },
      "source": [
        "##Declare Predict Methods##"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kYGPhdqk5wYp"
      },
      "source": [
        "def create_output_pred(pred, origin_f, outFileName):\n",
        "    labels = ['C','H','E']\n",
        "    read_file = open(origin_f,\"r\")\n",
        "    output_file = open(outFileName,\"w\")\n",
        "    count =1 \n",
        "    target_name =1\n",
        "    target_secondary = 3\n",
        "    counter = 0\n",
        "    while True:\n",
        "        line = read_file.readline()\n",
        "        if not line:\n",
        "            break\n",
        "        if count == target_name:\n",
        "            output_file.write(line)\n",
        "            target_name+=3\n",
        "        if count == target_secondary:\n",
        "            output_file.write(line)\n",
        "            target_secondary+=3\n",
        "            line = line.replace(\"\\n\",\"\") \n",
        "            prediction = \"\"\n",
        "            for c in line:\n",
        "                if (c!='!'):\n",
        "                    prediction = prediction + labels[pred[counter]]\n",
        "                    counter +=1\n",
        "            output_file.write(prediction + \"\\n\")        \n",
        "        count+=1\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EA8Pq5a0C3m5"
      },
      "source": [
        "# import tensorflow as tf \n",
        "# tf.compat.v1.disable_eager_execution()\n",
        "# from utilities import predict, read_data, normalize_and_reshape\n",
        "# from net.net import CNN\n",
        "# import numpy as np \n",
        "# import argparse\n",
        "# import pdb\n",
        "\n",
        "def parse_args():\n",
        "        parser = argparse.ArgumentParser(description='prediction')\n",
        "        parser.add_argument('--test_set', dest='test_set',\n",
        "                            help='provide the directory of .mat file for testing',\n",
        "                            default=None, type=str)\n",
        "        parser.add_argument('--valid_set', dest='valid_set',\n",
        "                            help='provide the directory of .mat file for validation',\n",
        "                            default=None, type=str)\n",
        "        parser.add_argument('--train_set', dest='train_set',\n",
        "                            help='provide the directory of .mat file for training',\n",
        "                            default=None, type=str)\n",
        "        parser.add_argument('--model', dest='model_file',\n",
        "                            help='provide file storing network parameters, i.e. ./dir/model.ckpt',\n",
        "                            default='./saved_model/model.ckpt', type=str)\n",
        "        parser.add_argument('--bsize', dest='bsize',\n",
        "                            help='batch size',\n",
        "                            default=1024, type=int)\n",
        "        parser.add_argument('--loss', dest='loss', \n",
        "                            help='which loss function to use: MSELoss or CrossEntropy',\n",
        "                            default='MSELoss', type=str)\n",
        "        parser.add_argument('--dim', dest='dim', nargs='+', help='input dimension of data,'+\\\n",
        "                            'shape must be:  height width num_channels',\n",
        "                            default=[32, 32, 3], type=int)\n",
        "\n",
        "        args = parser.parse_args(args=pred_args)\n",
        "        return args\n",
        "\n",
        "def predict_model():\n",
        "        args = parse_args()\n",
        "\n",
        "        sess_config = tf.compat.v1.ConfigProto()\n",
        "        sess_config.gpu_options.allow_growth = True\n",
        "\n",
        "        with tf.compat.v1.Session(config=sess_config) as sess:\n",
        "                graph_address = args.model_file + '.meta'\n",
        "                imported_graph = tf.compat.v1.train.import_meta_graph(graph_address)\n",
        "                imported_graph.restore(sess, args.model_file)\n",
        "                mean_param = [v for v in tf.compat.v1.global_variables() if 'mean_tr:0' in v.name][0]\n",
        "                label_enum_var = [v for v in tf.compat.v1.global_variables() if 'label_enum:0' in v.name][0]\n",
        "\n",
        "                sess.run(tf.compat.v1.variables_initializer([mean_param, label_enum_var]))\n",
        "                mean_tr = sess.run(mean_param)\n",
        "                label_enum = sess.run(label_enum_var)\n",
        "\n",
        "                x = tf.compat.v1.get_default_graph().get_tensor_by_name('main_params/input_of_net:0')\n",
        "                y = tf.compat.v1.get_default_graph().get_tensor_by_name('main_params/labels:0')\n",
        "                outputs = tf.compat.v1.get_default_graph().get_tensor_by_name('output_of_net:0')\n",
        "\n",
        "                if args.loss == 'MSELoss':\n",
        "                        loss = tf.reduce_sum(input_tensor=tf.pow(outputs-y, 2))\n",
        "                else:\n",
        "                        loss = tf.reduce_sum(input_tensor=\n",
        "                            tf.nn.softmax_cross_entropy_with_logits(logits=outputs, labels=tf.stop_gradient(y)))\n",
        "                \n",
        "                network = (x, y, loss, outputs)\n",
        "\n",
        "                if args.valid_set is not None:\n",
        "                        valid_batch, num_cls, _ = read_data(args.valid_set, dim=args.dim, label_enum=label_enum)\n",
        "                        valid_batch[0], _ = normalize_and_reshape(valid_batch[0], dim=args.dim, mean_tr=mean_tr)\n",
        "            \n",
        "                        avg_loss_valid, avg_acc_valid, results_valid = predict(sess, network, valid_batch, args.bsize)\n",
        "\n",
        "                        # convert results back to the original labels\n",
        "                        inverse_map = dict(zip(np.arange(num_cls), label_enum))\n",
        "                        results_valid = np.expand_dims(results_valid, axis=1)\n",
        "                        results_valid = np.apply_along_axis(lambda x: inverse_map[x[0]], axis=1, arr=results_valid)\n",
        "                        create_output_pred(results_valid, valid_f, VALID_PRED_FILE)\n",
        "                        print('In valid phase, average loss: {:.3f} | average accuracy: {:.3f}%'.\\\n",
        "                            format(avg_loss_valid, avg_acc_valid*100))\n",
        "                \n",
        "                if args.train_set is not None:\n",
        "                        train_batch, num_cls, _ = read_data(args.train_set, dim=args.dim, label_enum=label_enum)\n",
        "                        train_batch[0], _ = normalize_and_reshape(train_batch[0], dim=args.dim, mean_tr=mean_tr)\n",
        "\n",
        "                        avg_loss_train, avg_acc_train, results_train = predict(sess, network, train_batch, args.bsize)\n",
        "                        # convert results back to the original labels\n",
        "                        inverse_map = dict(zip(np.arange(num_cls), label_enum))\n",
        "                        results_train = np.expand_dims(results_train, axis=1)\n",
        "                        results_train = np.apply_along_axis(lambda x: inverse_map[x[0]], axis=1, arr=results_train)\n",
        "                        # create_output_pred(results, results_train)\n",
        "\n",
        "                        create_output_pred(results_train, train_f, TRAIN_PRED_FILE)\n",
        "                        print('In train phase, average loss: {:.3f} | average accuracy: {:.3f}%'.\\\n",
        "                            format(avg_loss_train, avg_acc_train*100))\n",
        "\n",
        "                if args.test_set is not None:\n",
        "                        test_batch, num_cls, _ = read_data(args.test_set, dim=args.dim, label_enum=label_enum)\n",
        "                        test_batch[0], _ = normalize_and_reshape(test_batch[0], dim=args.dim, mean_tr=mean_tr)\n",
        "\n",
        "                        avg_loss_test, avg_acc_test, results_test = predict(sess, network, test_batch, args.bsize)\n",
        "                        # convert results back to the original labels\n",
        "                        inverse_map = dict(zip(np.arange(num_cls), label_enum))\n",
        "                        results_test = np.expand_dims(results_test, axis=1)\n",
        "                        results_test = np.apply_along_axis(lambda x: inverse_map[x[0]], axis=1, arr=results_test)\n",
        "                        # create_output_pred(results, results_train)\n",
        "\n",
        "                        create_output_pred(results_test, test_f, TEST_PRED_FILE)\n",
        "                        print('In test phase, average loss: {:.3f} | average accuracy: {:.3f}%'.\\\n",
        "                            format(avg_loss_test, avg_acc_test*100))\n",
        "            "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aN8BTASh6eSL"
      },
      "source": [
        "##Run Predict and Display output##"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gCInY5uB6Y3G"
      },
      "source": [
        "predict_model()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "loR25EiOE4cQ"
      },
      "source": [
        "# !head \"$VALID_PRED_FILE\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NOnpS-N3DwhK"
      },
      "source": [
        "# !head \"$TRAIN_PRED_FILE\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exgdCTXb68Vl"
      },
      "source": [
        "## Check Test score on CASP13 ##"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lda3hrM4dmWk"
      },
      "source": [
        "# Arguments for prediction PSSP dataset\n",
        "pred_args = (\"--bsize 1024 --test_set \" + TEST_FILE + \n",
        "\t\t\t\t\t\t \" --model ./saved_model/model.ckpt --dim \" +\n",
        "             str(HEIGHT) + \" \" + str(WIDTH) + \" \" + str(DEPTH)).split()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3JCC5-2mk0rs"
      },
      "source": [
        "predict_model()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PMY6ihMjG1iO"
      },
      "source": [
        "# !head \"$TEST_PRED_FILE\""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}